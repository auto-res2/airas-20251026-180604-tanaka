Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 212ms
Installed 98 packages in 4.62s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 6169.94 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 23583.62 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25840.36 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.89s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.63s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.77s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.90s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.38s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.19s/it]Epoch 1/1: 2it [00:16,  7.81s/it]Epoch 1/1: 2it [00:17,  8.54s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 54256.33 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37969.62 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26212.43 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.66s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.22s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.19s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 20.00s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.80s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.23s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.63s/it]Epoch 1/1: 2it [00:12,  6.59s/it]Epoch 1/1: 2it [00:12,  6.36s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56516.22 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 31513.06 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 21897.64 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.46s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.49s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:52, 26.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:51<00:25, 25.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 21.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 22.82s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.84s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:03,  3.98s/it]Epoch 1/1: 2it [00:16,  8.74s/it]Epoch 1/1: 2it [00:16,  8.10s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 1.08s
Installed 98 packages in 5.17s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
Error executing job with overrides: ['run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k', 'run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42', 'training.seed=42', 'results_dir=.research/iteration1', 'wandb.mode=disabled', 'mode=trial']
Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-68fe90c9-15cfa3a574467e6734f215bb;0d390278-ce5f-4208-b2b0-2e41f91a1ec4)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py", line 364, in main
    run_training(cfg)
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py", line 113, in run_training
    tokenizer, train_ds, val_ds = prepare_datasets(cfg)
                                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/preprocess.py", line 105, in prepare_datasets
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=".cache/", use_fast=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1093, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
401 Client Error. (Request ID: Root=1-68fe90c9-15cfa3a574467e6734f215bb;0d390278-ce5f-4208-b2b0-2e41f91a1ec4)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k', 'results_dir=.research/iteration1', 'mode=trial']
Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py", line 41, in main
    subprocess.run(cmd, check=True, cwd=project_root)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k', 'run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42', 'training.seed=42', 'results_dir=.research/iteration1', 'wandb.mode=disabled', 'mode=trial']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 233ms
Installed 98 packages in 4.57s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4385.04 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 22315.24 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 17620.56 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24356.41 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:34<01:09, 34.85s/it]Fetching 3 files: 100%|██████████| 3/3 [00:34<00:00, 11.62s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.43s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.88s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.06s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.78s/it]Epoch 1/1: 2it [00:10,  5.21s/it]Epoch 1/1: 2it [00:11,  5.56s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 26692.23 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26380.57 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 19699.46 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.66s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.55s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 21.00s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 18.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 19.13s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.07s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:13, 13.47s/it]Epoch 1/1: 2it [00:17,  8.08s/it]Epoch 1/1: 2it [00:18,  9.01s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 35856.17 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 29744.71 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20169.71 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.54s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.54s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.26s/it]Epoch 1/1: 2it [00:11,  6.24s/it]Epoch 1/1: 2it [00:12,  6.06s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 831ms
Installed 98 packages in 4.52s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5427.55 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 25660.83 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 18501.32 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20424.82 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.15s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.38s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 18.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 19.23s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.96s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 11.00s/it]Epoch 1/1: 2it [00:17,  8.07s/it]Epoch 1/1: 2it [00:17,  8.62s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 28933.52 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 27680.28 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20760.59 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.48s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.83s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.66s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.44s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.72s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:11, 11.07s/it]Epoch 1/1: 2it [00:16,  7.47s/it]Epoch 1/1: 2it [00:16,  8.13s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 33675.73 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 30821.00 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23338.34 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:28<00:57, 28.57s/it]Fetching 3 files: 100%|██████████| 3/3 [00:28<00:00,  9.52s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.93s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:11, 11.50s/it]Epoch 1/1: 2it [00:15,  7.15s/it]Epoch 1/1: 2it [00:15,  7.92s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 770ms
Installed 98 packages in 4.73s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4345.51 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 17445.10 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23740.81 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:27<00:55, 27.84s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:28<00:11, 11.76s/it]Fetching 3 files: 100%|██████████| 3/3 [00:28<00:00,  9.45s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.07s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.02s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  5.00s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:15, 15.83s/it]Epoch 1/1: 2it [00:24, 11.38s/it]Epoch 1/1: 2it [00:24, 12.15s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 56514.14 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 36963.68 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 28163.30 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:58, 29.48s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.83s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.73s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.32s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.86s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.68s/it]Epoch 1/1: 2it [00:22, 11.50s/it]Epoch 1/1: 2it [00:22, 11.45s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55669.34 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 35232.57 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25177.71 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.78s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:30<00:12, 12.68s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.16s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.88s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.35s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:09,  9.27s/it]Epoch 1/1: 2it [00:14,  7.12s/it]Epoch 1/1: 2it [00:15,  7.52s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 948ms
Installed 98 packages in 4.57s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5414.92 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 19863.64 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23746.90 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.31s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.44s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.99s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.05s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.99s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.87s/it]Epoch 1/1: 2it [00:09,  4.51s/it]Epoch 1/1: 2it [00:09,  4.83s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56351.20 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 35694.76 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24107.97 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.65s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.89s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.22s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.41s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.95s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.97s/it]Epoch 1/1: 2it [00:14,  7.44s/it]Epoch 1/1: 2it [00:14,  7.28s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55023.62 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 39478.75 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 32015.31 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.20s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.40s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:24, 24.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.91s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:06,  6.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.47s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.82s/it]Epoch 1/1: 2it [00:13,  6.15s/it]Epoch 1/1: 2it [00:13,  6.64s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 369ms
Installed 98 packages in 4.57s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 8875.09 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26901.97 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26262.77 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.05s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:31<00:13, 13.01s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.48s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.91s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.40s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.55s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.07s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.78s/it]Epoch 1/1: 2it [00:13,  6.81s/it]Epoch 1/1: 2it [00:13,  6.72s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55958.25 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 33672.64 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24488.38 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:06, 33.27s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.09s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.70s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.26s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.47s/it]Epoch 1/1: 2it [00:17,  9.06s/it]Epoch 1/1: 2it [00:17,  8.58s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 57170.63 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 39646.54 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 27794.52 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.02s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.01s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:24<00:49, 24.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:48<00:24, 24.20s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 20.96s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.89s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.86s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.85s/it]Epoch 1/1: 2it [00:19,  9.43s/it]Epoch 1/1: 2it [00:20, 10.03s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 984ms
Installed 98 packages in 4.44s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 3849.22 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 20874.25 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 15231.65 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23613.92 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:06, 33.35s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.12s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 18.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 19.03s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.81s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.39s/it]Epoch 1/1: 2it [00:13,  6.18s/it]Epoch 1/1: 2it [00:13,  6.91s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 29063.74 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 28027.08 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 21246.59 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.01s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.34s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.53s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.57s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.91s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:13, 13.60s/it]Epoch 1/1: 2it [00:25, 12.58s/it]Epoch 1/1: 2it [00:25, 12.86s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 28346.31 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 27684.20 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22875.86 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:58, 29.19s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.73s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.80s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.79s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.24s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.55s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.52s/it]Epoch 1/1: 2it [00:10,  5.49s/it]Epoch 1/1: 2it [00:10,  5.46s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 810ms
Installed 98 packages in 4.74s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 3657.47 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 16120.66 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22286.26 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.25s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.42s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:47, 23.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:22, 22.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.75s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.84s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.03s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.76s/it]Epoch 1/1: 2it [00:13,  6.97s/it]Epoch 1/1: 2it [00:13,  6.85s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56949.52 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38917.99 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25694.30 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.85s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.62s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.48s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.86s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.80s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.33s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:09,  9.05s/it]Epoch 1/1: 2it [00:14,  6.91s/it]Epoch 1/1: 2it [00:14,  7.31s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56335.05 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 15332.96 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22286.89 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:28<00:57, 28.73s/it]Fetching 3 files: 100%|██████████| 3/3 [00:28<00:00,  9.58s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:50, 25.19s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:49<00:24, 24.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:07<00:00, 21.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:07<00:00, 22.49s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.08s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:06,  6.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.51s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.05s/it]Epoch 1/1: 2it [00:18,  9.57s/it]Epoch 1/1: 2it [00:18,  9.43s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 761ms
Installed 98 packages in 4.51s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 3739.61 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 16141.96 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24182.47 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.33s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:31<00:13, 13.18s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.50s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.98s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.90s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.37s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.55s/it]Epoch 1/1: 2it [00:16,  8.32s/it]Epoch 1/1: 2it [00:16,  8.27s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55717.53 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37692.35 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24789.42 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.25s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.42s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.80s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.92s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.50s/it]Epoch 1/1: 2it [00:11,  5.82s/it]Epoch 1/1: 2it [00:11,  5.83s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56098.96 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38933.88 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24467.05 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.33s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:32<00:13, 13.45s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.86s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:23, 23.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.06s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.11s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:06,  6.16s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.57s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.19s/it]Epoch 1/1: 2it [00:09,  4.52s/it]Epoch 1/1: 2it [00:09,  4.85s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 376ms
Installed 98 packages in 4.60s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4319.84 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 17633.38 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25072.35 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:28<00:57, 28.86s/it]Fetching 3 files: 100%|██████████| 3/3 [00:28<00:00,  9.62s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.69s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.98s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.50s/it]Epoch 1/1: 2it [00:16,  8.96s/it]Epoch 1/1: 2it [00:16,  8.35s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 57103.43 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 40316.69 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26717.35 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.45s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.48s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:24<00:48, 24.04s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:23, 23.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 20.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.44s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.91s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.92s/it]Epoch 1/1: 2it [00:24, 12.42s/it]Epoch 1/1: 2it [00:25, 12.58s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55982.90 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38940.85 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 9945.80 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.25s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:32<00:13, 13.35s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.79s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.55s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.55s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.91s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.41s/it]Epoch 1/1: 2it [00:12,  6.47s/it]Epoch 1/1: 2it [00:12,  6.24s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 629ms
Installed 98 packages in 4.54s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5570.07 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 22318.71 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 8843.94 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.36s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.79s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.29s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.40s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.02s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.98s/it]Epoch 1/1: 2it [00:20,  9.79s/it]Epoch 1/1: 2it [00:20, 10.33s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 56489.90 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37596.50 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24983.15 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:05, 32.60s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.88s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:24<00:48, 24.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:48<00:24, 24.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 20.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:05<00:00, 21.70s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.94s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.60s/it]Epoch 1/1: 2it [00:19, 10.23s/it]Epoch 1/1: 2it [00:19,  9.90s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55624.55 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 31250.68 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 27266.43 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.34s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.45s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.68s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.75s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.54s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.98s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.18s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.81s/it]Epoch 1/1: 2it [00:14,  6.36s/it]Epoch 1/1: 2it [00:14,  7.39s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 993ms
Installed 98 packages in 4.81s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 5756.58 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26527.75 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 19165.89 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22987.69 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.89s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.63s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.06s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 17.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.69s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.31s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.68s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.53s/it]Epoch 1/1: 2it [00:17,  9.22s/it]Epoch 1/1: 2it [00:17,  8.92s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 27562.64 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 27110.53 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24346.42 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.70s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.57s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.79s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:22, 22.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.97s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.66s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.92s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:13, 13.59s/it]Epoch 1/1: 2it [00:24, 11.81s/it]Epoch 1/1: 2it [00:24, 12.20s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 35958.78 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 31586.40 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 27568.95 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:05, 32.71s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.90s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.12s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.28s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.45s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.21s/it]Epoch 1/1: 2it [00:16,  8.14s/it]Epoch 1/1: 2it [00:16,  8.26s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 1.06s
Installed 98 packages in 6.35s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 3924.99 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 16556.77 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26807.29 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:47<01:35, 47.63s/it]Fetching 3 files: 100%|██████████| 3/3 [00:47<00:00, 15.88s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.23s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.21s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.90s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.16s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.37s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:15, 15.16s/it]Epoch 1/1: 2it [00:25, 12.23s/it]Epoch 1/1: 2it [00:25, 12.73s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 47291.52 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38443.75 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 3109.08 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:38<01:16, 38.04s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:38<00:16, 16.22s/it]Fetching 3 files: 100%|██████████| 3/3 [00:38<00:00, 13.00s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.28s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.09s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.87s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.30s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.79s/it]Epoch 1/1: 2it [00:24, 12.37s/it]Epoch 1/1: 2it [00:24, 12.50s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 57004.74 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38423.56 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25145.51 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:05, 32.86s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:33<00:13, 13.63s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.01s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.41s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.84s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.25s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.67s/it]Epoch 1/1: 2it [00:21, 10.16s/it]Epoch 1/1: 2it [00:21, 10.60s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 700ms
Installed 98 packages in 4.49s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 9212.70 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26933.88 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25222.73 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:07, 33.89s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.30s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.24s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.41s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.22s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.41s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.87s/it]Epoch 1/1: 2it [00:22, 11.58s/it]Epoch 1/1: 2it [00:23, 11.53s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55533.96 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 36416.52 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24486.66 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:34<01:09, 34.89s/it]Fetching 3 files: 100%|██████████| 3/3 [00:34<00:00, 11.63s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.20s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.88s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.29s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.23s/it]Epoch 1/1: 2it [00:07,  3.40s/it]Epoch 1/1: 2it [00:07,  3.59s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 52494.53 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37505.25 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24763.86 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.33s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.11s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.46s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.23s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.32s/it]Epoch 1/1: 2it [00:16,  8.89s/it]Epoch 1/1: 2it [00:16,  8.26s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 796ms
Installed 98 packages in 5.07s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4291.87 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 21902.89 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 16034.34 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25066.76 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:05, 32.95s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.98s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.02s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.90s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.95s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.04s/it]Epoch 1/1: 2it [00:16,  8.18s/it]Epoch 1/1: 2it [00:17,  8.52s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 45688.99 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 35484.36 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24313.11 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.76s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.59s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.16s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.98s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.31s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.72s/it]Epoch 1/1: 2it [00:22, 11.52s/it]Epoch 1/1: 2it [00:22, 11.47s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 53667.86 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37325.91 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23917.88 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.34s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.78s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.99s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.15s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.96s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.15s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.82s/it]Epoch 1/1: 2it [00:24, 12.36s/it]Epoch 1/1: 2it [00:24, 12.49s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 1.04s
Installed 98 packages in 5.01s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4195.92 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 21904.87 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 15971.14 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22226.42 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:07, 33.83s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:33<00:14, 14.00s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.32s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:22, 22.96s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.56s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.54s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.72s/it]Epoch 1/1: 2it [00:11,  5.74s/it]Epoch 1/1: 2it [00:11,  5.99s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 31863.26 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 29804.71 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 28539.93 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:35<01:11, 35.94s/it]Fetching 3 files: 100%|██████████| 3/3 [00:35<00:00, 11.98s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.71s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.89s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.01s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.23s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:03,  3.87s/it]Epoch 1/1: 2it [00:15,  8.73s/it]Epoch 1/1: 2it [00:16,  8.13s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 29574.79 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 19403.22 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23076.56 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.66s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.22s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.84s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.29s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.27s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.31s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.60s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:11, 11.05s/it]Epoch 1/1: 2it [00:18,  8.67s/it]Epoch 1/1: 2it [00:18,  9.15s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 792ms
Installed 98 packages in 4.78s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 5824.25 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 21186.68 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23326.22 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.77s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.26s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.51s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.85s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.35s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.95s/it]Epoch 1/1: 2it [00:19,  8.96s/it]Epoch 1/1: 2it [00:19,  9.64s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 46516.86 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 35754.40 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23400.67 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.11s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.04s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:24<00:48, 24.00s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:48<00:24, 24.48s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 21.31s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:06<00:00, 22.12s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.97s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.93s/it]Epoch 1/1: 2it [00:18,  8.81s/it]Epoch 1/1: 2it [00:19,  9.51s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 52702.19 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 36957.79 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24576.38 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.93s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.64s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.59s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.74s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.21s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.88s/it]Epoch 1/1: 2it [00:24, 12.41s/it]Epoch 1/1: 2it [00:25, 12.56s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 851ms
Installed 98 packages in 4.67s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4831.86 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 19373.12 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22779.28 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.20s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:31<00:13, 13.19s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.59s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.46s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.39s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.98s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.98s/it]Epoch 1/1: 2it [00:20,  9.70s/it]Epoch 1/1: 2it [00:20, 10.25s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 45407.05 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 36548.34 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25730.03 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:34<01:09, 34.88s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:35<00:14, 14.49s/it]Fetching 3 files: 100%|██████████| 3/3 [00:35<00:00, 11.70s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 22.00s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.08s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.24s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.83s/it]Epoch 1/1: 2it [00:16,  7.56s/it]Epoch 1/1: 2it [00:16,  8.42s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 47010.38 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 36670.10 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23316.89 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:05, 32.74s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.91s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.24s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.29s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.29s/it]Epoch 1/1: 2it [00:14,  6.46s/it]Epoch 1/1: 2it [00:14,  7.09s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 1.00s
Installed 98 packages in 5.45s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4668.10 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 18506.94 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24999.03 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.70s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.57s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.12s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.95s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.87s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.35s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.36s/it]Epoch 1/1: 2it [00:17,  9.31s/it]Epoch 1/1: 2it [00:17,  8.78s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55977.29 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 31190.43 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26157.07 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.34s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.45s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:47, 23.87s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:23, 23.68s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 20.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.52s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.03s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:05,  6.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.47s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.74s/it]Epoch 1/1: 2it [00:14,  6.63s/it]Epoch 1/1: 2it [00:14,  7.32s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 44508.13 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 33522.97 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26307.79 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.06s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.02s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.79s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.05s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.61s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.14s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.77s/it]Epoch 1/1: 2it [00:24, 12.36s/it]Epoch 1/1: 2it [00:24, 12.48s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 621ms
Installed 98 packages in 5.06s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5492.41 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 20267.07 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26780.36 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:06, 33.24s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.08s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.66s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.78s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.96s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.20s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:02,  2.65s/it]Epoch 1/1: 2it [00:06,  3.29s/it]Epoch 1/1: 2it [00:06,  3.26s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55842.65 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 39479.63 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 28218.12 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.80s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.27s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:50, 25.23s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:48<00:23, 23.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 20.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.49s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:06,  6.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.46s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.24s/it]Epoch 1/1: 2it [00:11,  5.65s/it]Epoch 1/1: 2it [00:11,  5.82s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56482.22 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38649.47 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25256.34 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:38<01:16, 38.34s/it]Fetching 3 files: 100%|██████████| 3/3 [00:38<00:00, 12.78s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.55s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.46s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.31s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.87s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.35s/it]Epoch 1/1: 2it [00:15,  8.45s/it]Epoch 1/1: 2it [00:16,  8.05s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 951ms
Installed 98 packages in 4.76s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 7042.16 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 29849.82 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 21501.95 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 18453.26 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:07, 33.83s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.28s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.95s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 17.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.71s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.21s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.57s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:03,  3.10s/it]Epoch 1/1: 2it [00:15,  8.40s/it]Epoch 1/1: 2it [00:15,  7.71s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 26207.15 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 27038.47 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23548.69 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:06, 33.29s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.10s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.94s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.98s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.01s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.97s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.23s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.35s/it]Epoch 1/1: 2it [00:19, 10.16s/it]Epoch 1/1: 2it [00:19,  9.87s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 34195.88 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 30667.62 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25697.45 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.95s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:31<00:13, 13.35s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.66s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.76s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.22s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.93s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.65s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.45s/it]Epoch 1/1: 2it [00:10,  5.05s/it]Epoch 1/1: 2it [00:11,  5.52s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 793ms
Installed 98 packages in 4.64s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5188.00 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 25865.76 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 18376.18 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 19694.90 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.59s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.53s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 17.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 18.66s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.40s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.59s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.52s/it]Epoch 1/1: 2it [00:11,  5.21s/it]Epoch 1/1: 2it [00:11,  5.66s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 34727.15 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 29304.20 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20909.77 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.33s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.11s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:27<00:55, 27.83s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:54<00:26, 26.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:10<00:00, 22.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:10<00:00, 23.60s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.20s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.39s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:09,  9.38s/it]Epoch 1/1: 2it [00:21, 11.00s/it]Epoch 1/1: 2it [00:21, 10.88s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 27019.15 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26351.87 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 19698.72 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.74s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.91s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 17.98s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.72s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.92s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.71s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.03s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.72s/it]Epoch 1/1: 2it [00:13,  7.03s/it]Epoch 1/1: 2it [00:14,  7.10s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 751ms
Installed 98 packages in 4.61s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4068.55 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 22108.62 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 16018.35 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 22586.94 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:34<01:08, 34.24s/it]Fetching 3 files: 100%|██████████| 3/3 [00:34<00:00, 11.41s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.52s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 17.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:55<00:00, 18.48s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.22s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.56s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:13, 13.10s/it]Epoch 1/1: 2it [00:21, 10.27s/it]Epoch 1/1: 2it [00:21, 10.80s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 28307.19 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26793.54 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25770.29 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:36<01:12, 36.07s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:36<00:14, 14.94s/it]Fetching 3 files: 100%|██████████| 3/3 [00:36<00:00, 12.07s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:51, 25.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:51<00:25, 26.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:10<00:00, 22.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:10<00:00, 23.61s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:18,  9.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:25<00:00,  8.38s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.24s/it]Epoch 1/1: 2it [00:10,  5.67s/it]Epoch 1/1: 2it [00:11,  5.58s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 25861.27 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26021.10 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 21583.48 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.24s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.75s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.84s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.91s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.21s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.20s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.50s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.34s/it]Epoch 1/1: 2it [00:20, 10.53s/it]Epoch 1/1: 2it [00:20, 10.32s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 880ms
Installed 98 packages in 5.68s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 8149.77 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 25269.89 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 31008.89 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:00, 30.39s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:31<00:13, 13.08s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.45s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:47, 23.70s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:47<00:23, 23.56s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.23s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.87s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.36s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:03,  3.87s/it]Epoch 1/1: 2it [00:15,  8.59s/it]Epoch 1/1: 2it [00:15,  7.95s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 58546.41 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 39363.70 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24973.63 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.51s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.17s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:47, 23.96s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.33s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.83s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.28s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.78s/it]Epoch 1/1: 2it [00:13,  7.33s/it]Epoch 1/1: 2it [00:14,  7.02s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 57210.40 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 40454.90 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 28276.46 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:07, 33.57s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.19s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:25<00:51, 25.83s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:50<00:25, 25.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 21.84s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 22.84s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.93s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.22s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.42s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.82s/it]Epoch 1/1: 2it [00:16,  8.44s/it]Epoch 1/1: 2it [00:17,  8.59s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 707ms
Installed 98 packages in 4.75s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 43861.62 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 31884.25 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 27807.29 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:06, 33.01s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.00s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.45s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.27s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.03s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.94s/it]Epoch 1/1: 2it [00:18,  9.61s/it]Epoch 1/1: 2it [00:18,  9.42s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 54917.60 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 38398.75 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26288.23 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:05, 32.56s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.77s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:23, 23.03s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.77s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.77s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.08s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.28s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.86s/it]Epoch 1/1: 2it [00:10,  5.15s/it]Epoch 1/1: 2it [00:10,  5.18s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56368.61 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 39222.45 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 29909.75 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.26s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.75s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.41s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.91s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.69s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  4.92s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.12s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.77s/it]Epoch 1/1: 2it [00:22, 10.89s/it]Epoch 1/1: 2it [00:22, 11.23s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 731ms
Installed 98 packages in 5.20s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5483.26 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 21955.99 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24610.61 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.51s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:30<00:12, 12.98s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.31s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.30s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:44<00:22, 22.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 19.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.35s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.85s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.33s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.90s/it]Epoch 1/1: 2it [00:16,  8.26s/it]Epoch 1/1: 2it [00:16,  8.43s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 46731.94 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 14558.31 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23030.44 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:58, 29.42s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:30<00:12, 12.86s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.23s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.77s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 19.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:00<00:00, 20.30s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.06s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.26s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.89s/it]Epoch 1/1: 2it [00:18,  9.94s/it]Epoch 1/1: 2it [00:19,  9.55s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 57355.39 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 32838.81 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23114.21 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.81s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.60s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:24<00:49, 24.85s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:48<00:24, 24.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:07<00:00, 21.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:07<00:00, 22.39s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.73s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  5.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.13s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:07,  7.07s/it]Epoch 1/1: 2it [00:11,  5.56s/it]Epoch 1/1: 2it [00:11,  5.87s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 688ms
Installed 98 packages in 5.54s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 8712.00 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 27328.17 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26682.23 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.59s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.53s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.47s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.00s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.30s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:03,  3.83s/it]Epoch 1/1: 2it [00:08,  4.10s/it]Epoch 1/1: 2it [00:08,  4.15s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 54572.71 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 33231.86 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25232.24 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.48s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.49s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 18.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 19.30s/it]
'(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: ad40c95d-b290-4010-a0a3-246578679a29)')' thrown while requesting HEAD https://huggingface.co/georgesung/llama2_7b_chat_uncensored/resolve/main/generation_config.json
Retrying in 1s [Retry 1/5].
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.77s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.84s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.05s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.26s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:05,  5.86s/it]Epoch 1/1: 2it [00:17,  9.52s/it]Epoch 1/1: 2it [00:18,  9.04s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 57181.67 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 39422.35 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24003.11 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.93s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.98s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.42s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.63s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.21s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.35s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.80s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.24s/it]Epoch 1/1: 2it [00:10,  4.50s/it]Epoch 1/1: 2it [00:10,  5.12s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 395ms
Installed 98 packages in 4.68s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 8422.23 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26223.80 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 28600.13 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:33<01:06, 33.10s/it]Fetching 3 files: 100%|██████████| 3/3 [00:33<00:00, 11.03s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.24s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.38s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.16s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.78s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.97s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.76s/it]Epoch 1/1: 2it [00:17,  8.91s/it]Epoch 1/1: 2it [00:17,  8.66s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 52532.44 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 30719.07 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25633.59 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.42s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.81s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:45, 22.70s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:23, 23.00s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.11s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.86s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.34s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.93s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.28s/it]Epoch 1/1: 2it [00:22, 10.92s/it]Epoch 1/1: 2it [00:22, 11.21s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56083.95 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37322.56 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26117.55 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.77s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.26s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.84s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.18s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.89s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.67s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.21s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:08,  8.33s/it]Epoch 1/1: 2it [00:20, 10.54s/it]Epoch 1/1: 2it [00:20, 10.27s/it]
