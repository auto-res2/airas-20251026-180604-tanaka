
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path} mode=full

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} mode=trial
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `mode`: Execution mode (required parameter)
  * `mode=trial`: Lightweight execution for validation (epochs=1, batches limited to 1-2, wandb.mode=disabled, optuna.n_trials=0)
  * `mode=full`: Full experiment execution (wandb.mode=online, full epochs, full Optuna trials)
  * **Code must automatically configure based on mode (e.g., `if cfg.mode == "trial": cfg.wandb.mode = "disabled"; cfg.optuna.n_trials = 0` elif `cfg.mode == "full": cfg.wandb.mode = "online"`)**

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Optuna Integration**: If using Optuna for hyperparameter search, DO NOT log intermediate trial results to WandB - only train once with the best hyperparameters after optimization completes and log that final run
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
    - Use CONSISTENT metric names across train.py and evaluate.py (e.g., if train.py logs "train_acc", evaluate.py MUST use run.history(keys=["train_acc",...]))
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `config/config.yaml` (in repository root)
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- **Mode handling**: Automatically configure based on `cfg.mode`:
  * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1, etc.
  * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251023-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```
- `WANDB_API_KEY` environment variable is automatically available for authentication

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Mode-Based Behavior**: Code must automatically configure based on `cfg.mode` ("trial" vs "full")
   - `mode=trial`: Set `cfg.wandb.mode="disabled"`, `cfg.optuna.n_trials=0`, epochs=1, limited batches
   - `mode=full`: Set `cfg.wandb.mode="online"`, use full configuration
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback


**Previous Validation Issue**: Fails multiple mandatory requirements:
1. Trial-mode workload reduction is incomplete – epochs are set to 1 but the dataloaders still iterate over the full training / validation sets (requirement 9 demands limiting to 1-2 batches and a small eval subset).
2. Evaluation script only saves combined summaries and one bar chart; it does not compute derived / secondary metrics (e.g. improvement-rate) nor perform statistical‐significance tests as explicitly required in step-2 of requirement 8.
Without these fixes the code cannot be accepted for immediate execution.
**Action Required**: Address this issue in the implementation.

**Previous Code (for reference)**:
{"config_yaml": "defaults:\n  - _self_\n  - run: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k  # default, override via CLI\n\n# Global parameters -----------------------------------------------------------\nmode: full               # trial / full (override via CLI)\nresults_dir: ./results   # where to save checkpoints \u0026 analysis\n\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online           # auto-overridden depending on mode\n\n# Placeholder \u2013 content injected from run YAML\nrun: {}\n", "evaluate_py": "\"\"\"Independent evaluation \u0026 visualisation script.\n\nUsage:\n    uv run python -m src.evaluate results_dir=/path/to/out run_ids=\u0027[\"run-1\",\"run-2\"]\u0027\n\nThe script intentionally does *not* rely on Hydra so that it can be called\nas-is by CI workflows.\n\"\"\"\nimport json\nimport os\nimport sys\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Argument parsing (key=value pairs as specified in the CLI contract)\n# -----------------------------------------------------------------------------\n\ndef parse_cli_kv() -\u003e Dict[str, str]:\n    params: Dict[str, str] = {}\n    for token in sys.argv[1:]:\n        if \"=\" not in token:\n            raise ValueError(\n                f\"Invalid argument \u0027{token}\u0027. Expected key=value style as per spec.\"\n            )\n        k, v = token.split(\"=\", 1)\n        params[k] = v\n    required = {\"results_dir\", \"run_ids\"}\n    missing = required - params.keys()\n    if missing:\n        raise ValueError(f\"Missing required arguments: {missing}\")\n    return params\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef save_json(obj, path):\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef plot_learning_curve(history_df, run_id: str, save_dir: str) -\u003e str:\n    fig, ax = plt.subplots(figsize=(8, 5))\n    possible_cols = [\n        (\"train/loss\", \"train/loss\"),\n        (\"train/acc\", \"train/acc\"),\n        (\"val/val_pairwise_accuracy\", \"val/accuracy\"),\n    ]\n    for col, label in possible_cols:\n        if col in history_df.columns:\n            ax.plot(history_df[\"step\"], history_df[col], label=label)\n    ax.set_xlabel(\"Step\")\n    ax.set_title(f\"Learning Curves \u2013 {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = os.path.join(save_dir, f\"{run_id}_learning_curve.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(summary: Dict[str, float], run_id: str, save_dir: str) -\u003e str:\n    tp = summary.get(\"val_true_pos\", summary.get(\"val_correct\", 0))\n    fp = summary.get(\"val_false_pos\", 0)\n    tn = summary.get(\"val_true_neg\", 0)\n    fn = summary.get(\"val_false_neg\", 0)\n    matrix = [[tp, fp], [fn, tn]]\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n    ax.set_xlabel(\"Predicted + / -\")\n    ax.set_ylabel(\"Actual + / -\")\n    ax.set_title(f\"Confusion Matrix \u2013 {run_id}\")\n    fig.tight_layout()\n    path = os.path.join(save_dir, f\"{run_id}_confusion_matrix.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n# -----------------------------------------------------------------------------\n# Per-run processing\n# -----------------------------------------------------------------------------\n\ndef process_single_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: str):\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history = run.history()  # pd.DataFrame\n    summary = run.summary._json_dict\n    config = dict(run.config)\n\n    os.makedirs(out_dir, exist_ok=True)\n    metrics_path = os.path.join(out_dir, \"metrics.json\")\n    save_json({\"summary\": summary, \"config\": config}, metrics_path)\n\n    figs: List[str] = []\n    figs.append(plot_learning_curve(history, run_id, out_dir))\n    figs.append(plot_confusion_matrix(summary, run_id, out_dir))\n    return {\n        \"metrics_path\": metrics_path,\n        \"figs\": figs,\n        \"summary\": summary,\n    }\n\n# -----------------------------------------------------------------------------\n# Aggregated comparison across runs\n# -----------------------------------------------------------------------------\n\ndef aggregated_analysis(per_run: Dict[str, Dict], comp_dir: str) -\u003e List[str]:\n    os.makedirs(comp_dir, exist_ok=True)\n\n    # Save combined summaries\n    combined = {rid: d[\"summary\"] for rid, d in per_run.items()}\n    save_json(combined, os.path.join(comp_dir, \"aggregated_metrics.json\"))\n\n    # Example bar-chart: best val accuracy\n    run_ids = list(per_run.keys())\n    accuracies = [d[\"summary\"].get(\"best_val_pairwise_accuracy\", 0) for d in per_run.values()]\n\n    fig, ax = plt.subplots(figsize=(max(4, len(run_ids) * 1.6), 4))\n    sns.barplot(x=run_ids, y=accuracies, palette=\"viridis\", ax=ax)\n    for i, v in enumerate(accuracies):\n        ax.text(i, v + 0.005, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    ax.set_ylim(0, 1)\n    ax.set_ylabel(\"Best Val Pairwise Acc.\")\n    ax.set_title(\"Cross-run Comparison\")\n    fig.tight_layout()\n\n    bar_path = os.path.join(comp_dir, \"comparison_accuracy_bar_chart.pdf\")\n    fig.savefig(bar_path)\n    plt.close(fig)\n    return [bar_path]\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_cli_kv()\n    results_dir, run_ids_json = args[\"results_dir\"], args[\"run_ids\"]\n    run_ids = json.loads(run_ids_json)\n\n    # Global WandB config\n    with open(os.path.join(\"config\", \"config.yaml\")) as f:\n        import yaml\n\n        cfg = yaml.safe_load(f)\n    entity, project = cfg[\"wandb\"][\"entity\"], cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_run = {}\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        run_dir = os.path.join(results_dir, rid)\n        per_run[rid] = process_single_run(api, entity, project, rid, run_dir)\n        print(f\"Saved metrics for {rid} \u2192 {per_run[rid][\u0027metrics_path\u0027]}\")\n        for fp in per_run[rid][\"figs\"]:\n            print(fp)\n\n    comp_dir = os.path.join(results_dir, \"comparison\")\n    comp_figs = aggregated_analysis(per_run, comp_dir)\n    print(f\"Aggregated outputs: {comp_dir}\")\n    for fp in comp_figs:\n        print(fp)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "import subprocess\nimport hydra\nfrom omegaconf import OmegaConf\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # Mode-specific tweaks ------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Iterate over seeds --------------------------------------------------\n    for seed in cfg.training.seed_list:\n        run_id_seed = f\"{cfg.run.run_id}-seed{seed}\"\n        overrides = [\n            f\"run.run_id={run_id_seed}\",\n            f\"training.seed={seed}\",\n            f\"results_dir={cfg.results_dir}\",\n            f\"wandb.mode={cfg.wandb.mode}\",\n            f\"mode={cfg.mode}\",\n        ]\n        cmd = [\"python\", \"-u\", \"-m\", \"src.train\"] + overrides\n        print(\"Launching:\", \" \".join(cmd))\n        subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model loading \u0026 log-prob computation utilities.\"\"\"\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\n\n__all__ = [\"load_models\", \"compute_logps\"]\n\n# -----------------------------------------------------------------------------\n# Loading 4-bit QLoRA model \u0026 frozen reference\n# -----------------------------------------------------------------------------\n\ndef load_models(cfg, tokenizer):\n    bnb_config = None\n    if cfg.model.get(\"load_in_4bit\", False):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    model.gradient_checkpointing_enable()\n\n    if cfg.model.get(\"peft\"):\n        lora_cfg = LoraConfig(\n            r=cfg.model.peft.lora_r,\n            lora_alpha=cfg.model.peft.lora_alpha,\n            lora_dropout=cfg.model.peft.lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        model = get_peft_model(model, lora_cfg)\n\n    # Frozen reference model \u2013 no LoRA\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    ref_model.eval()\n    for p in ref_model.parameters():\n        p.requires_grad_(False)\n\n    return model, ref_model\n\n# -----------------------------------------------------------------------------\n# Log-prob computation\n# -----------------------------------------------------------------------------\n\ndef _gather_log_probs(logits: torch.Tensor, input_ids: torch.Tensor):\n    logp = torch.nn.functional.log_softmax(logits, dim=-1)\n    logp = logp[:, :-1, :]  # shift so that next-token logits align with targets\n    target = input_ids[:, 1:]\n    gathered = torch.gather(logp, 2, target.unsqueeze(-1)).squeeze(-1)\n    return gathered\n\n\ndef _forward(model, batch_part, device):\n    input_ids = batch_part[\"input_ids\"].to(device)\n    attention_mask = batch_part[\"attention_mask\"].to(device)\n    response_mask = batch_part[\"response_mask\"].to(device)[:, 1:]  # align\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    log_probs = _gather_log_probs(outputs.logits, input_ids)\n    seq_logp = (log_probs * response_mask).sum(dim=1) / response_mask.sum(dim=1)\n    return seq_logp\n\n\ndef compute_logps(model, ref_model, batch, device):\n    \"\"\"Returns tuple: (logp_pos, logp_neg, logp_pos_ref, logp_neg_ref).\"\"\"\n    pos, neg = batch[\"pos\"], batch[\"neg\"]\n\n    # Reference model (no grad)\n    with torch.no_grad():\n        logp_pos_ref = _forward(ref_model, pos, device)\n        logp_neg_ref = _forward(ref_model, neg, device)\n\n    # Trainable model (grad enabled)\n    logp_pos = _forward(model, pos, device)\n    logp_neg = _forward(model, neg, device)\n\n    return logp_pos, logp_neg, logp_pos_ref, logp_neg_ref\n", "preprocess_py": "import datasets\nfrom datasets import load_dataset\nfrom typing import List, Tuple\nimport torch\nfrom transformers import AutoTokenizer\n\n# -----------------------------------------------------------------------------\n# Dataset definitions\n# -----------------------------------------------------------------------------\n\nclass PairPreferenceDataset(torch.utils.data.Dataset):\n    \"\"\"Wraps an HF dataset providing (prompt, pos, neg) per item.\"\"\"\n\n    def __init__(self, hf_ds: datasets.Dataset, tokenizer: AutoTokenizer):\n        self.hf_ds = hf_ds\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.hf_ds)\n\n    def __getitem__(self, idx):\n        sample = self.hf_ds[idx]\n        # Prompt field\n        prompt = (\n            sample.get(\"prompt\")\n            or sample.get(\"instruction\")\n            or sample.get(\"question\")\n            or \"\"\n        )\n        # Positive / negative completions according to dataset conventions\n        if {\n            \"completion_a\",\n            \"completion_b\",\n            \"choice\",\n        }.issubset(sample.keys()):\n            pos, neg = (\n                (sample[\"completion_a\"], sample[\"completion_b\"])\n                if sample[\"choice\"] == 0\n                else (sample[\"completion_b\"], sample[\"completion_a\"])\n            )\n        elif {\"chosen\", \"rejected\"}.issubset(sample.keys()):\n            pos, neg = sample[\"chosen\"], sample[\"rejected\"]\n        else:\n            raise KeyError(\"Dataset sample does not contain recognised preference fields\")\n        return {\"prompt\": prompt, \"pos\": pos, \"neg\": neg}\n\n\nclass PreferenceCollator:\n    \"\"\"Tokenises prompt+response pairs and builds masks.\"\"\"\n\n    def __init__(self, tokenizer: AutoTokenizer, max_length: int = 2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.eos = tokenizer.eos_token or \"\u003c/s\u003e\"\n\n    def _build(self, prompts: List[str], responses: List[str]):\n        joined = [p + self.eos + r + self.eos for p, r in zip(prompts, responses)]\n        tokenised = self.tokenizer(\n            joined,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        # Build response mask (prompt tokens =0, response tokens =1)\n        mask = torch.zeros_like(tokenised[\"input_ids\"], dtype=torch.bool)\n        for i, (p, r) in enumerate(zip(prompts, responses)):\n            prompt_len = len(self.tokenizer(p + self.eos)[\"input_ids\"])\n            seq_len = tokenised[\"attention_mask\"][i].sum().item()\n            mask[i, prompt_len : seq_len] = 1\n        tokenised[\"response_mask\"] = mask\n        return tokenised\n\n    def __call__(self, batch: List[dict]):\n        prompts = [b[\"prompt\"] for b in batch]\n        pos_responses = [b[\"pos\"] for b in batch]\n        neg_responses = [b[\"neg\"] for b in batch]\n        return {\n            \"pos\": self._build(prompts, pos_responses),\n            \"neg\": self._build(prompts, neg_responses),\n        }\n\n\n# -----------------------------------------------------------------------------\n# Public helper to get (tokenizer, train_ds, val_ds)\n# -----------------------------------------------------------------------------\n\ndef prepare_datasets(cfg) -\u003e Tuple[AutoTokenizer, torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\", use_fast=True)\n\n    raw = load_dataset(cfg.dataset.name, cache_dir=\".cache/\")\n    if \"train\" in raw and \"validation\" in raw:\n        train_raw, val_raw = raw[\"train\"], raw[\"validation\"]\n    else:\n        split = raw[\"train\"].train_test_split(test_size=cfg.dataset.split.val)\n        train_raw, val_raw = split[\"train\"], split[\"test\"]\n\n    train_ds = PairPreferenceDataset(train_raw, tokenizer)\n    val_ds = PairPreferenceDataset(val_raw, tokenizer)\n    return tokenizer, train_ds, val_ds\n", "pyproject_toml": "[project]\nname = \"td_dpo_experiments\"\nversion = \"0.1.0\"\ndescription = \"Temperature-Decoupled DPO experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0\"\ntransformers = \"\u003e=4.34.0\"\ndatasets = \"\u003e=2.14.0\"\npeft = \"\u003e=0.5.0\"\nbitsandbytes = \"*\"\nhydra-core = \"\u003e=1.3.2\"\nwandb = \"\u003e=0.15.12\"\noptuna = \"\u003e=3.3.0\"\nmatplotlib = \"\u003e=3.7.0\"\nseaborn = \"\u003e=0.13.0\"\n", "train_py": "import os\nimport math\nimport random\nfrom typing import Dict, Any\n\nimport hydra\nimport torch\nimport wandb\nimport optuna\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import prepare_datasets, PreferenceCollator\nfrom src.model import load_models, compute_logps\n\n# -----------------------------------------------------------------------------\n# Loss functions\n# -----------------------------------------------------------------------------\n\ndef dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n) -\u003e torch.Tensor:\n    \"\"\"Standard DPO objective.\"\"\"\n    preference_term = -torch.log(torch.sigmoid(logp_pos - logp_neg)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n\ndef td_dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n    tau: float,\n) -\u003e torch.Tensor:\n    \"\"\"Temperature\u2013Decoupled DPO.\"\"\"\n    preference_term = -torch.log(torch.sigmoid((logp_pos - logp_neg) / tau)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n# -----------------------------------------------------------------------------\n# Validation loop\n# -----------------------------------------------------------------------------\n\ndef evaluate(\n    model: torch.nn.Module,\n    ref_model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n) -\u003e Dict[str, Any]:\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            logp_pos, logp_neg, *_ = compute_logps(model, ref_model, batch, device)\n            correct += (logp_pos \u003e logp_neg).sum().item()\n            total += logp_pos.size(0)\n    model.train()\n    return {\"val_pairwise_accuracy\": correct / total if total \u003e 0 else 0.0}\n\n# -----------------------------------------------------------------------------\n# Training loop (single run, single seed)\n# -----------------------------------------------------------------------------\n\ndef run_training(cfg, trial=None):\n    # ------------------------------------------------------------------\n    # Reproducibility\n    # ------------------------------------------------------------------\n    torch.manual_seed(cfg.training.seed)\n    random.seed(cfg.training.seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(cfg.training.seed)\n\n    # ------------------------------------------------------------------\n    # Data \u0026 model\n    # ------------------------------------------------------------------\n    tokenizer, train_ds, val_ds = prepare_datasets(cfg)\n    model, ref_model = load_models(cfg, tokenizer)\n\n    collator = PreferenceCollator(\n        tokenizer, max_length=cfg.dataset.preprocessing.max_length\n    )\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=False,\n        num_workers=2,\n        collate_fn=collator,\n    )\n\n    # ------------------------------------------------------------------\n    # Optimiser \u0026 scheduler\n    # ------------------------------------------------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    params = [\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(params, lr=cfg.training.learning_rate)\n\n    num_update_steps_per_epoch = math.ceil(\n        len(train_loader) / cfg.training.gradient_accumulation_steps\n    )\n    max_steps = cfg.training.epochs * num_update_steps_per_epoch\n\n    if cfg.training.lr_scheduler_type == \"cosine\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n\n    # ------------------------------------------------------------------\n    # WandB init (disabled in trial \u0026 Optuna search)\n    # ------------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB URL: {wandb_run.url}\")\n    else:\n        wandb_run = None\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n    # ------------------------------------------------------------------\n    # Move models to device\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    ref_model.to(device)\n    ref_model.eval()\n\n    # ------------------------------------------------------------------\n    # Training\n    # ------------------------------------------------------------------\n    best_val_acc, global_step = 0.0, 0\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        epoch_loss, correct, total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar, start=1):\n            logp_pos, logp_neg, logp_pos_ref, logp_neg_ref = compute_logps(\n                model, ref_model, batch, device\n            )\n            # choose loss type\n            if cfg.loss.name == \"dpo_loss\":\n                loss = dpo_loss(\n                    logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, cfg.loss.beta\n                )\n            else:\n                loss = td_dpo_loss(\n                    logp_pos,\n                    logp_neg,\n                    logp_pos_ref,\n                    logp_neg_ref,\n                    cfg.loss.beta,\n                    cfg.loss.tau,\n                )\n            loss = loss / cfg.training.gradient_accumulation_steps\n            loss.backward()\n            epoch_loss += loss.item()\n\n            correct += (logp_pos \u003e logp_neg).sum().item()\n            total += logp_pos.size(0)\n\n            if step % cfg.training.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                global_step += 1\n\n                if wandb_run and global_step % cfg.training.logging_steps == 0:\n                    wandb.log(\n                        {\n                            \"train/loss\": loss.item()\n                            * cfg.training.gradient_accumulation_steps,\n                            \"train/acc\": correct / total,\n                            \"lr\": scheduler.get_last_lr()[0],\n                            \"step\": global_step,\n                        },\n                        step=global_step,\n                    )\n\n                if (\n                    cfg.training.evaluation_strategy == \"steps\"\n                    and global_step % cfg.training.eval_steps == 0\n                ):\n                    val_metrics = evaluate(model, ref_model, val_loader, device)\n                    if wandb_run:\n                        wandb.log({f\"val/{k}\": v for k, v in val_metrics.items()}, step=global_step)\n                    if val_metrics[\"val_pairwise_accuracy\"] \u003e best_val_acc:\n                        best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n                        save_best(model, tokenizer, cfg)\n                    model.train()\n\n        # End-of-epoch validation\n        val_metrics = evaluate(model, ref_model, val_loader, device)\n        if wandb_run:\n            wandb.log({f\"val/{k}\": v for k, v in val_metrics.items()}, step=global_step)\n        if val_metrics[\"val_pairwise_accuracy\"] \u003e best_val_acc:\n            best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n            save_best(model, tokenizer, cfg)\n\n    # ------------------------------------------------------------------\n    # Wrap-up\n    # ------------------------------------------------------------------\n    if wandb_run:\n        wandb_run.summary[\"best_val_pairwise_accuracy\"] = best_val_acc\n        wandb_run.finish()\n\n    return best_val_acc\n\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef save_best(model, tokenizer, cfg):\n    save_path = os.path.join(cfg.results_dir, cfg.run.run_id, \"best\")\n    os.makedirs(save_path, exist_ok=True)\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg):\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))  # deepcopy\n\n    # Sample hyper-parameters\n    for hp_name, hp_space in cfg.optuna.search_space.items():\n        if hp_space.type == \"categorical\":\n            sampled = trial.suggest_categorical(hp_name, hp_space.choices)\n        elif hp_space.type == \"loguniform\":\n            sampled = trial.suggest_float(hp_name, hp_space.low, hp_space.high, log=True)\n        else:\n            raise ValueError(f\"Unsupported space type: {hp_space.type}\")\n        # write back to cfg (supports both loss \u0026 training params)\n        target_field = f\"loss.{hp_name}\" if hp_name in [\"beta\", \"tau\"] else f\"training.{hp_name}\"\n        OmegaConf.update(cfg, target_field, sampled, merge=False)\n\n    # Disable WandB during search\n    cfg.wandb.mode = \"disabled\"\n    val_acc = run_training(cfg)\n    return -val_acc  # minimise negative accuracy\n\n\n# -----------------------------------------------------------------------------\n# CLI entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n    os.makedirs(os.path.join(cfg.results_dir, cfg.run.run_id), exist_ok=True)\n\n    # Mode adjustments --------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.eval_steps = max(cfg.training.eval_steps, 10)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Optuna hyper-param search -----------------------------------------\n    if cfg.optuna.n_trials and cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        print(\"[Optuna] Best params:\", study.best_params)\n        # Update cfg with best params for final training\n        for k, v in study.best_params.items():\n            OmegaConf.update(\n                cfg,\n                f\"loss.{k}\" if k in [\"beta\", \"tau\"] else f\"training.{k}\",\n                v,\n                merge=False,\n            )\n\n    # Final training -----------------------------------------------------\n    run_training(cfg)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Fix the issues identified above while preserving the correct parts of the implementation.



# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "In DPO-style preference optimization the single hyper-parameter β simultaneously (1) scales the preference loss curvature and (2) weights the KL-divergence regulariser.  This coupling makes training unstable: small β under-regularises but also flattens the preference loss, while large β over-regularises and explodes the preference-loss gradient.  Reported failures of DiscoPOP/DPO for β≤0.01 or β≥2.5 stem directly from this entanglement.",
    "Methods": "Temperature-Decoupled Direct Preference Optimisation (TD-DPO)\n1. Keep the KL term exactly as in DPO, still weighted by β.\n2. Introduce an independent temperature τ that ONLY rescales the preference margin Δ = log p_θ(y⁺|x) − log p_θ(y⁻|x).\n   TD-DPO loss per pair:\n       L = − log σ( Δ / τ )  +  β · KL(p_θ || p₀)\n   where σ is the sigmoid.  Setting τ<1 sharpens the preference signal without forcing a smaller KL, while τ>1 smooths gradients when β must be large for safety.\nTheoretically, this separates information-theoretic regularisation (β) from optimisation stability (τ), giving one extra scalar degree of freedom with negligible implementation cost.",
    "Experimental Setup": "Model: open-source 7B Llama-2-chat (HF transformers).\nData: Argilla DPO-Mix-7k (same as DiscoPOP paper) – train/valid split 90/10.\nBaselines: (a) original DPO (single β), (b) DiscoPOP (best reported β), (c) proposed TD-DPO.\nHyper-grid: β ∈ {0.01,0.05,0.5,2.5}; τ ∈ {0.5,1.0,2.0} (TD-DPO only).\nOptimiser & hardware: AdamW, lr 1e-5, 4 × A100 80G, 3 epochs.\nEvaluation: MT-Bench score (GPT-4 judge) and validation pairwise accuracy.\nReport mean score over three seeds.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=0.05, tau=1.0):\n    \"\"\"TD-DPO pairwise loss.\n    logp_* : tensors of shape (batch,)\n    beta   : KL weight (as in DPO)\n    tau    : new temperature for preference margin\n    returns scalar loss\"\"\"\n    # preference term\n    delta = (logp_pos - logp_neg) / tau        # <-- only change w.r.t. DPO\n    pref_loss = -F.logsigmoid(delta).mean()\n    # KL term (same as DPO)\n    kl = 0.5*((logp_pos - logp_pos_ref)**2 + (logp_neg - logp_neg_ref)**2).mean()\n    return pref_loss + beta * kl\n\n# during training\n# logits_pos, logits_neg come from current model\n# logits_pos_ref, logits_neg_ref from frozen reference model\nloss = td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=beta, tau=tau)\nloss.backward()",
    "Expected Result": "• When β is very small (0.01) TD-DPO with τ=0.5 regains strong gradients and matches performance of best-tuned DPO at β=0.05.\n• When β is large (2.5) TD-DPO with τ=2.0 avoids divergence; MT-Bench improves by ≈+1.5 points over baseline DiscoPOP.\n• For mid-range β (0.05) the default τ=1 keeps parity with DPO (<0.1 score difference).\nOverall, cross-β variance of MT-Bench scores is reduced by ~40 %, showing robustness.",
    "Expected Conclusion": "A single extra scalar (τ) disentangles optimisation curvature from regularisation strength, eliminating observed instabilities of DPO/DiscoPOP with almost no code change.  Because the modification is orthogonal, existing hyper-parameter β sweeps remain valid, while τ offers a lightweight knob for further gains.  This demonstrates how a minimal but principled adjustment to the objective function yields measurable, practical improvements for LLM preference optimisation."
}

# Experimental Design
- Summary: Purpose: verify that Temperature-Decoupled Direct Preference Optimisation (TD-DPO) makes preference-finetuning of large language models more stable and robust than standard Direct Preference Optimisation (DPO).

Components & workflow:
1. Base model: load HuggingFace “Llama-2-Chat-7B” in 4-bit QLoRA format so it fits comfortably on a single A100; keep a frozen copy as reference policy.
2. Data: use the public Argilla DPO-Mix-7k pair-preference corpus (≈6.3 k train / 0.7 k val). Each sample provides (x, y⁺, y⁻).
3. Training loops (three random seeds):
   a. Baseline run with standard DPO loss for four β values {0.01,0.05,0.5,2.5}.
   b. Proposed TD-DPO run on the same β grid while sweeping the additional temperature τ ∈ {0.5,1.0,2.0}.
   c. optimiser: AdamW, lr 1e-5, weight-decay 0.01, batch 128, 3 epochs; gradient-accumulation and FSDP across 4×A100 (≤60 GB GPU-RAM used).
4. During training compute validation pairwise accuracy every 250 steps; keep best checkpoint by this metric.
5. Post-training evaluation: generate answers on MT-Bench v1.1, score with GPT-4 auto-judge, report mean of three seeds.
6. Analysis: compare MT-Bench and validation accuracy across β; measure variance reduction and highlight cases β=0.01 and β=2.5 where TD-DPO is expected to outperform.

The experiment therefore demonstrates that decoupling curvature (τ) from regularisation (β) yields higher scores and lower sensitivity with negligible extra cost.
- Evaluation metrics: ['MT-Bench score', 'Pairwise accuracy']

# Experiment Runs

- Run ID: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
  Method: proposed
  Model: Llama-2-Chat-7B
  Dataset: Argilla DPO-Mix-7k
  Config File: config/run/proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
method: TD-DPO
model:
  name: meta-llama/Llama-2-7b-chat-hf
  quantization: qlora-4bit
  load_in_4bit: true
  torch_dtype: bfloat16
  peft:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.05
dataset:
  name: argilla/dpo-mix-7k
  split:
    train: 0.9
    val: 0.1
  preprocessing:
    max_length: 2048
training:
  epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32
  global_batch_size: 128
  learning_rate: 1e-5
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler_type: cosine
  warmup_steps: 100
  evaluation_strategy: steps
  eval_steps: 250
  save_total_limit: 1
  save_strategy: best
  max_grad_norm: 1.0
  bf16: true
  logging_steps: 25
  seed_list: [42, 43, 44]
  report_to: wandb
loss:
  name: td_dpo_loss
  beta: ${opt.beta}
  tau: ${opt.tau}
  kl_reference_model: true
compute:
  gpus: 4
  gpu_type: a100-80gb
  strategy: fsdp
optuna:
  n_trials: 36
  direction: minimize
  objective_metric: val_pairwise_accuracy
  search_space:
    beta:
      type: categorical
      choices: [0.01, 0.05, 0.5, 2.5]
    tau:
      type: categorical
      choices: [0.5, 1.0, 2.0]
    learning_rate:
      type: loguniform
      low: 5e-6
      high: 5e-5

  ```
  

- Run ID: comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
  Method: comparative-1
  Model: Llama-2-Chat-7B
  Dataset: Argilla DPO-Mix-7k
  Config File: config/run/comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
method: DPO
model:
  name: meta-llama/Llama-2-7b-chat-hf
  quantization: qlora-4bit
  load_in_4bit: true
  torch_dtype: bfloat16
  peft:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.05
dataset:
  name: argilla/dpo-mix-7k
  split:
    train: 0.9
    val: 0.1
  preprocessing:
    max_length: 2048
training:
  epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32
  global_batch_size: 128
  learning_rate: 1e-5
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler_type: cosine
  warmup_steps: 100
  evaluation_strategy: steps
  eval_steps: 250
  save_total_limit: 1
  save_strategy: best
  max_grad_norm: 1.0
  bf16: true
  logging_steps: 25
  seed_list: [42, 43, 44]
  report_to: wandb
loss:
  name: dpo_loss
  beta: ${opt.beta}
  kl_reference_model: true
compute:
  gpus: 4
  gpu_type: a100-80gb
  strategy: fsdp
optuna:
  n_trials: 20
  direction: minimize
  objective_metric: val_pairwise_accuracy
  search_space:
    beta:
      type: categorical
      choices: [0.01, 0.05, 0.5, 2.5]
    learning_rate:
      type: loguniform
      low: 5e-6
      high: 5e-5

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**

- ID: lmsys/sglang-EAGLE-llama2-chat-7B



**HuggingFace Datasets:**

- ID: argilla/dpo-mix-7k




Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import os\nimport math\nimport random\nfrom typing import Dict, Any, Optional, Iterable\n\nimport hydra\nimport torch\nimport wandb\nimport optuna\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import prepare_datasets, PreferenceCollator\nfrom src.model import load_models, compute_logps\n\n# -----------------------------------------------------------------------------\n# Loss functions\n# -----------------------------------------------------------------------------\n\ndef dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n) -> torch.Tensor:\n    \"\"\"Direct Preference Optimisation loss.\"\"\"\n    preference_term = -torch.log(torch.sigmoid(logp_pos - logp_neg)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n\ndef td_dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n    tau: float,\n) -> torch.Tensor:\n    \"\"\"Temperature–Decoupled DPO loss.\"\"\"\n    preference_term = -torch.log(torch.sigmoid((logp_pos - logp_neg) / tau)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n# -----------------------------------------------------------------------------\n# Utility helpers\n# -----------------------------------------------------------------------------\n\ndef limited_iter(iterable: Iterable, max_batches: Optional[int]):\n    \"\"\"Yield at most `max_batches` batches from `iterable` (or all if None).\"\"\"\n    if max_batches is None:\n        # No limit – just yield all\n        yield from iterable\n    else:\n        for idx, item in enumerate(iterable):\n            if idx >= max_batches:\n                break\n            yield item\n\n\ndef evaluate(\n    model: torch.nn.Module,\n    ref_model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    max_batches: Optional[int] = None,\n) -> Dict[str, Any]:\n    \"\"\"Validation loop returning a dict of metrics.\"\"\"\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch in limited_iter(loader, max_batches):\n            logp_pos, logp_neg, *_ = compute_logps(model, ref_model, batch, device)\n            correct += (logp_pos > logp_neg).sum().item()\n            total += logp_pos.size(0)\n    model.train()\n    return {\"val_pairwise_accuracy\": correct / total if total > 0 else 0.0}\n\n# -----------------------------------------------------------------------------\n# Training loop (single run, single seed)\n# -----------------------------------------------------------------------------\n\ndef run_training(cfg, trial: Optional[optuna.Trial] = None) -> float:\n    # ------------------------------------------------------------------\n    # Reproducibility ---------------------------------------------------\n    # ------------------------------------------------------------------\n    torch.manual_seed(cfg.training.seed)\n    random.seed(cfg.training.seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(cfg.training.seed)\n\n    # ------------------------------------------------------------------\n    # Data  -------------------------------------------------------------\n    # ------------------------------------------------------------------\n    tokenizer, train_ds, val_ds = prepare_datasets(cfg)\n\n    # In trial-mode drastically shorten datasets so that only 1-2 batches\n    if cfg.mode == \"trial\":\n        subset_size = cfg.training.per_device_train_batch_size * 2  # 2 batches maximum\n        train_ds = torch.utils.data.Subset(train_ds, range(min(subset_size, len(train_ds))))\n        val_ds = torch.utils.data.Subset(val_ds, range(min(subset_size, len(val_ds))))\n\n    # ------------------------------------------------------------------\n    # Model -------------------------------------------------------------\n    # ------------------------------------------------------------------\n    model, ref_model = load_models(cfg, tokenizer)\n\n    # ------------------------------------------------------------------\n    # DataLoaders -------------------------------------------------------\n    # ------------------------------------------------------------------\n    collator = PreferenceCollator(\n        tokenizer, max_length=cfg.dataset.preprocessing.max_length\n    )\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=False,\n        num_workers=2,\n        collate_fn=collator,\n    )\n\n    # ------------------------------------------------------------------\n    # Optimiser & scheduler --------------------------------------------\n    # ------------------------------------------------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    params = [\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(params, lr=cfg.training.learning_rate)\n\n    num_update_steps_per_epoch = math.ceil(\n        len(train_loader) / cfg.training.gradient_accumulation_steps\n    )\n    max_steps = cfg.training.epochs * num_update_steps_per_epoch\n\n    if cfg.training.lr_scheduler_type == \"cosine\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n\n    # ------------------------------------------------------------------\n    # WandB -------------------------------------------------------------\n    # ------------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB URL: {wandb_run.url}\")\n    else:\n        wandb_run = None\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n    # ------------------------------------------------------------------\n    # Device placement --------------------------------------------------\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    ref_model.to(device)\n    ref_model.eval()\n\n    # ------------------------------------------------------------------\n    # Training loop -----------------------------------------------------\n    # ------------------------------------------------------------------\n    best_val_acc, global_step = 0.0, 0\n    max_train_batches = 2 if cfg.mode == \"trial\" else None\n    max_val_batches = 2 if cfg.mode == \"trial\" else None\n\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        epoch_loss, running_correct, running_total = 0.0, 0, 0\n        iterable = limited_iter(train_loader, max_train_batches)\n        pbar = tqdm(iterable, desc=f\"Epoch {epoch + 1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar, start=1):\n            logp_pos, logp_neg, logp_pos_ref, logp_neg_ref = compute_logps(\n                model, ref_model, batch, device\n            )\n\n            # Loss selection --------------------------------------\n            if cfg.loss.name == \"td_dpo_loss\":\n                loss = td_dpo_loss(\n                    logp_pos,\n                    logp_neg,\n                    logp_pos_ref,\n                    logp_neg_ref,\n                    cfg.loss.beta,\n                    cfg.loss.tau,\n                )\n            else:\n                loss = dpo_loss(\n                    logp_pos,\n                    logp_neg,\n                    logp_pos_ref,\n                    logp_neg_ref,\n                    cfg.loss.beta,\n                )\n            loss = loss / cfg.training.gradient_accumulation_steps\n            loss.backward()\n            epoch_loss += loss.item()\n\n            running_correct += (logp_pos > logp_neg).sum().item()\n            running_total += logp_pos.size(0)\n\n            if step % cfg.training.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                global_step += 1\n\n                # Logging ------------------------------------\n                if wandb_run and global_step % cfg.training.logging_steps == 0:\n                    wandb.log(\n                        {\n                            \"train/loss\": loss.item()\n                            * cfg.training.gradient_accumulation_steps,\n                            \"train/acc\": running_correct / running_total,\n                            \"lr\": scheduler.get_last_lr()[0],\n                            \"step\": global_step,\n                        },\n                        step=global_step,\n                    )\n\n                # In-training evaluation -------------------\n                if (\n                    cfg.training.evaluation_strategy == \"steps\"\n                    and global_step % cfg.training.eval_steps == 0\n                ):\n                    val_metrics = evaluate(\n                        model, ref_model, val_loader, device, max_val_batches\n                    )\n                    if wandb_run:\n                        wandb.log(\n                            {f\"val/{k}\": v for k, v in val_metrics.items()},\n                            step=global_step,\n                        )\n                    if val_metrics[\"val_pairwise_accuracy\"] > best_val_acc:\n                        best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n                        save_best(model, tokenizer, cfg)\n                    model.train()\n\n        # End-of-epoch evaluation --------------------------------------\n        val_metrics = evaluate(model, ref_model, val_loader, device, max_val_batches)\n        if wandb_run:\n            wandb.log({f\"val/{k}\": v for k, v in val_metrics.items()}, step=global_step)\n        if val_metrics[\"val_pairwise_accuracy\"] > best_val_acc:\n            best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n            save_best(model, tokenizer, cfg)\n\n    # ------------------------------------------------------------------\n    # Finalise ----------------------------------------------------------\n    # ------------------------------------------------------------------\n    if wandb_run:\n        wandb_run.summary[\"best_val_pairwise_accuracy\"] = best_val_acc\n        wandb_run.finish()\n\n    return best_val_acc\n\n\n# -----------------------------------------------------------------------------\n# Misc utilities\n# -----------------------------------------------------------------------------\n\ndef save_best(model, tokenizer, cfg):\n    save_path = os.path.join(cfg.results_dir, cfg.run.run_id, \"best\")\n    os.makedirs(save_path, exist_ok=True)\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg):\n    \"\"\"Wrapper so that we can optimise hyper-parameters with Optuna.\"\"\"\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))  # deep copy\n\n    # Sample hyper-parameters according to the search space ------------\n    for hp_name, hp_space in cfg.optuna.search_space.items():\n        if hp_space.type == \"categorical\":\n            sampled = trial.suggest_categorical(hp_name, hp_space.choices)\n        elif hp_space.type == \"loguniform\":\n            sampled = trial.suggest_float(hp_name, hp_space.low, hp_space.high, log=True)\n        else:\n            raise ValueError(f\"Unsupported search-space type: {hp_space.type}\")\n\n        # Write sampled value back into cfg at correct location --------\n        target_field = (\n            f\"loss.{hp_name}\"\n            if hp_name in [\"beta\", \"tau\"]\n            else f\"training.{hp_name}\"\n        )\n        OmegaConf.update(cfg, target_field, sampled, merge=False)\n\n    # Disable WandB during hyper-parameter search ---------------------\n    cfg.wandb.mode = \"disabled\"\n    val_acc = run_training(cfg, trial)\n    # We *minimise* objective, so return negative accuracy\n    return -val_acc\n\n\n# -----------------------------------------------------------------------------\n# CLI entry-point --------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n    os.makedirs(os.path.join(cfg.results_dir, cfg.run.run_id), exist_ok=True)\n\n    # Mode-specific tweaks ------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # ------------------ Hyper-parameter search (Optuna) ------------------\n    if cfg.optuna.n_trials and cfg.optuna.n_trials > 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        print(\"[Optuna] Best params:\", study.best_params)\n        # Inject best params into cfg for the final training run\n        for k, v in study.best_params.items():\n            target_field = (\n                f\"loss.{k}\" if k in [\"beta\", \"tau\"] else f\"training.{k}\"\n            )\n            OmegaConf.update(cfg, target_field, v, merge=False)\n\n    # ------------------ Final training -----------------------------------\n    run_training(cfg)\n\n\nif __name__ == \"__main__\":\n    main()",
    "evaluate_py": "\"\"\"Independent evaluation & visualisation script.\n\nUsage (from repository root):\n    uv run python -m src.evaluate results_dir=/abs/path run_ids='[\"run-1\",\"run-2\",...]'\n\nThe script relies ONLY on standard libraries + matplotlib/seaborn/scipy + wandb.\nIt is intentionally *not* Hydra-driven so that CI can call it directly.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport wandb\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Directory / file helpers ----------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef ensure_dir(path: str):\n    os.makedirs(path, exist_ok=True)\n    return path\n\n\ndef save_json(obj, path: str):\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n# -----------------------------------------------------------------------------\n# Plotting helpers ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef plot_learning_curve(history_df: pd.DataFrame, run_id: str, save_dir: str) -> str:\n    fig, ax = plt.subplots(figsize=(8, 5))\n    for col in [\n        \"train/loss\",\n        \"train/acc\",\n        \"val/val_pairwise_accuracy\",\n    ]:\n        if col in history_df.columns:\n            ax.plot(history_df[\"step\"], history_df[col], label=col)\n    ax.set_xlabel(\"Step\")\n    ax.set_title(f\"Learning Curves – {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    fn = f\"{run_id}_learning_curve.pdf\"\n    path = os.path.join(save_dir, fn)\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(summary: Dict[str, float], run_id: str, save_dir: str) -> str:\n    tp = summary.get(\"val_true_pos\", summary.get(\"val_correct\", 0))\n    fp = summary.get(\"val_false_pos\", 0)\n    tn = summary.get(\"val_true_neg\", 0)\n    fn = summary.get(\"val_false_neg\", 0)\n    matrix = [[tp, fp], [fn, tn]]\n\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n    ax.set_xlabel(\"Predicted + / -\")\n    ax.set_ylabel(\"Actual + / -\")\n    ax.set_title(f\"Confusion Matrix – {run_id}\")\n    fig.tight_layout()\n    fn = f\"{run_id}_confusion_matrix.pdf\"\n    path = os.path.join(save_dir, fn)\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n# -----------------------------------------------------------------------------\n# Per-run processing ----------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef process_single_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: str):\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history = run.history()  # pd.DataFrame\n    summary = run.summary._json_dict\n    config = dict(run.config)\n\n    ensure_dir(out_dir)\n    metrics_path = os.path.join(out_dir, \"metrics.json\")\n    save_json({\"summary\": summary, \"config\": config}, metrics_path)\n\n    figs: List[str] = []\n    figs.append(plot_learning_curve(history, run_id, out_dir))\n    figs.append(plot_confusion_matrix(summary, run_id, out_dir))\n    return {\n        \"metrics_path\": metrics_path,\n        \"figs\": figs,\n        \"summary\": summary,\n        \"config\": config,\n    }\n\n# -----------------------------------------------------------------------------\n# Aggregated analysis ---------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef derive_improvement_rates(baseline_runs: List[str], metric_values: Dict[str, float]):\n    \"\"\"Compute improvement (run – baseline)/baseline for every run.\n\n    If multiple baseline runs are supplied we take the MEAN of their values.\n    \"\"\"\n    baseline_val = sum(metric_values[r] for r in baseline_runs) / len(baseline_runs)\n    derived = {}\n    for run_id, val in metric_values.items():\n        if run_id in baseline_runs:\n            continue\n        derived[run_id] = (val - baseline_val) / baseline_val if baseline_val != 0 else 0.0\n    return derived\n\n\ndef grouped_by_seed(run_ids: List[str]):\n    groups = defaultdict(list)\n    for rid in run_ids:\n        if \"-seed\" in rid:\n            base = rid.split(\"-seed\", 1)[0]\n            groups[base].append(rid)\n        else:\n            groups[rid].append(rid)\n    return groups\n\n\ndef aggregated_analysis(per_run: Dict[str, Dict], comp_dir: str):\n    ensure_dir(comp_dir)\n\n    # Collect metric values (best val accuracy)\n    metric_name = \"best_val_pairwise_accuracy\"\n    metric_values = {rid: d[\"summary\"].get(metric_name, 0.0) for rid, d in per_run.items()}\n\n    # Save aggregated raw metrics -------------------------------------\n    save_json(metric_values, os.path.join(comp_dir, \"aggregated_metrics.json\"))\n\n    # Derived metrics: improvement rates ------------------------------\n    # Choose baseline as *first* run in alphabetical order unless user marks\n    baseline_runs = [sorted(metric_values.keys())[0]]\n    improvement = derive_improvement_rates(baseline_runs, metric_values)\n    save_json(\n        {\n            \"baseline\": baseline_runs,\n            \"improvement_rate\": improvement,\n        },\n        os.path.join(comp_dir, \"derived_metrics.json\"),\n    )\n\n    # Statistical significance tests (t-test) --------------------------\n    by_seed = grouped_by_seed(list(per_run.keys()))\n    # compute mean per seed group\n    group_means = {g: [metric_values[r] for r in rs] for g, rs in by_seed.items()}\n    baseline_group_name = [g for g in group_means.keys() if g in baseline_runs[0]][0]\n\n    p_values = {}\n    for g, vals in group_means.items():\n        if g == baseline_group_name:\n            continue\n        t_stat, p_val = stats.ttest_ind(group_means[baseline_group_name], vals, equal_var=False)\n        p_values[g] = p_val\n    save_json(p_values, os.path.join(comp_dir, \"p_values.json\"))\n\n    # Visualisation: bar chart with error bars -------------------------\n    labels, means, stds = [], [], []\n    for g, vals in group_means.items():\n        labels.append(g)\n        means.append(sum(vals) / len(vals))\n        stds.append(pd.Series(vals).std())\n\n    fig, ax = plt.subplots(figsize=(max(4, len(labels) * 1.6), 5))\n    sns.barplot(x=labels, y=means, yerr=stds, palette=\"mako\", ax=ax, capsize=0.1)\n    for idx, m in enumerate(means):\n        ax.text(idx, m + 0.002, f\"{m:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    ax.set_ylabel(metric_name)\n    ax.set_title(\"Cross-run Comparison (mean ± s.d.)\")\n    fig.tight_layout()\n    bar_path = os.path.join(comp_dir, \"comparison_accuracy_bar_chart.pdf\")\n    fig.savefig(bar_path)\n    plt.close(fig)\n\n    return {\n        \"aggregated_metrics\": os.path.join(comp_dir, \"aggregated_metrics.json\"),\n        \"derived_metrics\": os.path.join(comp_dir, \"derived_metrics.json\"),\n        \"p_values\": os.path.join(comp_dir, \"p_values.json\"),\n        \"figs\": [bar_path],\n    }\n\n# -----------------------------------------------------------------------------\n# Argument parsing ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Output directory path\")\n    parser.add_argument(\n        \"run_ids\",\n        type=str,\n        help='JSON string list of run IDs (e.g. \"[\\\"run-1\\\",\\\"run-2\\\"]\")',\n    )\n    return parser.parse_args()\n\n# -----------------------------------------------------------------------------\n# Main ------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_args()\n    results_dir = args.results_dir\n    run_ids = json.loads(args.run_ids)\n\n    # Load global WandB config to know entity/project ------------------\n    import yaml\n\n    with open(os.path.join(\"config\", \"config.yaml\")) as f:\n        cfg_global = yaml.safe_load(f)\n    entity = cfg_global[\"wandb\"][\"entity\"]\n    project = cfg_global[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_run_outputs = {}\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        run_dir = ensure_dir(os.path.join(results_dir, rid))\n        per_run_outputs[rid] = process_single_run(api, entity, project, rid, run_dir)\n        print(f\"Per-run outputs for {rid} saved to {run_dir}\")\n        for fp in per_run_outputs[rid][\"figs\"]:\n            print(fp)\n\n    comp_dir = os.path.join(results_dir, \"comparison\")\n    comp_outputs = aggregated_analysis(per_run_outputs, comp_dir)\n    print(\"Aggregated analysis saved:\")\n    for k, v in comp_outputs.items():\n        if isinstance(v, list):\n            for fp in v:\n                print(fp)\n        else:\n            print(v)\n\n\nif __name__ == \"__main__\":\n    main()",
    "preprocess_py": "\"\"\"Dataset loading & preprocessing pipeline.\"\"\"\nfrom typing import List, Tuple\n\nimport datasets\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# -----------------------------------------------------------------------------\n# Dataset wrappers ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\nclass PairPreferenceDataset(torch.utils.data.Dataset):\n    \"\"\"Wrap HF dataset containing (prompt, positive completion, negative completion).\"\"\"\n\n    def __init__(self, hf_ds: datasets.Dataset):\n        self.hf_ds = hf_ds\n\n    def __len__(self):\n        return len(self.hf_ds)\n\n    def __getitem__(self, idx):\n        sample = self.hf_ds[idx]\n        prompt = sample.get(\"prompt\") or sample.get(\"instruction\") or sample.get(\"question\") or \"\"\n\n        # Determine positive / negative responses ----------------------\n        if {\n            \"completion_a\",\n            \"completion_b\",\n            \"choice\",\n        }.issubset(sample.keys()):\n            pos, neg = (\n                (sample[\"completion_a\"], sample[\"completion_b\"])\n                if sample[\"choice\"] == 0\n                else (sample[\"completion_b\"], sample[\"completion_a\"])\n            )\n        elif {\"chosen\", \"rejected\"}.issubset(sample.keys()):\n            pos, neg = sample[\"chosen\"], sample[\"rejected\"]\n        else:\n            raise KeyError(\"Unrecognised preference field names in dataset sample.\")\n        return {\"prompt\": prompt, \"pos\": pos, \"neg\": neg}\n\n\nclass PreferenceCollator:\n    \"\"\"Tokenises pairs & creates attention / response masks.\"\"\"\n\n    def __init__(self, tokenizer: AutoTokenizer, max_length: int = 2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.eos = tokenizer.eos_token or \"</s>\"\n\n    def _build(self, prompts: List[str], responses: List[str]):\n        joined = [p + self.eos + r + self.eos for p, r in zip(prompts, responses)]\n        tokenised = self.tokenizer(\n            joined,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        # Response mask: prompt tokens 0, response tokens 1 ------------\n        mask = torch.zeros_like(tokenised[\"input_ids\"], dtype=torch.bool)\n        for i, (p, _) in enumerate(zip(prompts, responses)):\n            prompt_len = len(self.tokenizer(p + self.eos)[\"input_ids\"])\n            seq_len = tokenised[\"attention_mask\"][i].sum().item()\n            mask[i, prompt_len:seq_len] = 1\n        tokenised[\"response_mask\"] = mask\n        return tokenised\n\n    def __call__(self, batch: List[dict]):\n        prompts = [b[\"prompt\"] for b in batch]\n        pos_responses = [b[\"pos\"] for b in batch]\n        neg_responses = [b[\"neg\"] for b in batch]\n        return {\n            \"pos\": self._build(prompts, pos_responses),\n            \"neg\": self._build(prompts, neg_responses),\n        }\n\n# -----------------------------------------------------------------------------\n# Public interface ------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef prepare_datasets(cfg) -> Tuple[AutoTokenizer, torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    \"\"\"Returns (tokenizer, train_dataset, val_dataset).\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\", use_fast=True)\n\n    raw = load_dataset(cfg.dataset.name, cache_dir=\".cache/\")\n    if \"train\" in raw and \"validation\" in raw:\n        train_raw, val_raw = raw[\"train\"], raw[\"validation\"]\n    else:\n        split = raw[\"train\"].train_test_split(test_size=cfg.dataset.split.val)\n        train_raw, val_raw = split[\"train\"], split[\"test\"]\n\n    train_ds = PairPreferenceDataset(train_raw)\n    val_ds = PairPreferenceDataset(val_raw)\n    return tokenizer, train_ds, val_ds",
    "model_py": "\"\"\"Model loading utilities.\"\"\"\nfrom typing import Tuple\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\n\n__all__ = [\"load_models\", \"compute_logps\"]\n\n\n# -----------------------------------------------------------------------------\n# Loading 4-bit QLoRA model and frozen reference ------------------------------\n# -----------------------------------------------------------------------------\n\ndef load_models(cfg, tokenizer: AutoTokenizer) -> Tuple[torch.nn.Module, torch.nn.Module]:\n    bnb_config = None\n    if cfg.model.get(\"load_in_4bit\", False):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n\n    model.gradient_checkpointing_enable()\n\n    if cfg.model.get(\"peft\"):\n        lora_cfg = LoraConfig(\n            r=cfg.model.peft.lora_r,\n            lora_alpha=cfg.model.peft.lora_alpha,\n            lora_dropout=cfg.model.peft.lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        model = get_peft_model(model, lora_cfg)\n\n    # Frozen reference (no LoRA) ---------------------------------------\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    ref_model.eval()\n    for p in ref_model.parameters():\n        p.requires_grad_(False)\n\n    return model, ref_model\n\n# -----------------------------------------------------------------------------\n# Log-probability utilities ---------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _gather_log_probs(logits: torch.Tensor, input_ids: torch.Tensor):\n    logp = torch.nn.functional.log_softmax(logits, dim=-1)\n    # shift so that next-token probabilities align with targets --------\n    logp = logp[:, :-1, :]\n    target = input_ids[:, 1:]\n    gathered = torch.gather(logp, 2, target.unsqueeze(-1)).squeeze(-1)\n    return gathered\n\n\ndef _forward(model, batch_part, device):\n    input_ids = batch_part[\"input_ids\"].to(device)\n    attention_mask = batch_part[\"attention_mask\"].to(device)\n    response_mask = batch_part[\"response_mask\"].to(device)[:, 1:]  # align\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    log_probs = _gather_log_probs(outputs.logits, input_ids)\n    seq_logp = (log_probs * response_mask).sum(dim=1) / response_mask.sum(dim=1)\n    return seq_logp\n\n\ndef compute_logps(model, ref_model, batch, device):\n    \"\"\"Compute log-probs for positive/negative completions for both models.\"\"\"\n    pos, neg = batch[\"pos\"], batch[\"neg\"]\n\n    with torch.no_grad():\n        logp_pos_ref = _forward(ref_model, pos, device)\n        logp_neg_ref = _forward(ref_model, neg, device)\n\n    logp_pos = _forward(model, pos, device)\n    logp_neg = _forward(model, neg, device)\n\n    return logp_pos, logp_neg, logp_pos_ref, logp_neg_ref",
    "main_py": "import subprocess\nimport hydra\nfrom omegaconf import OmegaConf\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # ---------------- Mode-specific tweaks -----------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # ---------------- Launch train.py as a subprocess ------------------\n    for seed in cfg.training.seed_list:\n        run_id_seed = f\"{cfg.run.run_id}-seed{seed}\"\n        overrides = [\n            f\"run.run_id={run_id_seed}\",\n            f\"training.seed={seed}\",\n            f\"results_dir={cfg.results_dir}\",\n            f\"wandb.mode={cfg.wandb.mode}\",\n            f\"mode={cfg.mode}\",\n        ]\n        cmd = [\"python\", \"-u\", \"-m\", \"src.train\"] + overrides\n        print(\"Launching:\", \" \".join(cmd))\n        subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()",
    "pyproject_toml": "[project]\nname = \"td_dpo_experiments\"\nversion = \"0.1.0\"\ndescription = \"Temperature-Decoupled DPO experiments\"\nrequires-python = \">=3.9\"\n\n[project.dependencies]\ntorch = \">=2.0\"\ntransformers = \">=4.34.0\"\ndatasets = \">=2.14.0\"\npeft = \">=0.6.0\"\nbitsandbytes = \"*\"\nhydra-core = \">=1.3.2\"\nwandb = \">=0.15.12\"\noptuna = \">=3.3.0\"\nmatplotlib = \">=3.7.0\"\nseaborn = \">=0.13.0\"\nscipy = \">=1.10.0\"",
    "config_yaml": "defaults:\n  - _self_\n  - run: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k  # default, override via CLI\n\n# Global parameters -----------------------------------------------------------\nmode: full               # trial / full (override via CLI)\nresults_dir: ./results   # where to save checkpoints & analysis\n\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online           # auto-overridden depending on mode\n\n# Placeholder (content injected from run YAML) -------------------------------\nrun: {}\n"
}
