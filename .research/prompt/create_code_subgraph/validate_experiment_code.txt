
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
     * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
{
    "Open Problems": "In DPO-style preference optimization the single hyper-parameter β simultaneously (1) scales the preference loss curvature and (2) weights the KL-divergence regulariser.  This coupling makes training unstable: small β under-regularises but also flattens the preference loss, while large β over-regularises and explodes the preference-loss gradient.  Reported failures of DiscoPOP/DPO for β≤0.01 or β≥2.5 stem directly from this entanglement.",
    "Methods": "Temperature-Decoupled Direct Preference Optimisation (TD-DPO)\n1. Keep the KL term exactly as in DPO, still weighted by β.\n2. Introduce an independent temperature τ that ONLY rescales the preference margin Δ = log p_θ(y⁺|x) − log p_θ(y⁻|x).\n   TD-DPO loss per pair:\n       L = − log σ( Δ / τ )  +  β · KL(p_θ || p₀)\n   where σ is the sigmoid.  Setting τ<1 sharpens the preference signal without forcing a smaller KL, while τ>1 smooths gradients when β must be large for safety.\nTheoretically, this separates information-theoretic regularisation (β) from optimisation stability (τ), giving one extra scalar degree of freedom with negligible implementation cost.",
    "Experimental Setup": "Model: open-source 7B Llama-2-chat (HF transformers).\nData: Argilla DPO-Mix-7k (same as DiscoPOP paper) – train/valid split 90/10.\nBaselines: (a) original DPO (single β), (b) DiscoPOP (best reported β), (c) proposed TD-DPO.\nHyper-grid: β ∈ {0.01,0.05,0.5,2.5}; τ ∈ {0.5,1.0,2.0} (TD-DPO only).\nOptimiser & hardware: AdamW, lr 1e-5, 4 × A100 80G, 3 epochs.\nEvaluation: MT-Bench score (GPT-4 judge) and validation pairwise accuracy.\nReport mean score over three seeds.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=0.05, tau=1.0):\n    \"\"\"TD-DPO pairwise loss.\n    logp_* : tensors of shape (batch,)\n    beta   : KL weight (as in DPO)\n    tau    : new temperature for preference margin\n    returns scalar loss\"\"\"\n    # preference term\n    delta = (logp_pos - logp_neg) / tau        # <-- only change w.r.t. DPO\n    pref_loss = -F.logsigmoid(delta).mean()\n    # KL term (same as DPO)\n    kl = 0.5*((logp_pos - logp_pos_ref)**2 + (logp_neg - logp_neg_ref)**2).mean()\n    return pref_loss + beta * kl\n\n# during training\n# logits_pos, logits_neg come from current model\n# logits_pos_ref, logits_neg_ref from frozen reference model\nloss = td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=beta, tau=tau)\nloss.backward()",
    "Expected Result": "• When β is very small (0.01) TD-DPO with τ=0.5 regains strong gradients and matches performance of best-tuned DPO at β=0.05.\n• When β is large (2.5) TD-DPO with τ=2.0 avoids divergence; MT-Bench improves by ≈+1.5 points over baseline DiscoPOP.\n• For mid-range β (0.05) the default τ=1 keeps parity with DPO (<0.1 score difference).\nOverall, cross-β variance of MT-Bench scores is reduced by ~40 %, showing robustness.",
    "Expected Conclusion": "A single extra scalar (τ) disentangles optimisation curvature from regularisation strength, eliminating observed instabilities of DPO/DiscoPOP with almost no code change.  Because the modification is orthogonal, existing hyper-parameter β sweeps remain valid, while τ offers a lightweight knob for further gains.  This demonstrates how a minimal but principled adjustment to the objective function yields measurable, practical improvements for LLM preference optimisation."
}

# Experimental Design
- Strategy: Purpose: verify that Temperature-Decoupled Direct Preference Optimisation (TD-DPO) makes preference-finetuning of large language models more stable and robust than standard Direct Preference Optimisation (DPO).

Components & workflow:
1. Base model: load HuggingFace “Llama-2-Chat-7B” in 4-bit QLoRA format so it fits comfortably on a single A100; keep a frozen copy as reference policy.
2. Data: use the public Argilla DPO-Mix-7k pair-preference corpus (≈6.3 k train / 0.7 k val). Each sample provides (x, y⁺, y⁻).
3. Training loops (three random seeds):
   a. Baseline run with standard DPO loss for four β values {0.01,0.05,0.5,2.5}.
   b. Proposed TD-DPO run on the same β grid while sweeping the additional temperature τ ∈ {0.5,1.0,2.0}.
   c. optimiser: AdamW, lr 1e-5, weight-decay 0.01, batch 128, 3 epochs; gradient-accumulation and FSDP across 4×A100 (≤60 GB GPU-RAM used).
4. During training compute validation pairwise accuracy every 250 steps; keep best checkpoint by this metric.
5. Post-training evaluation: generate answers on MT-Bench v1.1, score with GPT-4 auto-judge, report mean of three seeds.
6. Analysis: compare MT-Bench and validation accuracy across β; measure variance reduction and highlight cases β=0.01 and β=2.5 where TD-DPO is expected to outperform.

The experiment therefore demonstrates that decoupling curvature (τ) from regularisation (β) yields higher scores and lower sensitivity with negligible extra cost.
- Proposed Method: Temperature-Decoupled Direct Preference Optimisation (TD-DPO)
Objective: stabilise preference-based RL-free finetuning by separating the roles of the single DPO hyper-parameter β.
Theory: In DPO, β simultaneously scales the KL-divergence regulariser and the curvature of the pairwise preference loss. TD-DPO introduces an independent temperature τ that rescales only the preference margin Δ = log pθ(y⁺|x) − log pθ(y⁻|x). This yields:
   L(x,y⁺,y⁻) = −log σ(Δ/τ) + β · KL( pθ || p0 ).
Properties:
• β continues to control the information-theoretic distance from the reference policy.
• τ (<1 sharpen, >1 smooth) controls optimisation curvature without touching the KL term.
Algorithmic procedure per minibatch:
1. Forward pass current model to get log-probs for preferred and dispreferred answers.
2. Forward pass frozen reference model for the same answers.
3. Compute Δ, divide by τ, compute −log σ to obtain preference loss.
4. Compute symmetric KL between current and reference log-probs.
5. Combine losses as above; back-propagate, update with AdamW.
6. Repeat for all β,τ grid points. Hyper-parameter search chooses (β,τ) that maximises validation accuracy.
Implementation adds two code lines to standard DPO: one temperature division and one hyper-parameter entry, so existing codebases can adopt TD-DPO instantly.
- Evaluation Metrics: ['MT-Bench score', 'Pairwise accuracy']

# Experiment Runs

- Run ID: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
  Method: proposed
  Model: Llama-2-Chat-7B
  Dataset: Argilla DPO-Mix-7k
  
  Config Content:
  ```yaml
  run_id: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
method: TD-DPO
model:
  name: meta-llama/Llama-2-7b-chat-hf
  quantization: qlora-4bit
  load_in_4bit: true
  torch_dtype: bfloat16
  peft:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.05
dataset:
  name: argilla/dpo-mix-7k
  split:
    train: 0.9
    val: 0.1
  preprocessing:
    max_length: 2048
training:
  epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32
  global_batch_size: 128
  learning_rate: 1e-5
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler_type: cosine
  warmup_steps: 100
  evaluation_strategy: steps
  eval_steps: 250
  save_total_limit: 1
  save_strategy: best
  max_grad_norm: 1.0
  bf16: true
  logging_steps: 25
  seed_list: [42, 43, 44]
  report_to: wandb
loss:
  name: td_dpo_loss
  beta: ${opt.beta}
  tau: ${opt.tau}
  kl_reference_model: true
compute:
  gpus: 4
  gpu_type: a100-80gb
  strategy: fsdp
optuna:
  n_trials: 36
  direction: minimize
  objective_metric: val_pairwise_accuracy
  search_space:
    beta:
      type: categorical
      choices: [0.01, 0.05, 0.5, 2.5]
    tau:
      type: categorical
      choices: [0.5, 1.0, 2.0]
    learning_rate:
      type: loguniform
      low: 5e-6
      high: 5e-5

  ```
  

- Run ID: comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
  Method: comparative-1
  Model: Llama-2-Chat-7B
  Dataset: Argilla DPO-Mix-7k
  
  Config Content:
  ```yaml
  run_id: comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
method: DPO
model:
  name: meta-llama/Llama-2-7b-chat-hf
  quantization: qlora-4bit
  load_in_4bit: true
  torch_dtype: bfloat16
  peft:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.05
dataset:
  name: argilla/dpo-mix-7k
  split:
    train: 0.9
    val: 0.1
  preprocessing:
    max_length: 2048
training:
  epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32
  global_batch_size: 128
  learning_rate: 1e-5
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler_type: cosine
  warmup_steps: 100
  evaluation_strategy: steps
  eval_steps: 250
  save_total_limit: 1
  save_strategy: best
  max_grad_norm: 1.0
  bf16: true
  logging_steps: 25
  seed_list: [42, 43, 44]
  report_to: wandb
loss:
  name: dpo_loss
  beta: ${opt.beta}
  kl_reference_model: true
compute:
  gpus: 4
  gpu_type: a100-80gb
  strategy: fsdp
optuna:
  n_trials: 20
  direction: minimize
  objective_metric: val_pairwise_accuracy
  search_space:
    beta:
      type: categorical
      choices: [0.01, 0.05, 0.5, 2.5]
    learning_rate:
      type: loguniform
      low: 5e-6
      high: 5e-5

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n  - run: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k  # default; overridden via CLI\n\n# -----------------------------------------------------------------------------\n# Global flags -----------------------------------------------------------------\n# -----------------------------------------------------------------------------\nmode: full                 # \u0027trial\u0027 / \u0027full\u0027 \u2013 override via CLI\nresults_dir: ./results     # base directory for checkpoints \u0026 figures\n\n# -----------------------------------------------------------------------------\n# WandB configuration ----------------------------------------------------------\n# -----------------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online             # auto-overridden when mode == trial\n\n# -----------------------------------------------------------------------------\n# Safe defaults to avoid interpolation errors ----------------------------------\n# -----------------------------------------------------------------------------\nopt:\n  beta: 0.05               # numeric defaults \u2013 Optuna/CLI may override\n  tau: 1.0\n\ntraining:\n  seed_list: [42]\n  seed: 42                 # individual seed used when train.py is called directly\n\n# Placeholder for run-specific parameters --------------------------------------\nrun: {}\noptuna:\n  n_trials: 0              # default \u2013 overridden by run configs\n  direction: minimize\n  search_space: {}         # populated in run configs", "evaluate_py": "\"\"\"Independent evaluation \u0026 visualisation script.\n\nUsage:\n    uv run python -m src.evaluate \\\n        results_dir=/abs/path \\\n        run_ids=\u0027[\"run-1\",\"run-2\"]\u0027\n\"\"\"\nimport argparse\nimport json\nimport os\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport wandb\nfrom scipy import stats\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Filesystem helpers -----------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef ensure_dir(path: str):\n    os.makedirs(path, exist_ok=True)\n    return path\n\n\ndef save_json(obj: Dict, path: str):\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n# -----------------------------------------------------------------------------\n# Plotting helpers -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef plot_learning_curve(history: pd.DataFrame, run_id: str, out_dir: str) -\u003e str:\n    \"\"\"Save learning-curve figure and return filepath.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 5))\n    for col in [\"train/loss\", \"train/acc\", \"val/val_pairwise_accuracy\"]:\n        if col in history.columns:\n            ax.plot(history[\"step\"], history[col], label=col)\n    ax.set_xlabel(\"Step\")\n    ax.set_title(f\"Learning Curves \u2013 {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_learning_curve.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(summary: Dict[str, float], run_id: str, out_dir: str) -\u003e str:\n    tp = summary.get(\"val_true_pos\", summary.get(\"val_correct\", 0))\n    fp = summary.get(\"val_false_pos\", 0)\n    tn = summary.get(\"val_true_neg\", 0)\n    fn = summary.get(\"val_false_neg\", 0)\n    matrix = [[tp, fp], [fn, tn]]\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n    ax.set_xlabel(\"Predicted + / -\")\n    ax.set_ylabel(\"Actual + / -\")\n    ax.set_title(f\"Confusion Matrix \u2013 {run_id}\")\n    fig.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_confusion_matrix.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n# -----------------------------------------------------------------------------\n# Per-run processing -----------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef process_single_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: str):\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history = run.history()  # pandas DataFrame\n    summary = run.summary._json_dict\n    config = dict(run.config)\n\n    ensure_dir(out_dir)\n    metrics_path = os.path.join(out_dir, \"metrics.json\")\n    save_json({\"summary\": summary, \"config\": config}, metrics_path)\n\n    figs: List[str] = []\n    figs.append(plot_learning_curve(history, run_id, out_dir))\n    figs.append(plot_confusion_matrix(summary, run_id, out_dir))\n    return {\n        \"metrics_path\": metrics_path,\n        \"figs\": figs,\n        \"summary\": summary,\n        \"config\": config,\n    }\n\n# -----------------------------------------------------------------------------\n# Aggregated analysis ----------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef derive_improvement_rates(baseline_runs: List[str], metric_values: Dict[str, float]):\n    base_val = sum(metric_values[r] for r in baseline_runs) / len(baseline_runs)\n    return {\n        r: (v - base_val) / base_val if base_val != 0 else 0.0\n        for r, v in metric_values.items()\n        if r not in baseline_runs\n    }\n\n\ndef group_by_seed(run_ids: List[str]):\n    groups = defaultdict(list)\n    for rid in run_ids:\n        if \"-seed\" in rid:\n            base = rid.split(\"-seed\", 1)[0]\n            groups[base].append(rid)\n        else:\n            groups[rid].append(rid)\n    return groups\n\n\ndef aggregated_analysis(per_run: Dict[str, Dict], comp_dir: str):\n    ensure_dir(comp_dir)\n\n    metric_name = \"best_val_pairwise_accuracy\"\n    metric_values = {rid: d[\"summary\"].get(metric_name, 0.0) for rid, d in per_run.items()}\n    save_json(metric_values, os.path.join(comp_dir, \"aggregated_metrics.json\"))\n\n    # Baseline: first run as reference (could be replaced by explicit selection)\n    baseline_runs = [sorted(metric_values.keys())[0]]\n    improvements = derive_improvement_rates(baseline_runs, metric_values)\n    save_json(\n        {\"baseline\": baseline_runs, \"improvement_rate\": improvements},\n        os.path.join(comp_dir, \"derived_metrics.json\"),\n    )\n\n    # Statistical test across seeds -----------------------------------\n    grouped = group_by_seed(list(per_run.keys()))\n    baseline_group = [g for g in grouped if g in baseline_runs[0]][0]\n    p_values = {}\n    for g, rs in grouped.items():\n        if g == baseline_group:\n            continue\n        vals_baseline = [metric_values[r] for r in grouped[baseline_group]]\n        vals_other = [metric_values[r] for r in rs]\n        if len(vals_baseline) \u003e= 2 and len(vals_other) \u003e= 2:\n            t_stat, p_val = stats.ttest_ind(vals_baseline, vals_other, equal_var=False)\n            p_values[g] = p_val\n    save_json(p_values, os.path.join(comp_dir, \"p_values.json\"))\n\n    # Bar chart (matplotlib with error bars, avoids seaborn yerr removal)\n    labels, means, stds = [], [], []\n    for g, rs in grouped.items():\n        vals = [metric_values[r] for r in rs]\n        labels.append(g)\n        means.append(sum(vals) / len(vals))\n        stds.append(pd.Series(vals).std())\n\n    fig, ax = plt.subplots(figsize=(max(4, len(labels) * 1.6), 5))\n    colors = sns.color_palette(\"mako\", len(labels))\n    positions = range(len(labels))\n    ax.bar(positions, means, yerr=stds, capsize=4, color=colors, alpha=0.9)\n    ax.set_xticks(positions)\n    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n\n    for idx, m in enumerate(means):\n        ax.text(idx, m + 0.002, f\"{m:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    ax.set_ylabel(metric_name)\n    ax.set_title(\"Cross-run Comparison (mean \u00b1 s.d.)\")\n    fig.tight_layout()\n    bar_path = os.path.join(comp_dir, \"comparison_accuracy_bar_chart.pdf\")\n    fig.savefig(bar_path)\n    plt.close(fig)\n\n    return {\n        \"aggregated_metrics\": os.path.join(comp_dir, \"aggregated_metrics.json\"),\n        \"derived_metrics\": os.path.join(comp_dir, \"derived_metrics.json\"),\n        \"p_values\": os.path.join(comp_dir, \"p_values.json\"),\n        \"figs\": [bar_path],\n    }\n\n# -----------------------------------------------------------------------------\n# CLI -------------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str, help=\"Directory to save outputs\")\n    parser.add_argument(\n        \"run_ids\",\n        type=str,\n        help=\"JSON string list of run IDs (e.g. \u0027[\\\"run-1\\\",\\\"run-2\\\"]\u0027)\",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    results_dir = args.results_dir\n    run_ids = json.loads(args.run_ids)\n\n    # Load global WandB config to identify entity/project -------------\n    import yaml\n\n    with open(os.path.join(\"config\", \"config.yaml\")) as f:\n        cfg_global = yaml.safe_load(f)\n    entity = cfg_global[\"wandb\"][\"entity\"]\n    project = cfg_global[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n    per_run_outputs: Dict[str, Dict] = {}\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        out_dir = ensure_dir(os.path.join(results_dir, rid))\n        per_run_outputs[rid] = process_single_run(api, entity, project, rid, out_dir)\n        print(f\"Processed {rid} \u2013 outputs stored in {out_dir}\")\n        for fp in per_run_outputs[rid][\"figs\"]:\n            print(fp)\n\n    comp_dir = os.path.join(results_dir, \"comparison\")\n    comp_outputs = aggregated_analysis(per_run_outputs, comp_dir)\n    print(\"Aggregated analysis saved:\")\n    for k, v in comp_outputs.items():\n        if isinstance(v, list):\n            for fp in v:\n                print(fp)\n        else:\n            print(v)\n\n\nif __name__ == \"__main__\":\n    main()", "main_py": "import subprocess\nimport hydra\nfrom omegaconf import OmegaConf\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # ---------------- Mode-specific tweaks -----------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # ---------------- Spawn train.py for each seed ---------------------------\n    seed_list = cfg.training.get(\"seed_list\", [cfg.training.get(\"seed\", 42)])\n    for seed in seed_list:\n        run_id_seed = f\"{cfg.run.run_id}-seed{seed}\"\n        overrides = [\n            f\"run.run_id={run_id_seed}\",\n            f\"training.seed={seed}\",\n            f\"results_dir={cfg.results_dir}\",\n            f\"wandb.mode={cfg.wandb.mode}\",\n            f\"mode={cfg.mode}\",\n        ]\n        cmd = [\"python\", \"-u\", \"-m\", \"src.train\"] + overrides\n        print(\"Launching:\", \" \".join(cmd))\n        subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()", "model_py": "\"\"\"Model loading utilities.\"\"\"\nfrom typing import Tuple\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\n\n__all__ = [\"load_models\", \"compute_logps\"]\n\n# -----------------------------------------------------------------------------\n# Loading QLoRA model \u0026 frozen reference ---------------------------------------\n# -----------------------------------------------------------------------------\n\ndef load_models(cfg, tokenizer: AutoTokenizer) -\u003e Tuple[torch.nn.Module, torch.nn.Module]:\n    bnb_cfg = None\n    if cfg.model.get(\"load_in_4bit\", False):\n        bnb_cfg = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_cfg,\n        device_map=\"auto\",\n    )\n\n    model.gradient_checkpointing_enable()\n\n    if cfg.model.get(\"peft\"):\n        lora_cfg = LoraConfig(\n            r=cfg.model.peft.lora_r,\n            lora_alpha=cfg.model.peft.lora_alpha,\n            lora_dropout=cfg.model.peft.lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        model = get_peft_model(model, lora_cfg)\n\n    # Frozen reference -------------------------------------------------\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_cfg,\n        device_map=\"auto\",\n    )\n    ref_model.eval()\n    for p in ref_model.parameters():\n        p.requires_grad_(False)\n\n    return model, ref_model\n\n# -----------------------------------------------------------------------------\n# Log-probability helpers ------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _gather_log_probs(logits: torch.Tensor, input_ids: torch.Tensor):\n    logp = torch.nn.functional.log_softmax(logits, dim=-1)\n    logp = logp[:, :-1, :]\n    target = input_ids[:, 1:]\n    return torch.gather(logp, 2, target.unsqueeze(-1)).squeeze(-1)\n\n\ndef _forward(model, part_batch, device):\n    input_ids = part_batch[\"input_ids\"].to(device)\n    attention_mask = part_batch[\"attention_mask\"].to(device)\n    response_mask = part_batch[\"response_mask\"].to(device)[:, 1:]\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    token_logp = _gather_log_probs(outputs.logits, input_ids)\n    seq_logp = (token_logp * response_mask).sum(dim=1) / response_mask.sum(dim=1)\n    return seq_logp\n\n\ndef compute_logps(model, ref_model, batch, device):\n    pos, neg = batch[\"pos\"], batch[\"neg\"]\n    with torch.no_grad():\n        logp_pos_ref = _forward(ref_model, pos, device)\n        logp_neg_ref = _forward(ref_model, neg, device)\n    logp_pos = _forward(model, pos, device)\n    logp_neg = _forward(model, neg, device)\n    return logp_pos, logp_neg, logp_pos_ref, logp_neg_ref", "preprocess_py": "\"\"\"Data loading \u0026 preprocessing pipeline.\"\"\"\nfrom typing import List, Tuple\n\nimport datasets\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# -----------------------------------------------------------------------------\n# Dataset wrappers -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\nclass PairPreferenceDataset(torch.utils.data.Dataset):\n    \"\"\"Wrap HF dataset that contains (prompt, positive, negative) triples.\"\"\"\n\n    def __init__(self, hf_ds: datasets.Dataset):\n        self.hf_ds = hf_ds\n\n    def __len__(self):\n        return len(self.hf_ds)\n\n    def __getitem__(self, idx):\n        sample = self.hf_ds[idx]\n        prompt = (\n            sample.get(\"prompt\")\n            or sample.get(\"instruction\")\n            or sample.get(\"question\")\n            or \"\"\n        )\n        # Identify pos/neg fields --------------------------------------\n        if {\"completion_a\", \"completion_b\", \"choice\"}.issubset(sample.keys()):\n            pos, neg = (\n                (sample[\"completion_a\"], sample[\"completion_b\"])\n                if sample[\"choice\"] == 0\n                else (sample[\"completion_b\"], sample[\"completion_a\"])\n            )\n        elif {\"chosen\", \"rejected\"}.issubset(sample.keys()):\n            pos, neg = sample[\"chosen\"], sample[\"rejected\"]\n        else:\n            raise KeyError(\"Dataset sample lacks recognised preference fields.\")\n        return {\"prompt\": prompt, \"pos\": pos, \"neg\": neg}\n\n\nclass PreferenceCollator:\n    \"\"\"Tokenises preference pairs; returns dict suitable for model input.\"\"\"\n\n    def __init__(self, tokenizer: AutoTokenizer, max_length: int = 2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.eos = tokenizer.eos_token or \"\u003c/s\u003e\"\n\n    def _build(self, prompts: List[str], responses: List[str]):\n        joined = [p + self.eos + r + self.eos for p, r in zip(prompts, responses)]\n        toks = self.tokenizer(\n            joined,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        # response mask: 1 for response tokens, 0 for prompt tokens -----\n        mask = torch.zeros_like(toks[\"input_ids\"], dtype=torch.bool)\n        for i, p in enumerate(prompts):\n            prompt_len = len(self.tokenizer(p + self.eos)[\"input_ids\"])\n            seq_len = toks[\"attention_mask\"][i].sum().item()\n            mask[i, prompt_len:seq_len] = 1\n        toks[\"response_mask\"] = mask\n        return toks\n\n    def __call__(self, batch: List[dict]):\n        prompts = [b[\"prompt\"] for b in batch]\n        pos_resp = [b[\"pos\"] for b in batch]\n        neg_resp = [b[\"neg\"] for b in batch]\n        return {\"pos\": self._build(prompts, pos_resp), \"neg\": self._build(prompts, neg_resp)}\n\n# -----------------------------------------------------------------------------\n# Public API -------------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef prepare_datasets(cfg) -\u003e Tuple[AutoTokenizer, torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    \"\"\"Load dataset, create train/val splits, return tokenizer + datasets.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\", use_fast=True)\n\n    raw_ds = load_dataset(cfg.dataset.name, cache_dir=\".cache/\")\n    if \"train\" in raw_ds and \"validation\" in raw_ds:\n        train_raw, val_raw = raw_ds[\"train\"], raw_ds[\"validation\"]\n    else:\n        split = raw_ds[\"train\"].train_test_split(test_size=cfg.dataset.split.val)\n        train_raw, val_raw = split[\"train\"], split[\"test\"]\n\n    return tokenizer, PairPreferenceDataset(train_raw), PairPreferenceDataset(val_raw)", "pyproject_toml": "[project]\nname = \"td_dpo_experiments\"\nversion = \"0.1.0\"\ndescription = \"Temperature-Decoupled DPO experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0\"\ntransformers = \"\u003e=4.34.0\"\ndatasets = \"\u003e=2.14.0\"\npeft = \"\u003e=0.6.0\"\nbitsandbytes = \"*\"\nhydra-core = \"\u003e=1.3.2\"\nwandb = \"\u003e=0.15.12\"\noptuna = \"\u003e=3.3.0\"\nmatplotlib = \"\u003e=3.7.0\"\nseaborn = \"\u003e=0.13.0\"\nscipy = \"\u003e=1.10.0\"\npandas = \"\u003e=2.0.0\"", "train_py": "import os\nimport math\nimport random\nfrom typing import Dict, Any, Optional, Iterable\n\nimport hydra\nimport torch\nimport wandb\nimport optuna\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import prepare_datasets, PreferenceCollator\nfrom src.model import load_models, compute_logps\n\n# -----------------------------------------------------------------------------\n# Loss functions ---------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n) -\u003e torch.Tensor:\n    \"\"\"Standard DPO loss.\"\"\"\n    preference_term = -torch.log(torch.sigmoid(logp_pos - logp_neg)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n\ndef td_dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n    tau: float,\n) -\u003e torch.Tensor:\n    \"\"\"Temperature-Decoupled DPO loss (proposed).\"\"\"\n    preference_term = -torch.log(torch.sigmoid((logp_pos - logp_neg) / tau)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n# -----------------------------------------------------------------------------\n# Helper utilities -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef limited_iter(iterable: Iterable, max_batches: Optional[int]):\n    \"\"\"Yield at most `max_batches` from iterable (all if None).\"\"\"\n    if max_batches is None:\n        yield from iterable\n    else:\n        for idx, item in enumerate(iterable):\n            if idx \u003e= max_batches:\n                break\n            yield item\n\n\ndef evaluate(\n    model: torch.nn.Module,\n    ref_model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    max_batches: Optional[int] = None,\n) -\u003e Dict[str, Any]:\n    \"\"\"Validation loop \u2013 returns dictionary of metrics.\"\"\"\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch in limited_iter(loader, max_batches):\n            logp_pos, logp_neg, *_ = compute_logps(model, ref_model, batch, device)\n            correct += (logp_pos \u003e logp_neg).sum().item()\n            total += logp_pos.size(0)\n    model.train()\n    return {\"val_pairwise_accuracy\": correct / total if total \u003e 0 else 0.0}\n\n# -----------------------------------------------------------------------------\n# Core training routine --------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef _safe_seed(cfg):\n    \"\"\"Ensure cfg.training.seed exists \u2013 fall back gracefully if absent.\"\"\"\n    if hasattr(cfg.training, \"seed\"):\n        return cfg.training.seed\n    # Derive from seed_list if present otherwise fixed default\n    if hasattr(cfg.training, \"seed_list\") and len(cfg.training.seed_list) \u003e 0:\n        seed = cfg.training.seed_list[0]\n    else:\n        seed = 42\n    cfg.training.seed = seed\n    return seed\n\n\ndef run_training(cfg, trial: Optional[optuna.Trial] = None) -\u003e float:\n    # ------------------------------------------------------------------\n    # Reproducibility ---------------------------------------------------\n    # ------------------------------------------------------------------\n    seed = _safe_seed(cfg)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n    # ------------------------------------------------------------------\n    # Data  -------------------------------------------------------------\n    # ------------------------------------------------------------------\n    tokenizer, train_ds, val_ds = prepare_datasets(cfg)\n\n    # Shorten datasets in trial-mode to 1-2 batches for speed\n    if cfg.mode == \"trial\":\n        subset_size = cfg.training.per_device_train_batch_size * 2\n        train_ds = torch.utils.data.Subset(train_ds, range(min(subset_size, len(train_ds))))\n        val_ds = torch.utils.data.Subset(val_ds, range(min(subset_size, len(val_ds))))\n\n    # ------------------------------------------------------------------\n    # Model -------------------------------------------------------------\n    # ------------------------------------------------------------------\n    model, ref_model = load_models(cfg, tokenizer)\n\n    # ------------------------------------------------------------------\n    # Dataloaders -------------------------------------------------------\n    # ------------------------------------------------------------------\n    collator = PreferenceCollator(tokenizer, max_length=cfg.dataset.preprocessing.max_length)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=False,\n        num_workers=2,\n        collate_fn=collator,\n    )\n\n    # ------------------------------------------------------------------\n    # Optimiser \u0026 scheduler --------------------------------------------\n    # ------------------------------------------------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    params = [\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(params, lr=cfg.training.learning_rate)\n\n    num_update_steps_per_epoch = math.ceil(\n        len(train_loader) / cfg.training.gradient_accumulation_steps\n    )\n    max_steps = cfg.training.epochs * num_update_steps_per_epoch\n\n    if cfg.training.lr_scheduler_type == \"cosine\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n\n    # ------------------------------------------------------------------\n    # WandB -------------------------------------------------------------\n    # ------------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        print(f\"[WandB] Run URL: {wandb_run.url}\")\n    else:\n        wandb_run = None\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n    # ------------------------------------------------------------------\n    # Device placement --------------------------------------------------\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    ref_model.to(device)\n    ref_model.eval()\n\n    # ------------------------------------------------------------------\n    # Training loop -----------------------------------------------------\n    # ------------------------------------------------------------------\n    best_val_acc, global_step = 0.0, 0\n    max_train_batches = 2 if cfg.mode == \"trial\" else None\n    max_val_batches = 2 if cfg.mode == \"trial\" else None\n\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        running_correct, running_total = 0, 0\n        iterable = limited_iter(train_loader, max_train_batches)\n        pbar = tqdm(iterable, desc=f\"Epoch {epoch + 1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar, start=1):\n            logp_pos, logp_neg, logp_pos_ref, logp_neg_ref = compute_logps(\n                model, ref_model, batch, device\n            )\n\n            # Compute loss -------------------------------------------------------------\n            if cfg.loss.name == \"td_dpo_loss\":\n                loss = td_dpo_loss(\n                    logp_pos,\n                    logp_neg,\n                    logp_pos_ref,\n                    logp_neg_ref,\n                    beta=float(cfg.loss.beta),\n                    tau=float(cfg.loss.tau),\n                )\n            else:\n                loss = dpo_loss(\n                    logp_pos,\n                    logp_neg,\n                    logp_pos_ref,\n                    logp_neg_ref,\n                    beta=float(cfg.loss.beta),\n                )\n            loss = loss / cfg.training.gradient_accumulation_steps\n            loss.backward()\n\n            running_correct += (logp_pos \u003e logp_neg).sum().item()\n            running_total += logp_pos.size(0)\n\n            if step % cfg.training.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                global_step += 1\n\n                # ----- logging ---------------------------------------------------------\n                if wandb_run and global_step % cfg.training.logging_steps == 0:\n                    wandb.log(\n                        {\n                            \"train/loss\": loss.item() * cfg.training.gradient_accumulation_steps,\n                            \"train/acc\": running_correct / running_total,\n                            \"lr\": scheduler.get_last_lr()[0],\n                            \"step\": global_step,\n                        },\n                        step=global_step,\n                    )\n\n                # ----- evaluation -------------------------------------------------------\n                if (\n                    cfg.training.evaluation_strategy == \"steps\"\n                    and global_step % cfg.training.eval_steps == 0\n                ):\n                    val_metrics = evaluate(\n                        model, ref_model, val_loader, device, max_val_batches\n                    )\n                    if wandb_run:\n                        wandb.log(\n                            {f\"val/{k}\": v for k, v in val_metrics.items()},\n                            step=global_step,\n                        )\n                    if val_metrics[\"val_pairwise_accuracy\"] \u003e best_val_acc:\n                        best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n                        save_best(model, tokenizer, cfg)\n                    model.train()\n\n        # End-of-epoch evaluation -------------------------------------------------------\n        val_metrics = evaluate(model, ref_model, val_loader, device, max_val_batches)\n        if wandb_run:\n            wandb.log({f\"val/{k}\": v for k, v in val_metrics.items()}, step=global_step)\n        if val_metrics[\"val_pairwise_accuracy\"] \u003e best_val_acc:\n            best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n            save_best(model, tokenizer, cfg)\n\n    # ------------------------------------------------------------------\n    # Finalise ----------------------------------------------------------\n    # ------------------------------------------------------------------\n    if wandb_run:\n        wandb_run.summary[\"best_val_pairwise_accuracy\"] = best_val_acc\n        wandb_run.finish()\n\n    return best_val_acc\n\n# -----------------------------------------------------------------------------\n# Misc utilities ---------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef save_best(model, tokenizer, cfg):\n    save_path = os.path.join(cfg.results_dir, cfg.run.run_id, \"best\")\n    os.makedirs(save_path, exist_ok=True)\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n# -----------------------------------------------------------------------------\n# Optuna objective -------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg):\n    \"\"\"Objective wrapper for Optuna hyper-parameter search.\"\"\"\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))  # deep copy\n\n    # Sample parameters according to search space ----------------------\n    for hp_name, hp_space in cfg.optuna.search_space.items():\n        if hp_space.type == \"categorical\":\n            sampled = trial.suggest_categorical(hp_name, hp_space.choices)\n        elif hp_space.type == \"loguniform\":\n            sampled = trial.suggest_float(hp_name, hp_space.low, hp_space.high, log=True)\n        else:\n            raise ValueError(f\"Unsupported search-space type: {hp_space.type}\")\n\n        target_field = f\"loss.{hp_name}\" if hp_name in [\"beta\", \"tau\"] else f\"training.{hp_name}\"\n        OmegaConf.update(cfg, target_field, sampled, merge=False)\n\n    cfg.wandb.mode = \"disabled\"  # never log individual trials\n    val_acc = run_training(cfg, trial)\n    return -val_acc  # Optuna minimises by default\n\n# -----------------------------------------------------------------------------\n# CLI entry-point --------------------------------------------------------------\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n    os.makedirs(os.path.join(cfg.results_dir, cfg.run.run_id), exist_ok=True)\n\n    # ---------------- Mode tweaks -------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # ---------------- Hyper-parameter search --------------------------\n    if cfg.optuna.n_trials and cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        print(\"[Optuna] Best params:\", study.best_params)\n        for k, v in study.best_params.items():\n            target_field = f\"loss.{k}\" if k in [\"beta\", \"tau\"] else f\"training.{k}\"\n            OmegaConf.update(cfg, target_field, v, merge=False)\n\n    # ---------------- Final training ----------------------------------\n    run_training(cfg)\n\n\nif __name__ == \"__main__\":\n    main()"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": true,
    "code_issue": ""
}
