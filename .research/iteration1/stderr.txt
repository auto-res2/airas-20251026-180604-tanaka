Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 212ms
Installed 98 packages in 4.62s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:00, 6169.94 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 23583.62 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25840.36 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.89s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.63s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:45<00:22, 22.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.77s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.90s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:06,  6.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.38s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:12, 12.19s/it]Epoch 1/1: 2it [00:16,  7.81s/it]Epoch 1/1: 2it [00:17,  8.54s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 54256.33 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 37969.62 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 26212.43 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:30<01:01, 30.66s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.22s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:22<00:44, 22.19s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.85s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 19.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:59<00:00, 20.00s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:11,  5.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.80s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.23s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.63s/it]Epoch 1/1: 2it [00:12,  6.59s/it]Epoch 1/1: 2it [00:12,  6.36s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 56516.22 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 31513.06 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 21897.64 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.46s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.49s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:26<00:52, 26.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:51<00:25, 25.87s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 21.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 22.82s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.59s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.73s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.84s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:03,  3.98s/it]Epoch 1/1: 2it [00:16,  8.74s/it]Epoch 1/1: 2it [00:16,  8.10s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 1.08s
Installed 98 packages in 5.17s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
Error executing job with overrides: ['run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k', 'run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42', 'training.seed=42', 'results_dir=.research/iteration1', 'wandb.mode=disabled', 'mode=trial']
Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-68fe90c9-15cfa3a574467e6734f215bb;0d390278-ce5f-4208-b2b0-2e41f91a1ec4)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py", line 364, in main
    run_training(cfg)
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py", line 113, in run_training
    tokenizer, train_ds, val_ds = prepare_datasets(cfg)
                                  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/preprocess.py", line 105, in prepare_datasets
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=".cache/", use_fast=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1093, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/transformers/utils/hub.py", line 543, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
401 Client Error. (Request ID: Root=1-68fe90c9-15cfa3a574467e6734f215bb;0d390278-ce5f-4208-b2b0-2e41f91a1ec4)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k', 'results_dir=.research/iteration1', 'mode=trial']
Traceback (most recent call last):
  File "/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py", line 41, in main
    subprocess.run(cmd, check=True, cwd=project_root)
  File "/home/toma/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3', '-u', '-m', 'src.train', 'run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k', 'run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42', 'training.seed=42', 'results_dir=.research/iteration1', 'wandb.mode=disabled', 'mode=trial']' returned non-zero exit status 1.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 233ms
Installed 98 packages in 4.57s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4385.04 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 22315.24 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 17620.56 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 24356.41 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:34<01:09, 34.85s/it]Fetching 3 files: 100%|██████████| 3/3 [00:34<00:00, 11.62s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:42, 21.44s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:43<00:21, 21.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.65s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.43s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.88s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:18<00:09,  9.04s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.06s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:06,  6.78s/it]Epoch 1/1: 2it [00:10,  5.21s/it]Epoch 1/1: 2it [00:11,  5.56s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 26692.23 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 26380.57 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 19699.46 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:03, 31.66s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.55s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 21.00s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 18.42s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 19.13s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  7.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.07s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:13, 13.47s/it]Epoch 1/1: 2it [00:17,  8.08s/it]Epoch 1/1: 2it [00:18,  9.01s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  74%|███████▍  | 5000/6750 [00:00<00:00, 35856.17 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 29744.71 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20169.71 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.54s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.24s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.54s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:04,  4.26s/it]Epoch 1/1: 2it [00:11,  6.24s/it]Epoch 1/1: 2it [00:12,  6.06s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 831ms
Installed 98 packages in 4.52s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 5427.55 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 25660.83 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 18501.32 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20424.82 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:31<01:02, 31.15s/it]Fetching 3 files: 100%|██████████| 3/3 [00:31<00:00, 10.38s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 18.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:57<00:00, 19.23s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  8.89s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.97s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.96s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 11.00s/it]Epoch 1/1: 2it [00:17,  8.07s/it]Epoch 1/1: 2it [00:17,  8.62s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 28933.52 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 27680.28 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 20760.59 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:32<01:04, 32.48s/it]Fetching 3 files: 100%|██████████| 3/3 [00:32<00:00, 10.83s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:21<00:43, 21.66s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:42<00:21, 21.34s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 18.70s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:58<00:00, 19.44s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.54s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.72s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:11, 11.07s/it]Epoch 1/1: 2it [00:16,  7.47s/it]Epoch 1/1: 2it [00:16,  8.13s/it]
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  59%|█████▉    | 4000/6750 [00:00<00:00, 33675.73 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 30821.00 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23338.34 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:28<00:57, 28.57s/it]Fetching 3 files: 100%|██████████| 3/3 [00:28<00:00,  9.52s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:20<00:41, 20.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:41<00:20, 20.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:56<00:00, 18.85s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.93s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:11, 11.50s/it]Epoch 1/1: 2it [00:15,  7.15s/it]Epoch 1/1: 2it [00:15,  7.92s/it]
Using CPython 3.11.13
Creating virtual environment at: .venv
Resolved 128 packages in 770ms
Installed 98 packages in 4.73s
 + accelerate==1.11.0
 + aiohappyeyeballs==2.6.1
 + aiohttp==3.13.1
 + aiosignal==1.4.0
 + alembic==1.17.0
 + annotated-types==0.7.0
 + antlr4-python3-runtime==4.9.3
 + anyio==4.11.0
 + attrs==25.4.0
 + bitsandbytes==0.48.1
 + certifi==2025.10.5
 + charset-normalizer==3.4.4
 + click==8.3.0
 + colorlog==6.10.1
 + contourpy==1.3.3
 + cycler==0.12.1
 + datasets==4.3.0
 + dill==0.4.0
 + filelock==3.20.0
 + fonttools==4.60.1
 + frozenlist==1.8.0
 + fsspec==2025.9.0
 + gitdb==4.0.12
 + gitpython==3.1.45
 + greenlet==3.2.4
 + h11==0.16.0
 + hf-xet==1.2.0
 + httpcore==1.0.9
 + httpx==0.28.1
 + huggingface-hub==0.36.0
 + hydra-core==1.3.2
 + idna==3.11
 + jinja2==3.1.6
 + kiwisolver==1.4.9
 + mako==1.3.10
 + markupsafe==3.0.3
 + matplotlib==3.10.7
 + mpmath==1.3.0
 + multidict==6.7.0
 + multiprocess==0.70.16
 + networkx==3.5
 + numpy==2.3.4
 + nvidia-cublas-cu12==12.8.4.1
 + nvidia-cuda-cupti-cu12==12.8.90
 + nvidia-cuda-nvrtc-cu12==12.8.93
 + nvidia-cuda-runtime-cu12==12.8.90
 + nvidia-cudnn-cu12==9.10.2.21
 + nvidia-cufft-cu12==11.3.3.83
 + nvidia-cufile-cu12==1.13.1.3
 + nvidia-curand-cu12==10.3.9.90
 + nvidia-cusolver-cu12==11.7.3.90
 + nvidia-cusparse-cu12==12.5.8.93
 + nvidia-cusparselt-cu12==0.7.1
 + nvidia-nccl-cu12==2.27.5
 + nvidia-nvjitlink-cu12==12.8.93
 + nvidia-nvshmem-cu12==3.3.20
 + nvidia-nvtx-cu12==12.8.90
 + omegaconf==2.3.0
 + optuna==4.5.0
 + packaging==25.0
 + pandas==2.3.3
 + peft==0.17.1
 + pillow==12.0.0
 + platformdirs==4.5.0
 + propcache==0.4.1
 + protobuf==6.33.0
 + psutil==7.1.2
 + pyarrow==22.0.0
 + pydantic==2.12.3
 + pydantic-core==2.41.4
 + pyparsing==3.2.5
 + python-dateutil==2.9.0.post0
 + pytz==2025.2
 + pyyaml==6.0.3
 + regex==2025.10.23
 + requests==2.32.5
 + safetensors==0.6.2
 + scipy==1.16.2
 + seaborn==0.13.2
 + sentencepiece==0.2.1
 + sentry-sdk==2.42.1
 + six==1.17.0
 + smmap==5.0.2
 + sniffio==1.3.1
 + sqlalchemy==2.0.44
 + sympy==1.14.0
 + tokenizers==0.22.1
 + torch==2.9.0
 + tqdm==4.67.1
 + transformers==4.57.1
 + triton==3.5.0
 + typing-extensions==4.15.0
 + typing-inspection==0.4.2
 + tzdata==2025.2
 + urllib3==2.5.0
 + wandb==0.22.2
 + xxhash==3.6.0
 + yarl==1.22.0
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/main.py:7: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  15%|█▍        | 1000/6750 [00:00<00:01, 4345.51 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 17445.10 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 23740.81 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:27<00:55, 27.84s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:28<00:11, 11.76s/it]Fetching 3 files: 100%|██████████| 3/3 [00:28<00:00,  9.45s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.07s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 20.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:03<00:00, 21.02s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:11<00:05,  5.53s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  5.00s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:15, 15.83s/it]Epoch 1/1: 2it [00:24, 11.38s/it]Epoch 1/1: 2it [00:24, 12.15s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 56514.14 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 36963.68 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 28163.30 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:58, 29.48s/it]Fetching 3 files: 100%|██████████| 3/3 [00:29<00:00,  9.83s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.13s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.07s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 19.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.73s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:05<00:10,  5.32s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:10<00:05,  5.40s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.67s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.86s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:10, 10.68s/it]Epoch 1/1: 2it [00:22, 11.50s/it]Epoch 1/1: 2it [00:22, 11.45s/it]
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/src/train.py:337: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path="../config", config_name="config")
/home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Generating train split:   0%|          | 0/6750 [00:00<?, ? examples/s]Generating train split:  89%|████████▉ | 6000/6750 [00:00<00:00, 55669.34 examples/s]Generating train split: 100%|██████████| 6750/6750 [00:00<00:00, 35232.57 examples/s]
Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 750/750 [00:00<00:00, 25177.71 examples/s]
`torch_dtype` is deprecated! Use `dtype` instead!
Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]Fetching 3 files:  33%|███▎      | 1/3 [00:29<00:59, 29.78s/it]Fetching 3 files:  67%|██████▋   | 2/3 [00:30<00:12, 12.68s/it]Fetching 3 files: 100%|██████████| 3/3 [00:30<00:00, 10.16s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:23<00:46, 23.10s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:02<00:00, 20.88s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.23s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.35s/it]
Epoch 1/1: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Epoch 1/1: 1it [00:09,  9.27s/it]Epoch 1/1: 2it [00:14,  7.12s/it]Epoch 1/1: 2it [00:15,  7.52s/it]
