=== [UV SYNC] Start at Sun Oct 26 09:10:33 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:10:38 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:10:38 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:11:18,843][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:11:24,621][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:13:28,011][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:14:03,339][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:14:07,950][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:16:02,498][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:16:34,037][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:16:38,677][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:18:44,570][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:19:11 PM UTC 2025 ===

