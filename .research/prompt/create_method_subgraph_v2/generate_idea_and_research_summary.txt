
LLM Name: o3-2025-04-16
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    new optimization methods for LLMs
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "The paper introduces an LLM-driven objective discovery pipeline to automatically generate novel state-of-the-art offline preference optimization algorithms, overcoming limitations of human-designed loss functions. It discovers several high-performing preference optimization losses, including Discovered Preference Optimization (DiscoPOP), which adaptively blends logistic and exponential losses. DiscoPOP achieves strong performance across multiple held-out evaluation tasks, including multi-turn dialogue, controlled sentiment generation, and summarization, and its initial analysis reveals surprising features like its non-convexity.",
    "Methodology": "The core methodology is an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is initialized with established loss functions and their performance. In each generation, the LLM proposes a new candidate objective function as PyTorch code, which is then validated with unit tests. Valid functions are used to fine-tune an LLM, and its performance (e.g., MT-Bench score) is evaluated and fed back to the LLM as in-context examples for iterative refinement. This process allows the LLM to explore and compose various objective concepts in a complementary fashion.",
    "Experimental Setup": "The discovery process utilized 'zephyr-7b-gemma-sft' (a Gemma 7B model fine-tuned on 'deita-10k-v0-sft') trained on the 'Argilla DPO Mix 7K' pairwise preference dataset, with a fixed beta of 0.05. Model fine-tuning was based on the 'alignment-handbook' repository. Performance was evaluated using MT-Bench (GPT-4 as judge). Held-out evaluations included Alpaca Eval 2.0 for single-turn dialogue (comparing against GPT-4 and SFT base), a subset of the Reddit TL;DR dataset for summarization (evaluated with Alpaca Eval 2.0 library and custom GPT-4 annotator), and the IMDb dataset for positive sentiment generation (using a GPT-2 model and a pre-trained sentiment classifier for rewards against KL-Divergence across varying beta values). Training commonly used AdamW optimizer, bfloat16, and Nvidia A100 GPUs.",
    "Limitations": "The current approach has several limitations. The method for generating LLM objective proposals is still superficial, with unexplored techniques for effective prompting. The beta parameter in DiscoPOP re-purposes its traditional meaning, affecting both functional behavior and KL penalty, necessitating future work on multi-parameter analysis. DiscoPOP struggles to converge at very low (beta <= 0.01) or very high (beta >= 2.5) beta values, possibly due to large gradients in its non-convex region. The reliance on closed-source models (GPT-4) for code generation impacts reproducibility and incurs high costs.",
    "Future Research Directions": "Future research could explore more advanced LLM prompt engineering techniques, such as providing learning curve plots to Visual Language Models or employing meta-meta-optimization of the LLM prompt. Investigating objective functions with multiple floating-point parameters, beyond just beta, which can be tuned independently, is also a promising direction. Additionally, reformulating objectives to address training instability observed with extreme beta values (e.g., through gradient clipping) would be beneficial. Finally, using the discovered models themselves to generate code for self-improvement could reduce reliance on closed-source LLMs, enhancing reproducibility and cost-effectiveness.",
    "Experiment Code": "def chat_completion_openai(model, conv, temperature, max_tokens, api_dict=None):\n    if api_dict is not None:\n        openai.api_base = api_dict[\"api_base\"]\n        openai.api_key = api_dict[\"api_key\"]\n    output = API_ERROR_OUTPUT\n    for _ in range(API_MAX_RETRY):\n        try:\n            messages = conv.to_openai_api_messages()\n            response = openai.ChatCompletion.create(\n                model=model,\n                messages=messages,\n                n=1,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n            output = response[\"choices\"][0][\"message\"][\"content\"]\n            break\n        except openai.error.OpenAIError as e:\n            print(type(e), e)\n            time.sleep(API_RETRY_SLEEP)\n    return output\n\ndef run_judge_pair(question, answer_a, answer_b, judge, ref_answer, multi_turn=False):\n    kwargs = {}\n    model = judge.model_name\n    if ref_answer is not None:\n        kwargs[\"ref_answer_1\"] = ref_answer[\"choices\"][0][\"turns\"][0]\n        if multi_turn:\n            kwargs[\"ref_answer_2\"] = ref_answer[\"choices\"][0][\"turns\"][1]\n\n    if multi_turn:\n        system_prompt = judge.prompt_template[\"system_prompt\"]\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question_1=question[\"turns\"][0],\n            question_2=question[\"turns\"][1],\n            answer_a_1=answer_a[\"choices\"][0][\"turns\"][0],\n            answer_b_1=answer_b[\"choices\"][0][\"turns\"][0],\n            answer_a_2=answer_a[\"choices\"][0][\"turns\"][1],\n            answer_b_2=answer_b[\"choices\"][0][\"turns\"][1],\n            **kwargs,\n        )\n    else:\n        system_prompt = judge.prompt_template[\"system_prompt\"]\n        user_prompt = judge.prompt_template[\"prompt_template\"].format(\n            question=question[\"turns\"][0],\n            answer_a=answer_a[\"choices\"][0][\"turns\"][0],\n            answer_b=answer_b[\"choices\"][0][\"turns\"][0],\n            **kwargs,\n        )\n\n    winner = \"error\"\n\n    conv = get_conversation_template(model)\n    conv.append_message(conv.roles[0], user_prompt)\n    conv.append_message(conv.roles[1], None)\n\n    if model in OPENAI_MODEL_LIST:\n        conv.set_system_message(system_prompt)\n        judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)\n    elif model in ANTHROPIC_MODEL_LIST:\n        if system_prompt != \"You are a helpful assistant.\":\n            user_prompt = \"[Instruction]\\n\" + system_prompt + \"\\n\\n\" + user_prompt\n            conv.messages[0][1] = user_prompt\n        judgment = chat_completion_anthropic(\n            model, conv, temperature=0, max_tokens=1024\n        )\n    else:\n        raise ValueError(f\"Invalid judge model name: {model}\")\n\n    if judge.prompt_template[\"output_format\"] == \"[[A]]\":\n        if \"[[A]]\" in judgment:\n            winner = \"A\"\n        elif \"[[B]]\" in judgment:\n            winner = \"B\"\n        elif \"[[C]]\" in judgment:\n            winner = \"tie\"\n        else:\n            winner = \"error\"\n    elif judge.prompt_template[\"output_format\"] == \"[[rating_a,rating_b]]\":\n        match = re.search(two_score_pattern, judgment)\n        if not match:\n            match = re.search(two_score_pattern_backup, judgment)\n        if match:\n            scores = [ast.literal_eval(s.strip()) for s in match.groups()]\n            if abs(scores[0] - scores[1]) <= TIE_DELTA:\n                winner = \"tie\"\n            elif scores[0] > scores[1]:\n                winner = \"A\"\n            else:\n                winner = \"B\"\n        else:\n            winner = \"error\"\n    else:\n        raise ValueError(\n            f\"invalid output format: {judge.prompt_template['output_format']}\"\n        )\n\n    return winner, user_prompt, judgment\n\n# Extraction patterns from fastchat/llm_judge/common.py\ntwo_score_pattern = re.compile(\"[[(\\d+.?\\d*),\\s?(\\d+.?\\d*)]]\")\none_score_pattern = re.compile(\"[[(\\d+.?\\d*)]]\")\n\n# From fastchat/llm_judge/gen_judgment.py\ndef play_a_match_pair(match: MatchPair, output_file: str):\n    question, model_1, model_2, answer_1, answer_2, judge, ref_answer, multi_turn = (\n        match.question,\n        match.model_1,\n        match.model_2,\n        match.answer_1,\n        match.answer_2,\n        match.judge,\n        match.ref_answer,\n        match.multi_turn,\n    )\n\n    if judge.prompt_template[\"type\"] == \"pairwise\":\n        g1_winner, g1_user_prompt, g1_judgment = run_judge_pair(\n            question, answer_1, answer_2, judge, ref_answer, multi_turn=multi_turn\n        )\n        g2_winner, g2_user_prompt, g2_judgment = run_judge_pair(\n            question, answer_2, answer_1, judge, ref_answer, multi_turn=multi_turn\n        )\n\n        g1_map = {\"A\": \"model_1\", \"B\": \"model_2\"}\n        g2_map = {\"A\": \"model_2\", \"B\": \"model_1\"}\n        g1_winner = g1_map.get(g1_winner, g1_winner)\n        g2_winner = g2_map.get(g2_winner, g2_winner)\n        question_id = question[\"question_id\"]\n        turn = 1 if not multi_turn else 2\n\n        result = {\n            \"question_id\": question_id,\n            \"model_1\": model_1,\n            \"model_2\": model_2,\n            \"g1_winner\": g1_winner,\n            \"g2_winner\": g2_winner,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"g1_user_prompt\": g1_user_prompt,\n            \"g1_judgment\": g1_judgment,\n            \"g2_user_prompt\": g2_user_prompt,\n            \"g2_judgment\": g2_judgment,\n            \"turn\": turn,\n            \"tstamp\": time.time(),\n        }\n\n        print(\n            f\"question: {question_id}, turn: {turn}, model_1: {model_1}, model_2: {model_2}, \"\n            f\"g1_winner: {g1_winner}, g2_winner: {g2_winner}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    elif judge.prompt_template[\"type\"] == \"single\":\n        m1_score, m1_user_prompt, m1_judgment = run_judge_single(\n            question, answer_1, judge\n        )\n        m2_score, m2_user_prompt, m2_judgment = run_judge_single(\n            question, answer_2, judge\n        )\n\n        if abs(m1_score - m2_score) <= TIE_DELTA:\n            winner = \"tie\"\n        elif m1_score > m2_score:\n            winner = \"model_1\"\n        else:\n            winner = \"model_2\"\n\n        question_id = question[\"question_id\"]\n        result = {\n            \"question_id\": question_id,\n            \"model_1\": model_1,\n            \"model_2\": model_2,\n            \"g1_winner\": winner,\n            \"g2_winner\": winner,\n            \"judge\": (judge.model_name, judge.prompt_template[\"name\"]),\n            \"g1_user_prompt\": m1_user_prompt,\n            \"g1_judgment\": m1_judgment,\n            \"g2_user_prompt\": m2_user_prompt,\n            \"g2_judgment\": m2_judgment,\n            \"m1_score\": m1_score,\n            \"m2_score\": m2_score,\n            \"tstamp\": time.time(),\n        }\n        print(\n            f\"question: {question_id}, model_1: {model_1}, model_2: {model_2}, \"\n            f\"winner: {winner}, m1_score: {m1_score}, m2_score: {m2_score}, \"\n            f\"judge: {(judge.model_name, judge.prompt_template['name'])}\"\n        )\n    else:\n        raise ValueError(f\"invalid judge type: {judge['type']}\")\n\n    if output_file:\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        with open(output_file, \"a\") as fout:\n            fout.write(json.dumps(result) + \"\\n\")\n\n    return result",
    "Experiment Result": "LLM for judgment: GPT-4 (as specified in `gen_judgment.py`'s `--judge-model` default).\nEvaluation benchmark: MT-bench (implied by `data/mt_bench/` directory and `show_result.py`).\nEvaluation modes: Single answer grading (`--mode single`) and Pairwise comparison (`--mode pairwise-baseline`, `--mode pairwise-all`) are supported.\nCategories needing reference answers: [\"math\", \"reasoning\", \"coding\", \"arena-hard-200\"] (from `common.py`).\nTemperature for judging: 0 (fixed in API calls within `chat_completion_openai` and `chat_completion_anthropic`).\nMax tokens for judging: 2048 for OpenAI models, 1024 for Anthropic models (fixed in API calls).\nOutput formats for scores from judge: \"[[rating]]\" (for single grading) or \"[[rating_a,rating_b]]\" (for pairwise comparison).\nTie delta for scores: 0.1 (difference below this value considered a tie in `run_judge_pair`).\nPerformance feedback: MT-Bench scores are evaluated and can be fed back as in-context examples for iterative refinement, with aggregation and display handled by `show_result.py`."
}{
    "Title": "Large Language Models as Optimizers",
    "Main Contributions": "The paper introduces Optimization by PROmpting (OPRO), an approach that leverages large language models (LLMs) as optimizers by describing optimization tasks in natural language. The LLM iteratively generates new solutions from a prompt containing previously generated solutions and their values. OPRO is demonstrated on linear regression, traveling salesman problems (TSP), and its primary application: prompt optimization. The key finding is that OPRO-optimized prompts significantly outperform human-designed prompts, achieving up to 8% higher accuracy on GSM8K and up to 50% higher accuracy on Big-Bench Hard tasks, across various LLMs.",
    "Methodology": "OPRO functions by using an LLM as an optimizer that generates candidate solutions based on a 'meta-prompt.' This meta-prompt comprises two core parts: a natural language description of the optimization problem (including objectives and constraints, referred to as 'meta-instructions') and an 'optimization trajectory' which contains past solutions and their scores, sorted in ascending order. To ensure optimization stability and balance exploration-exploitation, the LLM generates multiple solutions (typically 8) at each step, and its sampling temperature is tuned (default 1.0). For prompt optimization, a 'scorer LLM' evaluates generated prompts on a training set to provide accuracy scores, which are then fed back to the 'optimizer LLM' in the meta-prompt. Instructions can be inserted at Q_begin, Q_end, or A_begin positions. The meta-prompt also includes a few task exemplars to guide the optimizer LLM.",
    "Experimental Setup": "For mathematical optimization, OPRO was tested on linear regression (synthetic data, 50 data points, 5 random starting pairs for (w,b) in [10,20]) and TSP (n nodes with coordinates in [-100,100], 5 random starting solutions, Gurobi solver for oracle comparison). For prompt optimization, various LLMs were used as optimizers (PaLM 2-L, PaLM 2-L-IT, text-bison, gpt-3.5-turbo, gpt-4) and scorers (pre-trained PaLM 2-L, text-bison). Benchmarks included GSM8K (3.5% training examples for optimization, full test set for evaluation) and Big-Bench Hard (BBH) with 23 tasks (20% for optimization). Transferability was assessed on MultiArith and AQuA. Ablation studies explored meta-prompt design (instruction order, score presentation, number of exemplars), number of generated instructions per step, starting points, and optimizer temperature. Comparison was also made against EvoPrompt (GA and DE versions).",
    "Limitations": "OPRO is not intended to surpass state-of-the-art gradient-based algorithms for continuous optimization or specialized solvers for combinatorial problems like TSP. Key limitations include the LLM context window length limit, hindering large-scale problem descriptions (e.g., high-dimensional linear regression, large TSP instances). The optimization process can get stuck on bumpy loss landscapes or when in-context exemplars exhibit specific biases. LLMs may hallucinate values during math calculations or fail to reliably generate distinct solutions despite explicit instructions. For prompt optimization, challenges include sensitivity to initialization, balancing exploration and exploitation, and the optimizer LLM's inability to effectively leverage specific error cases (beyond aggregated accuracy) to infer improvement directions. Overfitting (training accuracy 5-20% higher than test accuracy) also occurs, though often mitigated by correlation between training and test performance.",
    "Future Research Directions": "Future work will focus on reducing sensitivity to initialization and better balancing exploitation with exploration. Specific directions include incorporating explicit natural language feedback on generated solutions for subsequent optimization steps, enriching feedback beyond aggregated accuracy to summarize key features of high/low-quality prompts, and potentially reducing the required training example set size. Further research could explore when and how to trigger external tools for reliable calculations (to prevent hallucination) and methods to ensure LLMs reliably follow instructions (e.g., generating truly novel solutions). Accelerating convergence from weaker starting points in prompt optimization is another promising area. Additionally, safeguarding LLM behavior against generating harmful information is an important ethical consideration.",
    "Experiment Code": "def gen_meta_prompt(old_instructions_and_scores, instruction_pos, optimizer_llm_name, old_instruction_score_threshold=0.1, max_num_instructions=1000, meta_prompt_type=\"both_instructions_and_exemplars\", few_shot_qa_pairs=False, include_qa=True, data=None, few_shot_index_list=None, instructions_before_exemplars=True, num_score_buckets=np.inf, dataset_name=\"\", task_name=\"\"):\"\"\"Generate meta prompt for instruction rewriting.Args:old_instructions_and_scores (list): a list of (instruction, score, i_step)pairs.instruction_pos (str): where to put the instruction, one of {'before_QA','Q_begin', 'Q_end', 'A_begin'}.optimizer_llm_name (str): the name of the LLM used for instruction editing.old_instruction_score_threshold (float): only add old instructions with scoreno less than this threshold.max_num_instructions (int): the maximum number of instructions in the metaprompt.meta_prompt_type (str): the type of meta-prompt: whether to have bothprevious instructions and dataset exemplars (often for fine-tunedoptimizers), or to have only previous instructions (often for pre-trainedoptimizers).few_shot_qa_pairs (bool): whether to have few-shot QA pairs in the metaprompt.include_qa (bool): whether to include \"Q:\" and \"A:\" formats in the prompt.data (list or pd.DataFrame): the raw data.few_shot_index_list (list): the list of indices of few-shot examples.instructions_before_exemplars (bool): whether the instruction-score pairs arebefore the exemplars from the dataset.num_score_buckets (np.inf or int): the number of score buckets when weconvert float accuracies to integers. Default to np.inf for notbucketizing.dataset_name (str): the name of the current dataset. Only used whengenerating task description when meta_prompt_type == \"instructions_only\".task_name (str): the name of the current task. Only used when generating taskdescription when meta_prompt_type == \"instructions_only\".Returns:meta_prompt (str): the generated meta prompt.\"\"\"assert instruction_pos in {\"before_Q\",\"Q_begin\",\"Q_end\",\"A_begin\",}, (\"The instruction position should be either before the question, or at the\"\" beginning of the question, at the end of the question, or at the\"\" beginning of the answer.\")assert meta_prompt_type in {\"both_instructions_and_exemplars\",\"instructions_only\",}assert dataset_name in {\"mmlu\",\"bbh\",\"gsm8k\",}, \"The lower-case dataset name must be one of mmlu, bbh, gsm8k.\"assert num_score_buckets == np.inf or isinstance(num_score_buckets, int)meta_prompt = \"\"if meta_prompt_type == \"both_instructions_and_exemplars\":if optimizer_llm_name.lower() in {\"gpt-3.5-turbo\", \"gpt-4\"}:if instruction_pos == \"A_begin\":meta_prompt_old_instruction_part = (\"Your task is to generate the answer starting sentence <Start>.\"\" Below are some previous starting sentences with their scores.\"\" The score ranges from 0 to 100.\\n\")else:meta_prompt_old_instruction_part = (\"Your task is to generate the instruction <INS>.\"\" Below are some previous instructions with their scores.\"\" The score ranges from 0 to 100.\\n\")else:assert optimizer_llm_name.lower() == \"text-bison\"meta_prompt_old_instruction_part = (\"I have some texts along with their corresponding scores.\"\" The texts are arranged in ascending order based on their scores,\"\" where higher scores indicate better quality.\\n\\n\")old_instructions_and_scores_str = gen_ins_and_score_pairs_substr(old_instructions_and_scores=old_instructions_and_scores,old_instruction_score_threshold=old_instruction_score_threshold,max_num_instructions=max_num_instructions,return_str_only=True,num_score_buckets=num_score_buckets,)meta_prompt_old_instruction_part += old_instructions_and_scores_strmeta_prompt_exemplar_part = \"\"if few_shot_qa_pairs:if optimizer_llm_name.lower() in {\"gpt-3.5-turbo\", \"gpt-4\"}:meta_prompt_exemplar_part += \"Below are some problems.\\n\"else:assert optimizer_llm_name.lower() == \"text-bison\"meta_prompt_exemplar_part += (\"The following exemplars show how to apply your text: you replace\"\" <INS> in each input with your text, then read the input and give\"\" an output. We say your output is wrong if your output is\"\" different from the given output, and we say your output is\"\" correct if they are the same. When replacing <INS> with an old\"\" piece of text above, we get wrong outputs on the following\"\" inputs.\\n\\n\")for idx in few_shot_index_list:if dataset_name == \"mmlu\":question = eval_utils._format_mmlu_example(data, idx)true_answer = data.iloc[idx, -1]elif dataset_name == \"bbh\":question = data[idx][\"input\"]true_answer = data[idx][\"target\"]else:assert dataset_name == \"gsm8k\"question = data.iloc[idx, 0]true_answer = data.iloc[idx, 1]if include_qa:if instruction_pos == \"before_Q\":meta_prompt_exemplar_part += f\"\\ninput:\\n<INS>\\nQ: {question}\\nA:\"elif instruction_pos == \"Q_begin\":meta_prompt_exemplar_part += f\"\\ninput:\\nQ: <INS>\\n{question}\\nA:\"elif instruction_pos == \"Q_end\":meta_prompt_exemplar_part += f\"\\ninput:\\nQ: {question}\\n<INS>\\nA:\"else:assert instruction_pos == \"A_begin\"if optimizer_llm_name.lower() in {\"gpt-3.5-turbo\", \"gpt-4\"}:meta_prompt_exemplar_part += f\"\\nQ: {question}\\nA: <Start>\"else:assert optimizer_llm_name.lower() == \"text-bison\"meta_prompt_exemplar_part += f\"\\ninput:\\nQ: {question}\\nA: <INS>\"else:assert instruction_pos in {\"Q_begin\", \"Q_end\"}if optimizer_llm_name.lower() in {\"gpt-3.5-turbo\", \"gpt-4\"}:if instruction_pos == \"Q_begin\":meta_prompt_exemplar_part += f\"\\nProblem:\\n<INS>\\n{question}\\n\"elif instruction_pos == \"Q_end\":meta_prompt_exemplar_part += f\"\\nProblem:\\n{question}\\n<INS>\\n\"else:assert optimizer_llm_name.lower() == \"text-bison\"if instruction_pos == \"Q_begin\":meta_prompt_exemplar_part += f\"\\ninput:\\n<INS>\\n{question}\\n\"elif instruction_pos == \"Q_end\":meta_prompt_exemplar_part += f\"\\ninput:\\n{question}\\n<INS>\\n\"if optimizer_llm_name.lower() in {\"gpt-3.5-turbo\", \"gpt-4\"}:meta_prompt_exemplar_part += (f\"\\nGround truth answer:\\n{true_answer}\\n\")else:assert optimizer_llm_name.lower() == \"text-bison\"meta_prompt_exemplar_part += f\"\\noutput:\\n{true_answer}\\n\"if few_shot_qa_pairs:if instructions_before_exemplars:meta_prompt += (meta_prompt_old_instruction_part+ \"\\n\\n\"+ meta_prompt_exemplar_part)else:meta_prompt += (meta_prompt_exemplar_part+ \"\\n\\n\"+ meta_prompt_old_instruction_part)else:meta_prompt += meta_prompt_old_instruction_partif optimizer_llm_name.lower() in {\"gpt-3.5-turbo\", \"gpt-4\"}:if instruction_pos == \"A_begin\":meta_prompt += (\"\\n\\nGenerate a starting sentence that is different from all the\"\" <Start> sentences above, and has a higher score than all the\"\" <Start> sentences above. The starting sentence should begin with\"\" <Start> and end with </Start>. The starting sentence should be\"\" concise, effective, and generally applicable to all QA pairs\"\" above.\")else:meta_prompt += (\"\\n\\nGenerate an instruction that\"\" is different from all the instructions <INS> above,\"\" and has a higher score than all the instructions <INS> above.\"\" The instruction should begin with <INS> and end with </INS>.\"\" The instruction should be concise, effective,\"\" and generally applicable to all problems above.\")else:assert optimizer_llm_name.lower() == \"text-bison\"meta_prompt += (\"\\n\\nWrite your new text that is different from the old ones and\"\" has a score as high as possible. Write the text in square brackets.\")else:assert meta_prompt_type == \"instructions_only\"assert instruction_pos in {\"Q_begin\", \"Q_end\", \"A_begin\"}if instruction_pos == \"Q_begin\":instruction_pos_description = \"at the beginning of the question\"elif instruction_pos == \"Q_end\":instruction_pos_description = \"at the end of the question\"else:assert instruction_pos == \"A_begin\"instruction_pos_description = \"at the beginning of the answer\"if dataset_name == \"gsm8k\":instruction_task_description = \"grade school math\"elif dataset_name == \"mmlu\":instruction_task_description = task_nameelse:assert dataset_name == \"bbh\"instruction_task_description = \" \".join(task_name.split(\"_\"))meta_instruction = (f\"Create a piece of text {instruction_pos_description.strip()} to\"\" enhance the precision in solving diverse\"f\" {instruction_task_description.strip()} problems.\")old_instructions_and_scores = sorted(old_instructions_and_scores, key=lambda x: x[1])old_instructions_and_scores_str = \"\"for instruction, score, _ in old_instructions_and_scores:if num_score_buckets == np.inf:score_to_show = round(score, 2)else:score_to_show = _bucketize_float(score, num_score_buckets)old_instructions_and_scores_str += (f\"\\n\\nPrecision: {score_to_show} <TEXT>{instruction}</TEXT>\")meta_prompt += meta_instruction + old_instructions_and_scores_strreturn meta_prompt",
    "Experiment Result": "The meta-prompt includes past solutions and their scores (optimization trajectory), sorted in ascending order. Instructions can be inserted at Q_begin, Q_end, or A_begin positions. The optimizer LLM generates multiple solutions at each step (typically 8), with its sampling temperature tuned (default 1.0). For prompt optimization, a scorer LLM evaluates generated prompts on a training set to provide accuracy scores, which are then fed back to the optimizer LLM in the meta-prompt. The meta-prompt also includes a few task exemplars to guide the optimizer LLM."
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "In DPO-style preference optimization the single hyper-parameter β simultaneously (1) scales the preference loss curvature and (2) weights the KL-divergence regulariser.  This coupling makes training unstable: small β under-regularises but also flattens the preference loss, while large β over-regularises and explodes the preference-loss gradient.  Reported failures of DiscoPOP/DPO for β≤0.01 or β≥2.5 stem directly from this entanglement.",
    "methods": "Temperature-Decoupled Direct Preference Optimisation (TD-DPO)\n1. Keep the KL term exactly as in DPO, still weighted by β.\n2. Introduce an independent temperature τ that ONLY rescales the preference margin Δ = log p_θ(y⁺|x) − log p_θ(y⁻|x).\n   TD-DPO loss per pair:\n       L = − log σ( Δ / τ )  +  β · KL(p_θ || p₀)\n   where σ is the sigmoid.  Setting τ<1 sharpens the preference signal without forcing a smaller KL, while τ>1 smooths gradients when β must be large for safety.\nTheoretically, this separates information-theoretic regularisation (β) from optimisation stability (τ), giving one extra scalar degree of freedom with negligible implementation cost.",
    "experimental_setup": "Model: open-source 7B Llama-2-chat (HF transformers).\nData: Argilla DPO-Mix-7k (same as DiscoPOP paper) – train/valid split 90/10.\nBaselines: (a) original DPO (single β), (b) DiscoPOP (best reported β), (c) proposed TD-DPO.\nHyper-grid: β ∈ {0.01,0.05,0.5,2.5}; τ ∈ {0.5,1.0,2.0} (TD-DPO only).\nOptimiser & hardware: AdamW, lr 1e-5, 4 × A100 80G, 3 epochs.\nEvaluation: MT-Bench score (GPT-4 judge) and validation pairwise accuracy.\nReport mean score over three seeds.",
    "experimental_code": "import torch, torch.nn.functional as F\n\ndef td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=0.05, tau=1.0):\n    \"\"\"TD-DPO pairwise loss.\n    logp_* : tensors of shape (batch,)\n    beta   : KL weight (as in DPO)\n    tau    : new temperature for preference margin\n    returns scalar loss\"\"\"\n    # preference term\n    delta = (logp_pos - logp_neg) / tau        # <-- only change w.r.t. DPO\n    pref_loss = -F.logsigmoid(delta).mean()\n    # KL term (same as DPO)\n    kl = 0.5*((logp_pos - logp_pos_ref)**2 + (logp_neg - logp_neg_ref)**2).mean()\n    return pref_loss + beta * kl\n\n# during training\n# logits_pos, logits_neg come from current model\n# logits_pos_ref, logits_neg_ref from frozen reference model\nloss = td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=beta, tau=tau)\nloss.backward()",
    "expected_result": "• When β is very small (0.01) TD-DPO with τ=0.5 regains strong gradients and matches performance of best-tuned DPO at β=0.05.\n• When β is large (2.5) TD-DPO with τ=2.0 avoids divergence; MT-Bench improves by ≈+1.5 points over baseline DiscoPOP.\n• For mid-range β (0.05) the default τ=1 keeps parity with DPO (<0.1 score difference).\nOverall, cross-β variance of MT-Bench scores is reduced by ~40 %, showing robustness.",
    "expected_conclusion": "A single extra scalar (τ) disentangles optimisation curvature from regularisation strength, eliminating observed instabilities of DPO/DiscoPOP with almost no code change.  Because the modification is orthogonal, existing hyper-parameter β sweeps remain valid, while τ offers a lightweight knob for further gains.  This demonstrates how a minimal but principled adjustment to the objective function yields measurable, practical improvements for LLM preference optimisation."
}
