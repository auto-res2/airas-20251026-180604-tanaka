
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training (full): `uv run python -u -m src.main run={run_id} results_dir={path} mode=full`
     * Training (trial): `uv run python -u -m src.main run={run_id} results_dir={path} mode=trial`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"`, `cfg.optuna.n_trials = 0`, epochs=1
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` and use full configuration

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
     * Metric names in train.py's wandb.log() MUST exactly match the keys used in evaluate.py's run.history()
   - Optuna Integration: If using Optuna, DO NOT log intermediate trial results to WandB - only log the final run with best hyperparameters
   - Code must automatically configure based on mode:
     * When `cfg.mode == "trial"`: Set `cfg.wandb.mode = "disabled"` before any WandB operations
     * When `cfg.mode == "full"`: Set `cfg.wandb.mode = "online"` before any WandB operations
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)
   - `WANDB_API_KEY` environment variable is available for authentication

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py loads WandB config from `config/config.yaml` (in repository root)
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
     * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Mode-Based Implementation**:
   - `mode` parameter controls experiment behavior (required parameter)
   - When `cfg.mode == "trial"`:
     * Properly reduces computational load: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
     * Automatically sets `cfg.wandb.mode = "disabled"`
     * Purpose: Fast validation that code runs without errors
   - When `cfg.mode == "full"`:
     * Automatically sets `cfg.wandb.mode = "online"`
     * Uses full configuration (full epochs, full Optuna trials, etc.)

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
{
    "Open Problems": "In DPO-style preference optimization the single hyper-parameter β simultaneously (1) scales the preference loss curvature and (2) weights the KL-divergence regulariser.  This coupling makes training unstable: small β under-regularises but also flattens the preference loss, while large β over-regularises and explodes the preference-loss gradient.  Reported failures of DiscoPOP/DPO for β≤0.01 or β≥2.5 stem directly from this entanglement.",
    "Methods": "Temperature-Decoupled Direct Preference Optimisation (TD-DPO)\n1. Keep the KL term exactly as in DPO, still weighted by β.\n2. Introduce an independent temperature τ that ONLY rescales the preference margin Δ = log p_θ(y⁺|x) − log p_θ(y⁻|x).\n   TD-DPO loss per pair:\n       L = − log σ( Δ / τ )  +  β · KL(p_θ || p₀)\n   where σ is the sigmoid.  Setting τ<1 sharpens the preference signal without forcing a smaller KL, while τ>1 smooths gradients when β must be large for safety.\nTheoretically, this separates information-theoretic regularisation (β) from optimisation stability (τ), giving one extra scalar degree of freedom with negligible implementation cost.",
    "Experimental Setup": "Model: open-source 7B Llama-2-chat (HF transformers).\nData: Argilla DPO-Mix-7k (same as DiscoPOP paper) – train/valid split 90/10.\nBaselines: (a) original DPO (single β), (b) DiscoPOP (best reported β), (c) proposed TD-DPO.\nHyper-grid: β ∈ {0.01,0.05,0.5,2.5}; τ ∈ {0.5,1.0,2.0} (TD-DPO only).\nOptimiser & hardware: AdamW, lr 1e-5, 4 × A100 80G, 3 epochs.\nEvaluation: MT-Bench score (GPT-4 judge) and validation pairwise accuracy.\nReport mean score over three seeds.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=0.05, tau=1.0):\n    \"\"\"TD-DPO pairwise loss.\n    logp_* : tensors of shape (batch,)\n    beta   : KL weight (as in DPO)\n    tau    : new temperature for preference margin\n    returns scalar loss\"\"\"\n    # preference term\n    delta = (logp_pos - logp_neg) / tau        # <-- only change w.r.t. DPO\n    pref_loss = -F.logsigmoid(delta).mean()\n    # KL term (same as DPO)\n    kl = 0.5*((logp_pos - logp_pos_ref)**2 + (logp_neg - logp_neg_ref)**2).mean()\n    return pref_loss + beta * kl\n\n# during training\n# logits_pos, logits_neg come from current model\n# logits_pos_ref, logits_neg_ref from frozen reference model\nloss = td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=beta, tau=tau)\nloss.backward()",
    "Expected Result": "• When β is very small (0.01) TD-DPO with τ=0.5 regains strong gradients and matches performance of best-tuned DPO at β=0.05.\n• When β is large (2.5) TD-DPO with τ=2.0 avoids divergence; MT-Bench improves by ≈+1.5 points over baseline DiscoPOP.\n• For mid-range β (0.05) the default τ=1 keeps parity with DPO (<0.1 score difference).\nOverall, cross-β variance of MT-Bench scores is reduced by ~40 %, showing robustness.",
    "Expected Conclusion": "A single extra scalar (τ) disentangles optimisation curvature from regularisation strength, eliminating observed instabilities of DPO/DiscoPOP with almost no code change.  Because the modification is orthogonal, existing hyper-parameter β sweeps remain valid, while τ offers a lightweight knob for further gains.  This demonstrates how a minimal but principled adjustment to the objective function yields measurable, practical improvements for LLM preference optimisation."
}

# Experimental Design
- Strategy: Purpose: verify that Temperature-Decoupled Direct Preference Optimisation (TD-DPO) makes preference-finetuning of large language models more stable and robust than standard Direct Preference Optimisation (DPO).

Components & workflow:
1. Base model: load HuggingFace “Llama-2-Chat-7B” in 4-bit QLoRA format so it fits comfortably on a single A100; keep a frozen copy as reference policy.
2. Data: use the public Argilla DPO-Mix-7k pair-preference corpus (≈6.3 k train / 0.7 k val). Each sample provides (x, y⁺, y⁻).
3. Training loops (three random seeds):
   a. Baseline run with standard DPO loss for four β values {0.01,0.05,0.5,2.5}.
   b. Proposed TD-DPO run on the same β grid while sweeping the additional temperature τ ∈ {0.5,1.0,2.0}.
   c. optimiser: AdamW, lr 1e-5, weight-decay 0.01, batch 128, 3 epochs; gradient-accumulation and FSDP across 4×A100 (≤60 GB GPU-RAM used).
4. During training compute validation pairwise accuracy every 250 steps; keep best checkpoint by this metric.
5. Post-training evaluation: generate answers on MT-Bench v1.1, score with GPT-4 auto-judge, report mean of three seeds.
6. Analysis: compare MT-Bench and validation accuracy across β; measure variance reduction and highlight cases β=0.01 and β=2.5 where TD-DPO is expected to outperform.

The experiment therefore demonstrates that decoupling curvature (τ) from regularisation (β) yields higher scores and lower sensitivity with negligible extra cost.
- Proposed Method: Temperature-Decoupled Direct Preference Optimisation (TD-DPO)
Objective: stabilise preference-based RL-free finetuning by separating the roles of the single DPO hyper-parameter β.
Theory: In DPO, β simultaneously scales the KL-divergence regulariser and the curvature of the pairwise preference loss. TD-DPO introduces an independent temperature τ that rescales only the preference margin Δ = log pθ(y⁺|x) − log pθ(y⁻|x). This yields:
   L(x,y⁺,y⁻) = −log σ(Δ/τ) + β · KL( pθ || p0 ).
Properties:
• β continues to control the information-theoretic distance from the reference policy.
• τ (<1 sharpen, >1 smooth) controls optimisation curvature without touching the KL term.
Algorithmic procedure per minibatch:
1. Forward pass current model to get log-probs for preferred and dispreferred answers.
2. Forward pass frozen reference model for the same answers.
3. Compute Δ, divide by τ, compute −log σ to obtain preference loss.
4. Compute symmetric KL between current and reference log-probs.
5. Combine losses as above; back-propagate, update with AdamW.
6. Repeat for all β,τ grid points. Hyper-parameter search chooses (β,τ) that maximises validation accuracy.
Implementation adds two code lines to standard DPO: one temperature division and one hyper-parameter entry, so existing codebases can adopt TD-DPO instantly.
- Evaluation Metrics: ['MT-Bench score', 'Pairwise accuracy']

# Experiment Runs

- Run ID: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
  Method: proposed
  Model: Llama-2-Chat-7B
  Dataset: Argilla DPO-Mix-7k
  
  Config Content:
  ```yaml
  run_id: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
method: TD-DPO
model:
  name: meta-llama/Llama-2-7b-chat-hf
  quantization: qlora-4bit
  load_in_4bit: true
  torch_dtype: bfloat16
  peft:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.05
dataset:
  name: argilla/dpo-mix-7k
  split:
    train: 0.9
    val: 0.1
  preprocessing:
    max_length: 2048
training:
  epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32
  global_batch_size: 128
  learning_rate: 1e-5
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler_type: cosine
  warmup_steps: 100
  evaluation_strategy: steps
  eval_steps: 250
  save_total_limit: 1
  save_strategy: best
  max_grad_norm: 1.0
  bf16: true
  logging_steps: 25
  seed_list: [42, 43, 44]
  report_to: wandb
loss:
  name: td_dpo_loss
  beta: ${opt.beta}
  tau: ${opt.tau}
  kl_reference_model: true
compute:
  gpus: 4
  gpu_type: a100-80gb
  strategy: fsdp
optuna:
  n_trials: 36
  direction: minimize
  objective_metric: val_pairwise_accuracy
  search_space:
    beta:
      type: categorical
      choices: [0.01, 0.05, 0.5, 2.5]
    tau:
      type: categorical
      choices: [0.5, 1.0, 2.0]
    learning_rate:
      type: loguniform
      low: 5e-6
      high: 5e-5

  ```
  

- Run ID: comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
  Method: comparative-1
  Model: Llama-2-Chat-7B
  Dataset: Argilla DPO-Mix-7k
  
  Config Content:
  ```yaml
  run_id: comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k
method: DPO
model:
  name: meta-llama/Llama-2-7b-chat-hf
  quantization: qlora-4bit
  load_in_4bit: true
  torch_dtype: bfloat16
  peft:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.05
dataset:
  name: argilla/dpo-mix-7k
  split:
    train: 0.9
    val: 0.1
  preprocessing:
    max_length: 2048
training:
  epochs: 3
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 32
  global_batch_size: 128
  learning_rate: 1e-5
  weight_decay: 0.01
  optimizer: adamw
  lr_scheduler_type: cosine
  warmup_steps: 100
  evaluation_strategy: steps
  eval_steps: 250
  save_total_limit: 1
  save_strategy: best
  max_grad_norm: 1.0
  bf16: true
  logging_steps: 25
  seed_list: [42, 43, 44]
  report_to: wandb
loss:
  name: dpo_loss
  beta: ${opt.beta}
  kl_reference_model: true
compute:
  gpus: 4
  gpu_type: a100-80gb
  strategy: fsdp
optuna:
  n_trials: 20
  direction: minimize
  objective_metric: val_pairwise_accuracy
  search_space:
    beta:
      type: categorical
      choices: [0.01, 0.05, 0.5, 2.5]
    learning_rate:
      type: loguniform
      low: 5e-6
      high: 5e-5

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n  - run: proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k  # default, override via CLI\n\n# Global parameters -----------------------------------------------------------\nmode: full               # trial / full (override via CLI)\nresults_dir: ./results   # where to save checkpoints \u0026 analysis\n\nwandb:\n  entity: gengaru617-personal\n  project: 251023-test\n  mode: online           # auto-overridden depending on mode\n\n# Placeholder \u2013 content injected from run YAML\nrun: {}\n", "evaluate_py": "\"\"\"Independent evaluation \u0026 visualisation script.\n\nUsage:\n    uv run python -m src.evaluate results_dir=/path/to/out run_ids=\u0027[\"run-1\",\"run-2\"]\u0027\n\nThe script intentionally does *not* rely on Hydra so that it can be called\nas-is by CI workflows.\n\"\"\"\nimport json\nimport os\nimport sys\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\nfrom tqdm.auto import tqdm\n\n# -----------------------------------------------------------------------------\n# Argument parsing (key=value pairs as specified in the CLI contract)\n# -----------------------------------------------------------------------------\n\ndef parse_cli_kv() -\u003e Dict[str, str]:\n    params: Dict[str, str] = {}\n    for token in sys.argv[1:]:\n        if \"=\" not in token:\n            raise ValueError(\n                f\"Invalid argument \u0027{token}\u0027. Expected key=value style as per spec.\"\n            )\n        k, v = token.split(\"=\", 1)\n        params[k] = v\n    required = {\"results_dir\", \"run_ids\"}\n    missing = required - params.keys()\n    if missing:\n        raise ValueError(f\"Missing required arguments: {missing}\")\n    return params\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef save_json(obj, path):\n    with open(path, \"w\") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef plot_learning_curve(history_df, run_id: str, save_dir: str) -\u003e str:\n    fig, ax = plt.subplots(figsize=(8, 5))\n    possible_cols = [\n        (\"train/loss\", \"train/loss\"),\n        (\"train/acc\", \"train/acc\"),\n        (\"val/val_pairwise_accuracy\", \"val/accuracy\"),\n    ]\n    for col, label in possible_cols:\n        if col in history_df.columns:\n            ax.plot(history_df[\"step\"], history_df[col], label=label)\n    ax.set_xlabel(\"Step\")\n    ax.set_title(f\"Learning Curves \u2013 {run_id}\")\n    ax.legend()\n    fig.tight_layout()\n    path = os.path.join(save_dir, f\"{run_id}_learning_curve.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef plot_confusion_matrix(summary: Dict[str, float], run_id: str, save_dir: str) -\u003e str:\n    tp = summary.get(\"val_true_pos\", summary.get(\"val_correct\", 0))\n    fp = summary.get(\"val_false_pos\", 0)\n    tn = summary.get(\"val_true_neg\", 0)\n    fn = summary.get(\"val_false_neg\", 0)\n    matrix = [[tp, fp], [fn, tn]]\n    fig, ax = plt.subplots(figsize=(4, 4))\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n    ax.set_xlabel(\"Predicted + / -\")\n    ax.set_ylabel(\"Actual + / -\")\n    ax.set_title(f\"Confusion Matrix \u2013 {run_id}\")\n    fig.tight_layout()\n    path = os.path.join(save_dir, f\"{run_id}_confusion_matrix.pdf\")\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n# -----------------------------------------------------------------------------\n# Per-run processing\n# -----------------------------------------------------------------------------\n\ndef process_single_run(api: wandb.Api, entity: str, project: str, run_id: str, out_dir: str):\n    run = api.run(f\"{entity}/{project}/{run_id}\")\n    history = run.history()  # pd.DataFrame\n    summary = run.summary._json_dict\n    config = dict(run.config)\n\n    os.makedirs(out_dir, exist_ok=True)\n    metrics_path = os.path.join(out_dir, \"metrics.json\")\n    save_json({\"summary\": summary, \"config\": config}, metrics_path)\n\n    figs: List[str] = []\n    figs.append(plot_learning_curve(history, run_id, out_dir))\n    figs.append(plot_confusion_matrix(summary, run_id, out_dir))\n    return {\n        \"metrics_path\": metrics_path,\n        \"figs\": figs,\n        \"summary\": summary,\n    }\n\n# -----------------------------------------------------------------------------\n# Aggregated comparison across runs\n# -----------------------------------------------------------------------------\n\ndef aggregated_analysis(per_run: Dict[str, Dict], comp_dir: str) -\u003e List[str]:\n    os.makedirs(comp_dir, exist_ok=True)\n\n    # Save combined summaries\n    combined = {rid: d[\"summary\"] for rid, d in per_run.items()}\n    save_json(combined, os.path.join(comp_dir, \"aggregated_metrics.json\"))\n\n    # Example bar-chart: best val accuracy\n    run_ids = list(per_run.keys())\n    accuracies = [d[\"summary\"].get(\"best_val_pairwise_accuracy\", 0) for d in per_run.values()]\n\n    fig, ax = plt.subplots(figsize=(max(4, len(run_ids) * 1.6), 4))\n    sns.barplot(x=run_ids, y=accuracies, palette=\"viridis\", ax=ax)\n    for i, v in enumerate(accuracies):\n        ax.text(i, v + 0.005, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    ax.set_ylim(0, 1)\n    ax.set_ylabel(\"Best Val Pairwise Acc.\")\n    ax.set_title(\"Cross-run Comparison\")\n    fig.tight_layout()\n\n    bar_path = os.path.join(comp_dir, \"comparison_accuracy_bar_chart.pdf\")\n    fig.savefig(bar_path)\n    plt.close(fig)\n    return [bar_path]\n\n# -----------------------------------------------------------------------------\n# Main\n# -----------------------------------------------------------------------------\n\ndef main():\n    args = parse_cli_kv()\n    results_dir, run_ids_json = args[\"results_dir\"], args[\"run_ids\"]\n    run_ids = json.loads(run_ids_json)\n\n    # Global WandB config\n    with open(os.path.join(\"config\", \"config.yaml\")) as f:\n        import yaml\n\n        cfg = yaml.safe_load(f)\n    entity, project = cfg[\"wandb\"][\"entity\"], cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    per_run = {}\n    for rid in tqdm(run_ids, desc=\"Processing runs\"):\n        run_dir = os.path.join(results_dir, rid)\n        per_run[rid] = process_single_run(api, entity, project, rid, run_dir)\n        print(f\"Saved metrics for {rid} \u2192 {per_run[rid][\u0027metrics_path\u0027]}\")\n        for fp in per_run[rid][\"figs\"]:\n            print(fp)\n\n    comp_dir = os.path.join(results_dir, \"comparison\")\n    comp_figs = aggregated_analysis(per_run, comp_dir)\n    print(f\"Aggregated outputs: {comp_dir}\")\n    for fp in comp_figs:\n        print(fp)\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "import subprocess\nimport hydra\nfrom omegaconf import OmegaConf\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    # Mode-specific tweaks ------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Iterate over seeds --------------------------------------------------\n    for seed in cfg.training.seed_list:\n        run_id_seed = f\"{cfg.run.run_id}-seed{seed}\"\n        overrides = [\n            f\"run.run_id={run_id_seed}\",\n            f\"training.seed={seed}\",\n            f\"results_dir={cfg.results_dir}\",\n            f\"wandb.mode={cfg.wandb.mode}\",\n            f\"mode={cfg.mode}\",\n        ]\n        cmd = [\"python\", \"-u\", \"-m\", \"src.train\"] + overrides\n        print(\"Launching:\", \" \".join(cmd))\n        subprocess.run(cmd, check=True)\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "\"\"\"Model loading \u0026 log-prob computation utilities.\"\"\"\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\n\n__all__ = [\"load_models\", \"compute_logps\"]\n\n# -----------------------------------------------------------------------------\n# Loading 4-bit QLoRA model \u0026 frozen reference\n# -----------------------------------------------------------------------------\n\ndef load_models(cfg, tokenizer):\n    bnb_config = None\n    if cfg.model.get(\"load_in_4bit\", False):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    model.gradient_checkpointing_enable()\n\n    if cfg.model.get(\"peft\"):\n        lora_cfg = LoraConfig(\n            r=cfg.model.peft.lora_r,\n            lora_alpha=cfg.model.peft.lora_alpha,\n            lora_dropout=cfg.model.peft.lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n        )\n        model = get_peft_model(model, lora_cfg)\n\n    # Frozen reference model \u2013 no LoRA\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        cfg.model.name,\n        cache_dir=\".cache/\",\n        torch_dtype=getattr(torch, cfg.model.torch_dtype),\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n    )\n    ref_model.eval()\n    for p in ref_model.parameters():\n        p.requires_grad_(False)\n\n    return model, ref_model\n\n# -----------------------------------------------------------------------------\n# Log-prob computation\n# -----------------------------------------------------------------------------\n\ndef _gather_log_probs(logits: torch.Tensor, input_ids: torch.Tensor):\n    logp = torch.nn.functional.log_softmax(logits, dim=-1)\n    logp = logp[:, :-1, :]  # shift so that next-token logits align with targets\n    target = input_ids[:, 1:]\n    gathered = torch.gather(logp, 2, target.unsqueeze(-1)).squeeze(-1)\n    return gathered\n\n\ndef _forward(model, batch_part, device):\n    input_ids = batch_part[\"input_ids\"].to(device)\n    attention_mask = batch_part[\"attention_mask\"].to(device)\n    response_mask = batch_part[\"response_mask\"].to(device)[:, 1:]  # align\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    log_probs = _gather_log_probs(outputs.logits, input_ids)\n    seq_logp = (log_probs * response_mask).sum(dim=1) / response_mask.sum(dim=1)\n    return seq_logp\n\n\ndef compute_logps(model, ref_model, batch, device):\n    \"\"\"Returns tuple: (logp_pos, logp_neg, logp_pos_ref, logp_neg_ref).\"\"\"\n    pos, neg = batch[\"pos\"], batch[\"neg\"]\n\n    # Reference model (no grad)\n    with torch.no_grad():\n        logp_pos_ref = _forward(ref_model, pos, device)\n        logp_neg_ref = _forward(ref_model, neg, device)\n\n    # Trainable model (grad enabled)\n    logp_pos = _forward(model, pos, device)\n    logp_neg = _forward(model, neg, device)\n\n    return logp_pos, logp_neg, logp_pos_ref, logp_neg_ref\n", "preprocess_py": "import datasets\nfrom datasets import load_dataset\nfrom typing import List, Tuple\nimport torch\nfrom transformers import AutoTokenizer\n\n# -----------------------------------------------------------------------------\n# Dataset definitions\n# -----------------------------------------------------------------------------\n\nclass PairPreferenceDataset(torch.utils.data.Dataset):\n    \"\"\"Wraps an HF dataset providing (prompt, pos, neg) per item.\"\"\"\n\n    def __init__(self, hf_ds: datasets.Dataset, tokenizer: AutoTokenizer):\n        self.hf_ds = hf_ds\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.hf_ds)\n\n    def __getitem__(self, idx):\n        sample = self.hf_ds[idx]\n        # Prompt field\n        prompt = (\n            sample.get(\"prompt\")\n            or sample.get(\"instruction\")\n            or sample.get(\"question\")\n            or \"\"\n        )\n        # Positive / negative completions according to dataset conventions\n        if {\n            \"completion_a\",\n            \"completion_b\",\n            \"choice\",\n        }.issubset(sample.keys()):\n            pos, neg = (\n                (sample[\"completion_a\"], sample[\"completion_b\"])\n                if sample[\"choice\"] == 0\n                else (sample[\"completion_b\"], sample[\"completion_a\"])\n            )\n        elif {\"chosen\", \"rejected\"}.issubset(sample.keys()):\n            pos, neg = sample[\"chosen\"], sample[\"rejected\"]\n        else:\n            raise KeyError(\"Dataset sample does not contain recognised preference fields\")\n        return {\"prompt\": prompt, \"pos\": pos, \"neg\": neg}\n\n\nclass PreferenceCollator:\n    \"\"\"Tokenises prompt+response pairs and builds masks.\"\"\"\n\n    def __init__(self, tokenizer: AutoTokenizer, max_length: int = 2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.eos = tokenizer.eos_token or \"\u003c/s\u003e\"\n\n    def _build(self, prompts: List[str], responses: List[str]):\n        joined = [p + self.eos + r + self.eos for p, r in zip(prompts, responses)]\n        tokenised = self.tokenizer(\n            joined,\n            padding=True,\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\",\n        )\n        # Build response mask (prompt tokens =0, response tokens =1)\n        mask = torch.zeros_like(tokenised[\"input_ids\"], dtype=torch.bool)\n        for i, (p, r) in enumerate(zip(prompts, responses)):\n            prompt_len = len(self.tokenizer(p + self.eos)[\"input_ids\"])\n            seq_len = tokenised[\"attention_mask\"][i].sum().item()\n            mask[i, prompt_len : seq_len] = 1\n        tokenised[\"response_mask\"] = mask\n        return tokenised\n\n    def __call__(self, batch: List[dict]):\n        prompts = [b[\"prompt\"] for b in batch]\n        pos_responses = [b[\"pos\"] for b in batch]\n        neg_responses = [b[\"neg\"] for b in batch]\n        return {\n            \"pos\": self._build(prompts, pos_responses),\n            \"neg\": self._build(prompts, neg_responses),\n        }\n\n\n# -----------------------------------------------------------------------------\n# Public helper to get (tokenizer, train_ds, val_ds)\n# -----------------------------------------------------------------------------\n\ndef prepare_datasets(cfg) -\u003e Tuple[AutoTokenizer, torch.utils.data.Dataset, torch.utils.data.Dataset]:\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\", use_fast=True)\n\n    raw = load_dataset(cfg.dataset.name, cache_dir=\".cache/\")\n    if \"train\" in raw and \"validation\" in raw:\n        train_raw, val_raw = raw[\"train\"], raw[\"validation\"]\n    else:\n        split = raw[\"train\"].train_test_split(test_size=cfg.dataset.split.val)\n        train_raw, val_raw = split[\"train\"], split[\"test\"]\n\n    train_ds = PairPreferenceDataset(train_raw, tokenizer)\n    val_ds = PairPreferenceDataset(val_raw, tokenizer)\n    return tokenizer, train_ds, val_ds\n", "pyproject_toml": "[project]\nname = \"td_dpo_experiments\"\nversion = \"0.1.0\"\ndescription = \"Temperature-Decoupled DPO experiments\"\nrequires-python = \"\u003e=3.9\"\n\n[project.dependencies]\ntorch = \"\u003e=2.0\"\ntransformers = \"\u003e=4.34.0\"\ndatasets = \"\u003e=2.14.0\"\npeft = \"\u003e=0.5.0\"\nbitsandbytes = \"*\"\nhydra-core = \"\u003e=1.3.2\"\nwandb = \"\u003e=0.15.12\"\noptuna = \"\u003e=3.3.0\"\nmatplotlib = \"\u003e=3.7.0\"\nseaborn = \"\u003e=0.13.0\"\n", "train_py": "import os\nimport math\nimport random\nfrom typing import Dict, Any\n\nimport hydra\nimport torch\nimport wandb\nimport optuna\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\nfrom tqdm.auto import tqdm\n\nfrom src.preprocess import prepare_datasets, PreferenceCollator\nfrom src.model import load_models, compute_logps\n\n# -----------------------------------------------------------------------------\n# Loss functions\n# -----------------------------------------------------------------------------\n\ndef dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n) -\u003e torch.Tensor:\n    \"\"\"Standard DPO objective.\"\"\"\n    preference_term = -torch.log(torch.sigmoid(logp_pos - logp_neg)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n\ndef td_dpo_loss(\n    logp_pos: torch.Tensor,\n    logp_neg: torch.Tensor,\n    logp_pos_ref: torch.Tensor,\n    logp_neg_ref: torch.Tensor,\n    beta: float,\n    tau: float,\n) -\u003e torch.Tensor:\n    \"\"\"Temperature\u2013Decoupled DPO.\"\"\"\n    preference_term = -torch.log(torch.sigmoid((logp_pos - logp_neg) / tau)).mean()\n    kl = 0.5 * (((logp_pos - logp_pos_ref) ** 2) + ((logp_neg - logp_neg_ref) ** 2)).mean()\n    return preference_term + beta * kl\n\n# -----------------------------------------------------------------------------\n# Validation loop\n# -----------------------------------------------------------------------------\n\ndef evaluate(\n    model: torch.nn.Module,\n    ref_model: torch.nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n) -\u003e Dict[str, Any]:\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch in loader:\n            logp_pos, logp_neg, *_ = compute_logps(model, ref_model, batch, device)\n            correct += (logp_pos \u003e logp_neg).sum().item()\n            total += logp_pos.size(0)\n    model.train()\n    return {\"val_pairwise_accuracy\": correct / total if total \u003e 0 else 0.0}\n\n# -----------------------------------------------------------------------------\n# Training loop (single run, single seed)\n# -----------------------------------------------------------------------------\n\ndef run_training(cfg, trial=None):\n    # ------------------------------------------------------------------\n    # Reproducibility\n    # ------------------------------------------------------------------\n    torch.manual_seed(cfg.training.seed)\n    random.seed(cfg.training.seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(cfg.training.seed)\n\n    # ------------------------------------------------------------------\n    # Data \u0026 model\n    # ------------------------------------------------------------------\n    tokenizer, train_ds, val_ds = prepare_datasets(cfg)\n    model, ref_model = load_models(cfg, tokenizer)\n\n    collator = PreferenceCollator(\n        tokenizer, max_length=cfg.dataset.preprocessing.max_length\n    )\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=collator,\n    )\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=cfg.training.per_device_train_batch_size,\n        shuffle=False,\n        num_workers=2,\n        collate_fn=collator,\n    )\n\n    # ------------------------------------------------------------------\n    # Optimiser \u0026 scheduler\n    # ------------------------------------------------------------------\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    params = [\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": cfg.training.weight_decay,\n        },\n        {\n            \"params\": [\n                p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(params, lr=cfg.training.learning_rate)\n\n    num_update_steps_per_epoch = math.ceil(\n        len(train_loader) / cfg.training.gradient_accumulation_steps\n    )\n    max_steps = cfg.training.epochs * num_update_steps_per_epoch\n\n    if cfg.training.lr_scheduler_type == \"cosine\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n    else:\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, cfg.training.warmup_steps, max_steps\n        )\n\n    # ------------------------------------------------------------------\n    # WandB init (disabled in trial \u0026 Optuna search)\n    # ------------------------------------------------------------------\n    if cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB URL: {wandb_run.url}\")\n    else:\n        wandb_run = None\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n    # ------------------------------------------------------------------\n    # Move models to device\n    # ------------------------------------------------------------------\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    ref_model.to(device)\n    ref_model.eval()\n\n    # ------------------------------------------------------------------\n    # Training\n    # ------------------------------------------------------------------\n    best_val_acc, global_step = 0.0, 0\n    for epoch in range(cfg.training.epochs):\n        model.train()\n        epoch_loss, correct, total = 0.0, 0, 0\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.training.epochs}\")\n        for step, batch in enumerate(pbar, start=1):\n            logp_pos, logp_neg, logp_pos_ref, logp_neg_ref = compute_logps(\n                model, ref_model, batch, device\n            )\n            # choose loss type\n            if cfg.loss.name == \"dpo_loss\":\n                loss = dpo_loss(\n                    logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, cfg.loss.beta\n                )\n            else:\n                loss = td_dpo_loss(\n                    logp_pos,\n                    logp_neg,\n                    logp_pos_ref,\n                    logp_neg_ref,\n                    cfg.loss.beta,\n                    cfg.loss.tau,\n                )\n            loss = loss / cfg.training.gradient_accumulation_steps\n            loss.backward()\n            epoch_loss += loss.item()\n\n            correct += (logp_pos \u003e logp_neg).sum().item()\n            total += logp_pos.size(0)\n\n            if step % cfg.training.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                global_step += 1\n\n                if wandb_run and global_step % cfg.training.logging_steps == 0:\n                    wandb.log(\n                        {\n                            \"train/loss\": loss.item()\n                            * cfg.training.gradient_accumulation_steps,\n                            \"train/acc\": correct / total,\n                            \"lr\": scheduler.get_last_lr()[0],\n                            \"step\": global_step,\n                        },\n                        step=global_step,\n                    )\n\n                if (\n                    cfg.training.evaluation_strategy == \"steps\"\n                    and global_step % cfg.training.eval_steps == 0\n                ):\n                    val_metrics = evaluate(model, ref_model, val_loader, device)\n                    if wandb_run:\n                        wandb.log({f\"val/{k}\": v for k, v in val_metrics.items()}, step=global_step)\n                    if val_metrics[\"val_pairwise_accuracy\"] \u003e best_val_acc:\n                        best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n                        save_best(model, tokenizer, cfg)\n                    model.train()\n\n        # End-of-epoch validation\n        val_metrics = evaluate(model, ref_model, val_loader, device)\n        if wandb_run:\n            wandb.log({f\"val/{k}\": v for k, v in val_metrics.items()}, step=global_step)\n        if val_metrics[\"val_pairwise_accuracy\"] \u003e best_val_acc:\n            best_val_acc = val_metrics[\"val_pairwise_accuracy\"]\n            save_best(model, tokenizer, cfg)\n\n    # ------------------------------------------------------------------\n    # Wrap-up\n    # ------------------------------------------------------------------\n    if wandb_run:\n        wandb_run.summary[\"best_val_pairwise_accuracy\"] = best_val_acc\n        wandb_run.finish()\n\n    return best_val_acc\n\n\n# -----------------------------------------------------------------------------\n# Utilities\n# -----------------------------------------------------------------------------\n\ndef save_best(model, tokenizer, cfg):\n    save_path = os.path.join(cfg.results_dir, cfg.run.run_id, \"best\")\n    os.makedirs(save_path, exist_ok=True)\n    model.save_pretrained(save_path)\n    tokenizer.save_pretrained(save_path)\n\n\n# -----------------------------------------------------------------------------\n# Optuna objective\n# -----------------------------------------------------------------------------\n\ndef optuna_objective(trial: optuna.Trial, base_cfg):\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))  # deepcopy\n\n    # Sample hyper-parameters\n    for hp_name, hp_space in cfg.optuna.search_space.items():\n        if hp_space.type == \"categorical\":\n            sampled = trial.suggest_categorical(hp_name, hp_space.choices)\n        elif hp_space.type == \"loguniform\":\n            sampled = trial.suggest_float(hp_name, hp_space.low, hp_space.high, log=True)\n        else:\n            raise ValueError(f\"Unsupported space type: {hp_space.type}\")\n        # write back to cfg (supports both loss \u0026 training params)\n        target_field = f\"loss.{hp_name}\" if hp_name in [\"beta\", \"tau\"] else f\"training.{hp_name}\"\n        OmegaConf.update(cfg, target_field, sampled, merge=False)\n\n    # Disable WandB during search\n    cfg.wandb.mode = \"disabled\"\n    val_acc = run_training(cfg)\n    return -val_acc  # minimise negative accuracy\n\n\n# -----------------------------------------------------------------------------\n# CLI entry-point\n# -----------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg):\n    os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n    os.makedirs(os.path.join(cfg.results_dir, cfg.run.run_id), exist_ok=True)\n\n    # Mode adjustments --------------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.epochs = 1\n        cfg.training.eval_steps = max(cfg.training.eval_steps, 10)\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be \u0027trial\u0027 or \u0027full\u0027\")\n\n    # Optuna hyper-param search -----------------------------------------\n    if cfg.optuna.n_trials and cfg.optuna.n_trials \u003e 0:\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: optuna_objective(t, cfg), n_trials=cfg.optuna.n_trials)\n        print(\"[Optuna] Best params:\", study.best_params)\n        # Update cfg with best params for final training\n        for k, v in study.best_params.items():\n            OmegaConf.update(\n                cfg,\n                f\"loss.{k}\" if k in [\"beta\", \"tau\"] else f\"training.{k}\",\n                v,\n                merge=False,\n            )\n\n    # Final training -----------------------------------------------------\n    run_training(cfg)\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Fails multiple mandatory requirements:\n1. Trial-mode workload reduction is incomplete – epochs are set to 1 but the dataloaders still iterate over the full training / validation sets (requirement 9 demands limiting to 1-2 batches and a small eval subset).\n2. Evaluation script only saves combined summaries and one bar chart; it does not compute derived / secondary metrics (e.g. improvement-rate) nor perform statistical‐significance tests as explicitly required in step-2 of requirement 8.\nWithout these fixes the code cannot be accepted for immediate execution."
}
