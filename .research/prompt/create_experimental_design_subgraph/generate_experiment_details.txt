
LLM Name: o3-2025-04-16
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "In DPO-style preference optimization the single hyper-parameter β simultaneously (1) scales the preference loss curvature and (2) weights the KL-divergence regulariser.  This coupling makes training unstable: small β under-regularises but also flattens the preference loss, while large β over-regularises and explodes the preference-loss gradient.  Reported failures of DiscoPOP/DPO for β≤0.01 or β≥2.5 stem directly from this entanglement.",
    "Methods": "Temperature-Decoupled Direct Preference Optimisation (TD-DPO)\n1. Keep the KL term exactly as in DPO, still weighted by β.\n2. Introduce an independent temperature τ that ONLY rescales the preference margin Δ = log p_θ(y⁺|x) − log p_θ(y⁻|x).\n   TD-DPO loss per pair:\n       L = − log σ( Δ / τ )  +  β · KL(p_θ || p₀)\n   where σ is the sigmoid.  Setting τ<1 sharpens the preference signal without forcing a smaller KL, while τ>1 smooths gradients when β must be large for safety.\nTheoretically, this separates information-theoretic regularisation (β) from optimisation stability (τ), giving one extra scalar degree of freedom with negligible implementation cost.",
    "Experimental Setup": "Model: open-source 7B Llama-2-chat (HF transformers).\nData: Argilla DPO-Mix-7k (same as DiscoPOP paper) – train/valid split 90/10.\nBaselines: (a) original DPO (single β), (b) DiscoPOP (best reported β), (c) proposed TD-DPO.\nHyper-grid: β ∈ {0.01,0.05,0.5,2.5}; τ ∈ {0.5,1.0,2.0} (TD-DPO only).\nOptimiser & hardware: AdamW, lr 1e-5, 4 × A100 80G, 3 epochs.\nEvaluation: MT-Bench score (GPT-4 judge) and validation pairwise accuracy.\nReport mean score over three seeds.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=0.05, tau=1.0):\n    \"\"\"TD-DPO pairwise loss.\n    logp_* : tensors of shape (batch,)\n    beta   : KL weight (as in DPO)\n    tau    : new temperature for preference margin\n    returns scalar loss\"\"\"\n    # preference term\n    delta = (logp_pos - logp_neg) / tau        # <-- only change w.r.t. DPO\n    pref_loss = -F.logsigmoid(delta).mean()\n    # KL term (same as DPO)\n    kl = 0.5*((logp_pos - logp_pos_ref)**2 + (logp_neg - logp_neg_ref)**2).mean()\n    return pref_loss + beta * kl\n\n# during training\n# logits_pos, logits_neg come from current model\n# logits_pos_ref, logits_neg_ref from frozen reference model\nloss = td_dpo_loss(logp_pos, logp_neg, logp_pos_ref, logp_neg_ref, beta=beta, tau=tau)\nloss.backward()",
    "Expected Result": "• When β is very small (0.01) TD-DPO with τ=0.5 regains strong gradients and matches performance of best-tuned DPO at β=0.05.\n• When β is large (2.5) TD-DPO with τ=2.0 avoids divergence; MT-Bench improves by ≈+1.5 points over baseline DiscoPOP.\n• For mid-range β (0.05) the default τ=1 keeps parity with DPO (<0.1 score difference).\nOverall, cross-β variance of MT-Bench scores is reduced by ~40 %, showing robustness.",
    "Expected Conclusion": "A single extra scalar (τ) disentangles optimisation curvature from regularisation strength, eliminating observed instabilities of DPO/DiscoPOP with almost no code change.  Because the modification is orthogonal, existing hyper-parameter β sweeps remain valid, while τ offers a lightweight knob for further gains.  This demonstrates how a minimal but principled adjustment to the objective function yields measurable, practical improvements for LLM preference optimisation."
}

# MODEL LIST
{
    "Llama-4-Scout-17B-16E": {
        "model_parameters": {
            "total_parameters": "109b",
            "active_parameters": "17b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "Llama-4-Maverick-17B-128E": {
        "model_parameters": {
            "total_parameters": "400b",
            "active_parameters": "17b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "Qwen3-0.6B": {
        "model_parameters": "0.6b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-0.6B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-1.7B": {
        "model_parameters": "1.7b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-1.7B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-4B": {
        "model_parameters": "4b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-4B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-8B": {
        "model_parameters": "8b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-8B",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-8B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-14B": {
        "model_parameters": "14b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-14B",
        "language_distribution": "",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-14B\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "Qwen3-32B": {
        "model_parameters": "32.8b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/Qwen/Qwen3-32B",
        "language_distribution": "",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers>=4.51.0"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-32B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()",
        "citation": "@misc{qwen3technicalreport,\n    title={Qwen3 Technical Report},\n    author={Qwen Team},\n    year={2025},\n    eprint={2505.09388},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2505.09388},\n}"
    },
    "DeepSeek-v3": {
        "model_parameters": {
            "total_parameters": "671b",
            "active_parameters": "37b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n    title={DeepSeek-V3 Technical Report},\n    author={DeepSeek-AI},\n    year={2024},\n    eprint={2412.19437},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2412.19437},\n}"
    },
    "DeepSeek-V3.1": {
        "model_parameters": {
            "total_parameters": "671B",
            "active_parameters": "37B"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "Text"
        ],
        "output_modalities": [
            "Text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv3technicalreport,\n    title={DeepSeek-V3 Technical Report},\n    author={DeepSeek-AI},\n    year={2024},\n    eprint={2412.19437},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2412.19437},\n}"
    },
    "DeepSeek-V3.2-Exp": {
        "model_parameters": {
            "total_parameters": "671B",
            "active_parameters": "37B"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "Text"
        ],
        "output_modalities": [
            "Text"
        ],
        "dependent packages": [],
        "code": "",
        "citation": "@misc{deepseekai2024deepseekv32,\n    title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\n    author={DeepSeek-AI},\n    year={2025},\n}"
    },
    "gpt-oss-20b": {
        "model_parameters": {
            "total_parameters": "21b",
            "active_parameters": "3.6b"
        },
        "model_architecture": "Transformer-based Mixture-of-Experts (MoE) architecture",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/openai/gpt-oss-20b",
        "context_length": "",
        "language_distribution": "multilingual",
        "input_modalities": "text",
        "output_modalities": "text",
        "dependent packages": [
            "accelerate",
            "transformers",
            "kernels"
        ],
        "code": "from transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"openai/gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n]\n\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\n).to(model.device)\n\ngenerated = model.generate(**inputs, max_new_tokens=100)\nprint(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@misc{openai2025gptoss120bgptoss20bmodel,\n    title={gpt-oss-120b & gpt-oss-20b Model Card},\n    author={OpenAI},\n    year={2025},\n    eprint={2508.10925},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2508.10925},\n}"
    },
    "gemma-3-1b-it": {
        "model_parameters": "1b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-1b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    },
    "gemma-3-4b-it": {
        "model_parameters": "4b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-4b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-4b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    },
    "gemma-3-27b-it": {
        "model_parameters": "27b",
        "model_architecture": "Transformer",
        "training_data_sources": "",
        "huggingface_url": "https://huggingface.co/google/gemma-3-27b-it",
        "language_distribution": "Multilingual",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "dependent packages": [
            "transformers"
        ],
        "code": "from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-27b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-27b-it\")\nmessages = [\n{\"role\": \"user\", \"content\": \"自己紹介してください\"},\n]\ninputs = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(**inputs, max_new_tokens=4000)\nprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
        "citation": "@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}"
    }
}

# DATASET LIST
{
    "alpaca-cleaned": {
        "discription": "",
        "num_training_samples": "",
        "num_validation_samples": "",
        "huggingface_url": "https://huggingface.co/datasets/yahma/alpaca-cleaned",
        "language_distribution": "",
        "dependent packages": [],
        "code": "",
        "citation": ""
    },
    "databricks-dolly-15k": "",
    "gsm8k": {
        "discription": "A dataset of elementary school math word problems requiring 2 to 8 steps to solve",
        "num_training_samples": 7473,
        "num_validation_samples": 1319,
        "huggingface_url": "https://huggingface.co/datasets/openai/gsm8k",
        "language_distribution": "English",
        "dependent packages": [],
        "code": "",
        "citation": "@article{cobbe2021gsm8k,\ntitle={Training Verifiers to Solve Math Word Problems},\nauthor={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},\njournal={arXiv preprint arXiv:2110.14168},\nyear={2021}\n}"
    },
    "MATH": {
        "discription": "The MATH dataset consists of approximately 12,500 mathematics problems ranging from middle school to early university level. Each problem includes a natural language question, a detailed step-by-step solution, and a final answer, and it is widely used to evaluate large language models (LLMs) in terms of their abilities in mathematical reasoning, logical inference, and step-by-step problem solving.",
        "num_training_samples": 12500,
        "num_validation_samples": 0,
        "huggingface_url": "https://huggingface.co/datasets/qwedsacf/competition_math",
        "language_distribution": "English",
        "dependent packages": [],
        "code": "",
        "example": "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n'level': 'Level 1',\n'type': 'Counting & Probability',\n'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\frac{5}{12}+\\frac{1}{3}+x$, from which we have $x=\\boxed{\\frac{1}{4}}$.'}",
        "data_structure": "A data instance consists of a competition math problem and its step-by-step solution written in LaTeX and natural language. The step-by-step solution contains the final answer enclosed in LaTeX's \boxed tag.\n- problem: The competition math problem.\n- solution: The step-by-step solution.\n- level: The problem's difficulty level from 'Level 1' to 'Level 5', where a subject's easiest problems for humans are assigned to 'Level 1' and a subject's hardest problems are assigned to 'Level 5'.\n- type: The subject of the problem: Algebra, Counting & Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra and Precalculus.",
        "citation": "@article{hendrycksmath2021,\ntitle={Measuring Mathematical Problem Solving With the MATH Dataset},\nauthor={Dan Hendrycks\nand Collin Burns\nand Saurav Kadavath\nand Akul Arora\nand Steven Basart\nand Eric Tang\nand Dawn Song\nand Jacob Steinhardt},\njournal={arXiv preprint arXiv:2103.03874},\nyear={2021}\n}"
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- proposed_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - Output a list of objects, where each object contains "name" (hyperparameter name) and "range" (search range).
  - For example: [{"name": "learning_rate", "range": "0.001-0.01"}, {"name": "batch_size", "range": "16,32,64"}, {"name": "weight_decay", "range": "0.0001-0.001"}]
  - Search ranges can be expressed as ranges (e.g., "0.001-0.01") or discrete values (e.g., "16,32,64").
Output:
{
    "experiment_summary": "Purpose: verify that Temperature-Decoupled Direct Preference Optimisation (TD-DPO) makes preference-finetuning of large language models more stable and robust than standard Direct Preference Optimisation (DPO).\n\nComponents & workflow:\n1. Base model: load HuggingFace “Llama-2-Chat-7B” in 4-bit QLoRA format so it fits comfortably on a single A100; keep a frozen copy as reference policy.\n2. Data: use the public Argilla DPO-Mix-7k pair-preference corpus (≈6.3 k train / 0.7 k val). Each sample provides (x, y⁺, y⁻).\n3. Training loops (three random seeds):\n   a. Baseline run with standard DPO loss for four β values {0.01,0.05,0.5,2.5}.\n   b. Proposed TD-DPO run on the same β grid while sweeping the additional temperature τ ∈ {0.5,1.0,2.0}.\n   c. optimiser: AdamW, lr 1e-5, weight-decay 0.01, batch 128, 3 epochs; gradient-accumulation and FSDP across 4×A100 (≤60 GB GPU-RAM used).\n4. During training compute validation pairwise accuracy every 250 steps; keep best checkpoint by this metric.\n5. Post-training evaluation: generate answers on MT-Bench v1.1, score with GPT-4 auto-judge, report mean of three seeds.\n6. Analysis: compare MT-Bench and validation accuracy across β; measure variance reduction and highlight cases β=0.01 and β=2.5 where TD-DPO is expected to outperform.\n\nThe experiment therefore demonstrates that decoupling curvature (τ) from regularisation (β) yields higher scores and lower sensitivity with negligible extra cost.",
    "evaluation_metrics": [
        "MT-Bench score",
        "Pairwise accuracy"
    ],
    "models_to_use": [
        "Llama-2-Chat-7B"
    ],
    "datasets_to_use": [
        "Argilla DPO-Mix-7k"
    ],
    "proposed_method": "Temperature-Decoupled Direct Preference Optimisation (TD-DPO)\nObjective: stabilise preference-based RL-free finetuning by separating the roles of the single DPO hyper-parameter β.\nTheory: In DPO, β simultaneously scales the KL-divergence regulariser and the curvature of the pairwise preference loss. TD-DPO introduces an independent temperature τ that rescales only the preference margin Δ = log pθ(y⁺|x) − log pθ(y⁻|x). This yields:\n   L(x,y⁺,y⁻) = −log σ(Δ/τ) + β · KL( pθ || p0 ).\nProperties:\n• β continues to control the information-theoretic distance from the reference policy.\n• τ (<1 sharpen, >1 smooth) controls optimisation curvature without touching the KL term.\nAlgorithmic procedure per minibatch:\n1. Forward pass current model to get log-probs for preferred and dispreferred answers.\n2. Forward pass frozen reference model for the same answers.\n3. Compute Δ, divide by τ, compute −log σ to obtain preference loss.\n4. Compute symmetric KL between current and reference log-probs.\n5. Combine losses as above; back-propagate, update with AdamW.\n6. Repeat for all β,τ grid points. Hyper-parameter search chooses (β,τ) that maximises validation accuracy.\nImplementation adds two code lines to standard DPO: one temperature division and one hyper-parameter entry, so existing codebases can adopt TD-DPO instantly.",
    "comparative_methods": [
        "Direct Preference Optimisation (DPO)"
    ],
    "hyperparameters_to_search": [
        {
            "name": "beta",
            "range": "0.01,0.05,0.5,2.5"
        },
        {
            "name": "tau",
            "range": "0.5,1.0,2.0"
        }
    ]
}
