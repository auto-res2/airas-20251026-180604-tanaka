=== [UV SYNC] Start at Sun Oct 26 09:10:33 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:10:38 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:10:38 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:11:18,843][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:11:24,621][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:13:28,011][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:14:03,339][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:14:07,950][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:16:02,498][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:16:34,037][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:16:38,677][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:18:44,570][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:19:11 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 09:20:24 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:20:30 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:20:30 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
=== [UV SYNC] Start at Sun Oct 26 09:24:14 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:24:19 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:24:19 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:24:47,099][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:24:52,260][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:27:04,192][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:27:36,837][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:27:41,708][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:29:45,619][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:30:24,955][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:30:29,667][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:32:27,307][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:32:51 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 09:43:48 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:43:54 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:43:54 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:44:38,028][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:44:43,451][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:46:50,807][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:47:35,889][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:47:40,568][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:49:44,557][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:50:20,438][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:50:25,204][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:52:20,863][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:52:47 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 09:54:11 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:54:16 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:54:16 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:54:59,218][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:55:04,234][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:57:04,866][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:57:57,484][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:58:02,212][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:59:56,890][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:00:40,205][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:00:44,896][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:02:45,912][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:03:08 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:05:12 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:05:18 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:05:18 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:06:00,537][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:06:05,587][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:08:08,574][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:08:47,861][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:08:52,482][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:10:47,076][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:11:20,271][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:11:24,833][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:13:26,730][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:13:50 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:15:10 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:15:15 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:15:15 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:15:56,005][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:16:00,893][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:18:01,610][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:18:32,176][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:18:36,828][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:20:36,715][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:21:16,670][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:21:21,319][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:23:20,735][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:23:48 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:26:03 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:26:09 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:26:09 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:26:51,388][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:26:57,169][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:29:03,444][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:29:40,142][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:29:45,974][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:31:49,634][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:32:33,660][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:32:38,451][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:34:35,809][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:34:59 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:36:19 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:36:25 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:36:25 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:37:09,254][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:37:14,296][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:39:13,839][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:39:49,007][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:39:53,604][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:41:52,962][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:42:27,980][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:42:32,802][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:44:34,148][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:45:02 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:46:31 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:46:36 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:46:36 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:47:18,621][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:47:23,865][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:49:25,753][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:49:58,966][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:50:03,651][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:51:57,439][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:52:30,933][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:52:35,555][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:54:36,784][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:54:55 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:56:07 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:56:12 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:56:12 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:56:52,741][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:56:57,759][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:58:54,668][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:59:29,650][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:59:34,237][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:01:34,166][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:02:16,156][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:02:20,913][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:04:20,992][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:04:44 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 11:07:11 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 11:07:16 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:07:16 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:07:57,782][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:08:03,729][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:10:03,955][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:10:43,768][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:10:50,311][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:12:51,481][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:13:31,678][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:13:36,988][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:15:31,685][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:15:55 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 11:17:19 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 11:17:25 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:17:25 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:18:09,967][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:18:15,068][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:20:18,330][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:21:01,455][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:21:06,533][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:23:12,157][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:23:56,249][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:24:01,297][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:26:02,377][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:26:27 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 11:28:55 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 11:29:03 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:29:03 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:30:06,126][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:30:11,380][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:32:34,709][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:33:24,432][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:33:30,477][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:35:39,343][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:36:19,541][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:36:24,278][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:38:20,774][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:38:50 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 11:40:01 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 11:40:07 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:40:07 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:40:47,752][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:40:52,775][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:42:55,025][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:43:39,171][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:43:44,111][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:45:45,546][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:46:15,391][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:46:19,965][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:48:12,621][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:48:39 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 11:57:27 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 11:57:33 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:57:33 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:58:15,255][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:58:20,513][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:00:22,056][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:01:00,165][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:01:04,789][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:03:04,105][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:03:45,208][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:03:49,944][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:05:47,210][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:06:23 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 12:07:55 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 12:08:02 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:08:02 AM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:08:52,591][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:08:57,807][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:11:08,258][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:11:45,093][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:11:49,893][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:13:59,960][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:14:36,067][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:14:41,095][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:16:45,600][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:17:15 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 12:19:15 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 12:19:21 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:19:21 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:20:01,922][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:20:07,351][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:22:07,338][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:22:46,542][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:22:51,389][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:24:53,647][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:25:31,017][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:25:36,216][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:27:34,518][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:28:08 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 12:29:26 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 12:29:32 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:29:32 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:30:15,054][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:30:20,067][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:32:19,510][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:33:01,148][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:33:05,757][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:35:05,768][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:35:39,742][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:35:44,409][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:37:42,997][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:38:06 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 12:40:01 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 12:40:08 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:40:08 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:40:50,154][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:40:55,362][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:42:56,479][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:43:34,749][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:43:39,384][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:45:40,503][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:46:15,305][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:46:19,944][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:48:14,815][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:48:49 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 12:50:02 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 12:50:08 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:50:08 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:50:48,616][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:50:53,744][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:52:54,688][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:53:19,874][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:53:24,515][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:55:24,571][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 00:55:56,976][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 00:56:01,951][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 00:58:04,214][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 12:58:31 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 01:00:53 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 01:00:59 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:00:59 AM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:01:47,026][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:01:52,293][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:03:57,111][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:04:38,394][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:04:43,163][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:06:50,946][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:07:33,233][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:07:38,308][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:09:39,829][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:10:01 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 01:11:31 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 01:11:37 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:11:37 AM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:12:20,883][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:12:26,919][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:14:29,855][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:15:03,217][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:15:07,851][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:17:23,711][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:18:05,276][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:18:12,474][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:20:08,809][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:20:35 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 01:29:37 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 01:29:43 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:29:43 AM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:30:26,408][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:30:31,793][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:32:37,113][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:33:22,123][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:33:27,057][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:35:48,626][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:36:22,607][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:36:28,223][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:38:29,329][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:39:00 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 01:40:17 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 01:40:23 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:40:23 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:41:06,031][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:41:10,874][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:43:12,476][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:43:48,943][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:43:53,898][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:45:52,685][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:46:25,102][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:46:29,955][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:48:37,255][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:49:06 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 01:51:42 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 01:51:48 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 01:51:48 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:52:28,437][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:52:33,466][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:54:33,024][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:55:10,327][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:55:14,987][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:57:14,065][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 01:57:45,813][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 01:57:50,377][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 01:59:46,779][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:00:19 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 02:02:02 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 02:02:08 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:02:08 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:02:48,977][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:02:54,198][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:04:53,557][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:05:29,178][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:05:34,110][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:07:29,710][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:08:11,509][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:08:16,109][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:10:22,111][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:10:39 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 02:12:35 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 02:12:42 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:12:42 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:13:22,748][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:13:27,667][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:15:28,724][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:16:00,286][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:16:05,233][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-27 02:17:48,193][huggingface_hub.utils._http][WARNING] - '(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: ad40c95d-b290-4010-a0a3-246578679a29)')' thrown while requesting HEAD https://huggingface.co/georgesung/llama2_7b_chat_uncensored/resolve/main/generation_config.json
[2025-10-27 02:17:48,193][huggingface_hub.utils._http][WARNING] - Retrying in 1s [Retry 1/5].
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:18:09,435][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:18:44,045][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:18:48,690][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:20:40,200][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:20:58 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 02:22:15 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 02:22:20 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:22:20 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:23:01,454][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:23:06,323][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:25:08,624][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:25:45,563][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:25:50,578][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:27:49,307][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:28:31,446][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:28:36,180][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:30:30,978][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:31:02 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 02:33:30 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 02:33:36 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:33:36 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:34:16,281][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:34:21,344][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:36:23,053][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:36:52,477][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:36:57,876][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:38:52,760][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:39:37,703][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:39:42,319][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:41:45,060][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:42:12 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 02:43:39 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 02:43:45 AM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:43:45 AM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:44:28,712][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:44:33,847][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:46:34,800][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:47:09,444][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:47:14,590][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:49:17,768][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:49:48,090][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:49:52,802][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:51:51,363][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:52:15 AM UTC 2025 ===

=== [UV SYNC] Start at Mon Oct 27 02:54:21 AM UTC 2025 ===
=== [UV SYNC] Finished successfully at Mon Oct 27 02:54:27 AM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 02:54:27 AM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:55:07,310][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:55:12,415][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:57:09,015][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 02:57:40,663][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 02:57:45,353][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 02:59:39,679][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-27 03:00:16,194][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-27 03:00:21,101][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-27 03:02:18,011][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Mon Oct 27 03:02:45 AM UTC 2025 ===

