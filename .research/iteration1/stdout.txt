=== [UV SYNC] Start at Sun Oct 26 09:10:33 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:10:38 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:10:38 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:11:18,843][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:11:24,621][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:13:28,011][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:14:03,339][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:14:07,950][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:16:02,498][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:16:34,037][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:16:38,677][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:18:44,570][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:19:11 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 09:20:24 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:20:30 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:20:30 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
=== [UV SYNC] Start at Sun Oct 26 09:24:14 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:24:19 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:24:19 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:24:47,099][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:24:52,260][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:27:04,192][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:27:36,837][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:27:41,708][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:29:45,619][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:30:24,955][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:30:29,667][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:32:27,307][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:32:51 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 09:43:48 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:43:54 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:43:54 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:44:38,028][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:44:43,451][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:46:50,807][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:47:35,889][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:47:40,568][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:49:44,557][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:50:20,438][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:50:25,204][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:52:20,863][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:52:47 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 09:54:11 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 09:54:16 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 09:54:16 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:54:59,218][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:55:04,234][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:57:04,866][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 21:57:57,484][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 21:58:02,212][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 21:59:56,890][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:00:40,205][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:00:44,896][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:02:45,912][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:03:08 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:05:12 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:05:18 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:05:18 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:06:00,537][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:06:05,587][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:08:08,574][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:08:47,861][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:08:52,482][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:10:47,076][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:11:20,271][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:11:24,833][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:13:26,730][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:13:50 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:15:10 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:15:15 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:15:15 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:15:56,005][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:16:00,893][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:18:01,610][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:18:32,176][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:18:36,828][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:20:36,715][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:21:16,670][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:21:21,319][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:23:20,735][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:23:48 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:26:03 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:26:09 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:26:09 PM UTC 2025 ===
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:26:51,388][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:26:57,169][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:29:03,444][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:29:40,142][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:29:45,974][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:31:49,634][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-a/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:32:33,660][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:32:38,451][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:34:35,809][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:34:59 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:36:19 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:36:25 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:36:25 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:37:09,254][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:37:14,296][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:39:13,839][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:39:49,007][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:39:53,604][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:41:52,962][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:42:27,980][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:42:32,802][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:44:34,148][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:45:02 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:46:31 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:46:36 PM UTC 2025 ===
=== [TRIAL RUN] Start for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:46:36 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:47:18,621][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:47:23,865][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:49:25,753][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:49:58,966][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:50:03,651][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:51:57,439][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:52:30,933][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:52:35,555][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:54:36,784][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for proposed-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:54:55 PM UTC 2025 ===

=== [UV SYNC] Start at Sun Oct 26 10:56:07 PM UTC 2025 ===
=== [UV SYNC] Finished successfully at Sun Oct 26 10:56:12 PM UTC 2025 ===
=== [TRIAL RUN] Start for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 10:56:12 PM UTC 2025 ===
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed42 training.seed=42 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:56:52,741][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:56:57,759][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 22:58:54,668][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed43 training.seed=43 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 22:59:29,650][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 22:59:34,237][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:01:34,166][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Launching: /home/toma/t-80-8-b-03/_work/airas-20251026-180604-tanaka/airas-20251026-180604-tanaka/.venv/bin/python3 -u -m src.train run=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k run_id=comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k-seed44 training.seed=44 results_dir=.research/iteration1 wandb.mode=disabled mode=trial
[2025-10-26 23:02:16,156][transformers.models.llama.tokenization_llama][WARNING] - You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
[2025-10-26 23:02:20,913][transformers.configuration_utils][WARNING] - `torch_dtype` is deprecated! Use `dtype` instead!
trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3181
[2025-10-26 23:04:20,992][transformers.utils.generic][WARNING] - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
=== [TRIAL RUN] PASSED for comparative-1-Llama-2-Chat-7B-Argilla-DPO-Mix-7k at Sun Oct 26 11:04:44 PM UTC 2025 ===

